# üï∑Ô∏è Sistema Completo de Busca e Download com Scrapy

## Nova Corrente - Demand Forecasting System

---

## üìã Vis√£o Geral

Sistema completo de descoberta, busca e download de datasets estruturados usando Scrapy e APIs de m√∫ltiplas fontes.

---

## üöÄ Funcionalidades Implementadas

### 1. **Integra√ß√£o Scrapy com Pipeline Principal**

**Arquivo:** `src/pipeline/scrapy_integration.py`

‚úÖ Integra√ß√£o completa dos spiders Scrapy com o pipeline de download  
‚úÖ Execu√ß√£o autom√°tica de spiders quando necess√°rio  
‚úÖ Fallback para m√©todos manuais se Scrapy falhar  
‚úÖ Rastreamento de resultados de download

**Uso:**
```python
from src.pipeline.scrapy_integration import ScrapyIntegration

integration = ScrapyIntegration()
results = integration.run_all_scrapy_datasets()
```

---

### 2. **Busca Inteligente de Datasets**

**Arquivo:** `scripts/enhance_dataset_search.py`

‚úÖ Busca em Zenodo por query  
‚úÖ Busca em GitHub por reposit√≥rios  
‚úÖ Busca no Kaggle (requer autentica√ß√£o)  
‚úÖ Descoberta autom√°tica de datasets Anatel  
‚úÖ Valida√ß√£o de reposit√≥rios para arquivos de dados

**Funcionalidades:**
- `search_zenodo()` - Busca datasets no Zenodo
- `search_github()` - Busca reposit√≥rios GitHub
- `search_kaggle()` - Busca datasets Kaggle
- `discover_anatel_datasets()` - Descobre datasets Anatel
- `suggest_datasets()` - Busca em m√∫ltiplas fontes
- `save_discovered_datasets()` - Salva resultados

**Uso:**
```bash
# Buscar datasets
python scripts/enhance_dataset_search.py --keywords telecom demand forecast brazil

# Resultados salvos em: data/raw/discovered_datasets.json
```

---

### 3. **Pipeline Completo de Busca e Download**

**Arquivo:** `scripts/fetch_and_structure_all.py`

‚úÖ Pipeline completo: descoberta ‚Üí download ‚Üí estrutura√ß√£o  
‚úÖ Integra√ß√£o de todos os m√©todos de download  
‚úÖ Relat√≥rios detalhados de sucesso/falha  
‚úÖ Verifica√ß√£o de arquivos baixados

**Execu√ß√£o:**
```bash
python scripts/fetch_and_structure_all.py
```

**Fluxo:**
1. **Descoberta** - Busca datasets em todas as fontes
2. **Download Scrapy** - Baixa datasets que requerem scraping
3. **Download Padr√£o** - Baixa datasets via APIs/URLs diretas
4. **Verifica√ß√£o** - Confirma arquivos baixados

---

## üì¶ Scripts Dispon√≠veis

### Script 1: `scripts/fetch_all_datasets_scrapy.py`

**Prop√≥sito:** Executar todos os spiders Scrapy configurados

**Uso:**
```bash
python scripts/fetch_all_datasets_scrapy.py
```

**Funcionalidades:**
- Identifica datasets que requerem scraping
- Executa spiders apropriados
- Salva metadados de downloads
- Gera relat√≥rio de sucesso/falha

---

### Script 2: `scripts/enhance_dataset_search.py`

**Prop√≥sito:** Buscar e descobrir novos datasets estruturados

**Uso:**
```bash
# Busca padr√£o (telecom, demand, forecast)
python scripts/enhance_dataset_search.py

# Busca customizada
python scripts/enhance_dataset_search.py --keywords mobile broadband network

# Salvar em arquivo espec√≠fico
python scripts/enhance_dataset_search.py --output data/raw/my_discoveries.json
```

**Fontes de Busca:**
- Zenodo (API)
- GitHub (API search)
- Kaggle (API, requer token)
- Anatel/Data Basis (web scraping)

---

### Script 3: `scripts/fetch_and_structure_all.py`

**Prop√≥sito:** Pipeline completo de descoberta e download

**Uso:**
```bash
python scripts/fetch_and_structure_all.py
```

**Executa:**
1. Descoberta de datasets
2. Download via Scrapy
3. Download via m√©todos padr√£o
4. Verifica√ß√£o e relat√≥rio

---

## üîß Integra√ß√£o com Pipeline Principal

### Modifica√ß√£o em `download_datasets.py`

O m√©todo `download_all_datasets()` agora usa automaticamente Scrapy quando `download_method == 'scrape'`:

```python
elif download_method == 'scrape':
    # Use Scrapy integration for scraping
    from src.pipeline.scrapy_integration import ScrapyIntegration
    scrapy_integration = ScrapyIntegration(config_path=self.config_path)
    success = scrapy_integration.run_scrapy_spider(dataset_id, dataset_info)
    
    if not success:
        # Fallback to manual scraping methods
        ...
```

---

## üìä Estrutura de Dados Descobertos

### Formato JSON (`discovered_datasets.json`)

```json
{
  "zenodo": [
    {
      "id": "12345",
      "title": "Dataset Title",
      "doi": "10.1234/example",
      "url": "https://zenodo.org/record/12345",
      "csv_file": "https://zenodo.org/record/12345/files/data.csv",
      "creators": ["Author Name"],
      "description": "...",
      "keywords": ["telecom", "demand"],
      "publication_date": "2024-01-01"
    }
  ],
  "github": [
    {
      "id": 12345,
      "name": "repo-name",
      "full_name": "user/repo-name",
      "url": "https://github.com/user/repo-name",
      "description": "...",
      "stars": 42,
      "updated_at": "2024-01-01T00:00:00Z"
    }
  ],
  "kaggle": [
    {
      "ref": "user/dataset-name",
      "title": "Dataset Title",
      "url": "https://www.kaggle.com/datasets/user/dataset-name",
      "size": "123MB",
      "download_count": 1000
    }
  ],
  "anatel": [
    {
      "title": "Dataset Title",
      "url": "https://data-basis.org/dataset/...",
      "source": "anatel"
    }
  ]
}
```

---

## üéØ Casos de Uso

### Caso 1: Buscar Novos Datasets

```bash
# Buscar datasets relacionados a telecom no Brasil
python scripts/enhance_dataset_search.py --keywords telecom brazil mobile

# Resultados em: data/raw/discovered_datasets.json
```

### Caso 2: Download Autom√°tico via Scrapy

```bash
# Executar todos os spiders Scrapy
python scripts/fetch_all_datasets_scrapy.py

# Ou usar integra√ß√£o direta
python -c "from src.pipeline.scrapy_integration import ScrapyIntegration; ScrapyIntegration().run_all_scrapy_datasets()"
```

### Caso 3: Pipeline Completo

```bash
# Descoberta + Download + Estrutura√ß√£o
python scripts/fetch_and_structure_all.py
```

---

## üîç Detalhes T√©cnicos

### Zenodo Search

- **API:** `https://zenodo.org/api/records`
- **Par√¢metros:** query, type=dataset, size, sort
- **Campos extra√≠dos:** id, title, doi, url, files, creators, description

### GitHub Search

- **API:** `https://api.github.com/search/repositories`
- **Query:** `{keywords} dataset`
- **Valida√ß√£o:** Verifica se reposit√≥rio cont√©m arquivos CSV/JSON
- **Limita√ß√£o:** Rate limit (requer token para mais resultados)

### Kaggle Search

- **API:** Kaggle API (requer autentica√ß√£o)
- **Biblioteca:** `kaggle` Python package
- **Configura√ß√£o:** `~/.kaggle/kaggle.json`

### Anatel Discovery

- **M√©todo:** Web scraping do Data Basis
- **URL:** `https://data-basis.org/datasets`
- **Limita√ß√£o:** 20 datasets por busca

---

## üìù Metadados e Logging

### Metadados de Download

**Arquivo:** `data/raw/download_metadata.json`

Armazenado automaticamente pelos pipelines Scrapy.

### Logs

Todos os scripts geram logs detalhados:
- Informa√ß√µes de busca
- Status de downloads
- Erros e warnings
- Resumos finais

---

## üöÄ Pr√≥ximos Passos

### Melhorias Planejadas

1. **Auto-configura√ß√£o de Novos Datasets**
   - Detectar automaticamente estrutura de datasets descobertos
   - Gerar configura√ß√µes para `datasets_config.json`

2. **Valida√ß√£o Autom√°tica**
   - Validar datasets baixados antes de processar
   - Verificar schema e qualidade

3. **Notifica√ß√µes**
   - Alertas para novos datasets encontrados
   - Notifica√ß√µes de atualiza√ß√µes

4. **Dashboard**
   - Interface visual para descoberta
   - Monitoramento de downloads
   - Estat√≠sticas de uso

---

## ‚úÖ Checklist de Implementa√ß√£o

- [x] Integra√ß√£o Scrapy com pipeline principal
- [x] Busca em Zenodo implementada
- [x] Busca em GitHub implementada
- [x] Busca no Kaggle implementada
- [x] Descoberta Anatel implementada
- [x] Pipeline completo de busca e download
- [x] Scripts de execu√ß√£o criados
- [x] Documenta√ß√£o completa

---

## üìä Estat√≠sticas

### Datasets Configurados: 18
### Spiders Scrapy: 5
### Fontes de Busca: 4 (Zenodo, GitHub, Kaggle, Anatel)

---

**Status:** ‚úÖ **SISTEMA COMPLETO DE BUSCA E DOWNLOAD IMPLEMENTADO**

---

**Nova Corrente Grand Prix SENAI - Demand Forecasting System**


