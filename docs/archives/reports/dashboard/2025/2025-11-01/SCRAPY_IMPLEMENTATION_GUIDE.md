# üï∑Ô∏è Guia Completo de Implementa√ß√£o Scrapy

## Nova Corrente - Demand Forecasting System

---

## üìã Vis√£o Geral

Este documento detalha a implementa√ß√£o completa de spiders Scrapy para scraping de datasets de m√∫ltiplas fontes, especialmente focada em dados brasileiros de telecomunica√ß√µes.

---

## üï∑Ô∏è Spiders Implementados

### 1. **AnatelSpider** - Dados Regulat√≥rios Anatel

**Arquivo:** `src/scrapy/scrapy_spiders/anatel_spider.py`

**Funcionalidades:**
- ‚úÖ Scraping da p√°gina Data Basis (Base dos Dados)
- ‚úÖ Extra√ß√£o de links CSV via parsing HTML
- ‚úÖ Parsing de scripts JavaScript (React/Next.js)
- ‚úÖ Fallback para API Data Basis
- ‚úÖ Download direto de arquivos CSV

**Uso:**
```bash
python scripts/run_scrapy_spider.py anatel \
  --dataset-id anatel_mobile_brazil \
  --url "https://data-basis.org/dataset/d3c86a88-d9a4-4c0-bdec-08ab61e8f63c"
```

**Estrutura:**
- `parse_dataset_page()` - Parse da p√°gina do dataset
- `download_csv()` - Download do arquivo CSV
- Suporte para React/Next.js (extra√ß√£o de JSON embutido)
- Fallback para API Data Basis

---

### 2. **InternetAbertaSpider** - Relat√≥rios e Forecasts

**Arquivo:** `src/scrapy/scrapy_spiders/internet_aberta_spider.py`

**Funcionalidades:**
- ‚úÖ Download direto de PDFs
- ‚úÖ Busca de links PDF em p√°ginas web
- ‚úÖ Suporte para URLs diretas de PDF
- ‚úÖ Valida√ß√£o de arquivos baixados

**Uso:**
```bash
python scripts/run_scrapy_spider.py internet_aberta \
  --dataset-id internet_aberta_forecast \
  --url "https://internetaberta.com.br/wp-content/uploads/2024/05/Paper-1-EN-Data-Traffic-Demand-Forecast-for-Brazil.pdf"
```

**Estrutura:**
- `parse_page()` - Parse de p√°ginas web para encontrar PDFs
- `download_pdf()` - Download direto de PDFs
- Suporte para forecasts e relat√≥rios

---

### 3. **SpringerSpider** - Artigos Cient√≠ficos

**Arquivo:** `src/scrapy/scrapy_spiders/springer_spider.py`

**Funcionalidades:**
- ‚úÖ Parse de artigos cient√≠ficos Springer
- ‚úÖ Extra√ß√£o de links para datasets suplementares
- ‚úÖ Busca em se√ß√µes "Materials and Methods" e "Data Availability"
- ‚úÖ Detec√ß√£o de links para reposit√≥rios externos (Zenodo, GitHub, Figshare)
- ‚úÖ Download autom√°tico de arquivos de dados (CSV, ZIP, JSON)

**Uso:**
```bash
python scripts/run_scrapy_spider.py springer \
  --dataset-id springer_digital_divide \
  --url "https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-024-00508-8"
```

**Estrutura:**
- `parse_article()` - Parse do artigo cient√≠fico
- `download_data()` - Download de arquivos de dados
- Detec√ß√£o inteligente de links de dados
- Suporte para m√∫ltiplos formatos (CSV, ZIP, JSON)

---

### 4. **GitHubSpider** - Reposit√≥rios GitHub

**Arquivo:** `src/scrapy/scrapy_spiders/github_spider.py`

**Funcionalidades:**
- ‚úÖ Parse de reposit√≥rios GitHub
- ‚úÖ Convers√£o autom√°tica de URLs blob ‚Üí raw
- ‚úÖ Busca recursiva em diret√≥rios de dados
- ‚úÖ Download de m√∫ltiplos formatos (CSV, JSON, ZIP, Parquet, TSV, XLSX)

**Uso:**
```bash
python scripts/run_scrapy_spider.py github \
  --dataset-id github_5g3e \
  --url "https://github.com/cedric-cnam/5G3E-dataset"
```

**Estrutura:**
- `parse_repository()` - Parse do reposit√≥rio GitHub
- `download_file()` - Download de arquivos
- Convers√£o autom√°tica blob ‚Üí raw
- Busca recursiva em subdiret√≥rios

---

## üì¶ Items e Estrutura de Dados

### Items Definidos

**Arquivo:** `src/scrapy/scrapy_spiders/items.py`

#### 1. **DatasetItem** (Base)
```python
class DatasetItem(scrapy.Item):
    dataset_id = scrapy.Field()
    file_path = scrapy.Field()
    url = scrapy.Field()
    size = scrapy.Field()
    filename = scrapy.Field()
    file_type = scrapy.Field()
    download_date = scrapy.Field()
```

#### 2. **AnatelItem** (Espec√≠fico Anatel)
```python
class AnatelItem(DatasetItem):
    region = scrapy.Field()
    technology = scrapy.Field()
    subscribers = scrapy.Field()
    period = scrapy.Field()
```

#### 3. **ZenodoItem** (Espec√≠fico Zenodo)
```python
class ZenodoItem(DatasetItem):
    record_id = scrapy.Field()
    record_title = scrapy.Field()
    authors = scrapy.Field()
    description = scrapy.Field()
```

#### 4. **RepositoryItem** (Links de Reposit√≥rios)
```python
class RepositoryItem(scrapy.Item):
    repository_url = scrapy.Field()
    article_url = scrapy.Field()
    dataset_id = scrapy.Field()
    type = scrapy.Field()
```

---

## üîß Pipelines

### 1. **DatasetMetadataPipeline**

**Arquivo:** `src/scrapy/scrapy_spiders/pipelines.py`

**Funcionalidades:**
- ‚úÖ Salva metadados de todos os downloads
- ‚úÖ Rastreia URLs, tamanhos, datas
- ‚úÖ Armazena em `data/raw/download_metadata.json`

**Estrutura de Metadados:**
```json
{
  "dataset_id": [
    {
      "file_path": "data/raw/dataset/file.csv",
      "url": "https://...",
      "filename": "file.csv",
      "size": 12345,
      "file_type": "csv",
      "download_date": "2025-10-31T...",
      "spider": "anatel"
    }
  ]
}
```

---

### 2. **ValidateFilePipeline**

**Funcionalidades:**
- ‚úÖ Valida arquivos baixados
- ‚úÖ Verifica exist√™ncia e tamanho
- ‚úÖ Valida tipos de arquivo
- ‚úÖ Remove items inv√°lidos

---

## ‚öôÔ∏è Configura√ß√µes

### Settings do Scrapy

**Arquivo:** `src/scrapy/scrapy_spiders/settings.py`

**Configura√ß√µes Principais:**
- ‚úÖ `ROBOTSTXT_OBEY = False` (para scraping permitido)
- ‚úÖ `DOWNLOAD_DELAY = 1` (respeito aos sites)
- ‚úÖ `AUTOTHROTTLE_ENABLED = True` (controle autom√°tico de taxa)
- ‚úÖ `USER_AGENT` customizado
- ‚úÖ Pipelines ativados
- ‚úÖ Feed exports em JSON

---

## üöÄ Execu√ß√£o dos Spiders

### M√©todo 1: Script Individual

```bash
# Executar spider Anatel
python scripts/run_scrapy_spider.py anatel \
  --dataset-id anatel_mobile_brazil \
  --url "https://data-basis.org/dataset/d3c86a88-d9a4-4c0-bdec-08ab61e8f63c"

# Executar spider Internet Aberta
python scripts/run_scrapy_spider.py internet_aberta \
  --dataset-id internet_aberta_forecast \
  --url "https://internetaberta.com.br/wp-content/uploads/2024/05/Paper-1-EN-Data-Traffic-Demand-Forecast-for-Brazil.pdf"

# Executar spider Springer
python scripts/run_scrapy_spider.py springer \
  --dataset-id springer_digital_divide \
  --url "https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-024-00508-8"

# Executar spider GitHub
python scripts/run_scrapy_spider.py github \
  --dataset-id github_5g3e \
  --url "https://github.com/cedric-cnam/5G3E-dataset"
```

### M√©todo 2: Executar Todos os Spiders

```bash
# Executar todos os spiders configurados
python src/scrapy/run_all_spiders.py
```

**Este script:**
1. Carrega configura√ß√£o de datasets
2. Identifica datasets que requerem scraping
3. Executa spiders apropriados
4. Salva metadados de todos os downloads

---

## üìä Estrutura do Projeto Scrapy

```
src/scrapy/
‚îú‚îÄ‚îÄ __init__.py
‚îî‚îÄ‚îÄ scrapy_spiders/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ items.py                    # Defini√ß√µes de Items
    ‚îú‚îÄ‚îÄ pipelines.py               # Pipelines de processamento
    ‚îú‚îÄ‚îÄ settings.py                # Configura√ß√µes Scrapy
    ‚îú‚îÄ‚îÄ anatel_spider.py           # Spider Anatel
    ‚îú‚îÄ‚îÄ internet_aberta_spider.py   # Spider Internet Aberta
    ‚îú‚îÄ‚îÄ springer_spider.py         # Spider Springer
    ‚îú‚îÄ‚îÄ github_spider.py           # Spider GitHub
    ‚îî‚îÄ‚îÄ mit_spider.py              # Spider MIT (existente)
```

---

## üéØ Casos de Uso

### Caso 1: Scraping Anatel (Data Basis)

**Problema:** Data Basis usa React/Next.js, tornando dif√≠cil extrair links diretamente.

**Solu√ß√£o:**
1. Parse de HTML para encontrar links CSV
2. Extra√ß√£o de JSON embutido em scripts JavaScript
3. Fallback para API Data Basis se dispon√≠vel
4. Download direto de arquivos CSV

**Exemplo:**
```python
# Spider automaticamente:
# 1. Parse da p√°gina React
# 2. Busca links CSV
# 3. Tenta API Data Basis
# 4. Download do arquivo
```

---

### Caso 2: Download de PDFs (Internet Aberta)

**Problema:** PDFs podem estar em links diretos ou em p√°ginas intermedi√°rias.

**Solu√ß√£o:**
1. Download direto se URL √© PDF
2. Parse de p√°gina para encontrar links PDF
3. Valida√ß√£o de arquivos baixados

**Exemplo:**
```python
# Spider automaticamente:
# 1. Detecta se URL √© PDF direto
# 2. Ou parse p√°gina para encontrar PDF
# 3. Download e valida√ß√£o
```

---

### Caso 3: Artigos Cient√≠ficos (Springer)

**Problema:** Dados podem estar em m√∫ltiplos lugares (artigo, reposit√≥rios externos).

**Solu√ß√£o:**
1. Parse do artigo para encontrar links de dados
2. Detec√ß√£o de reposit√≥rios externos (Zenodo, GitHub)
3. Download autom√°tico de arquivos suplementares
4. Registro de links de reposit√≥rios para processamento posterior

**Exemplo:**
```python
# Spider automaticamente:
# 1. Parse artigo Springer
# 2. Busca se√ß√µes "Data Availability"
# 3. Encontra links CSV/ZIP
# 4. Detecta links Zenodo/GitHub
# 5. Download de arquivos diretos
```

---

### Caso 4: Reposit√≥rios GitHub

**Problema:** Arquivos podem estar em m√∫ltiplos diret√≥rios, URLs blob precisam ser convertidas.

**Solu√ß√£o:**
1. Convers√£o autom√°tica blob ‚Üí raw
2. Busca recursiva em diret√≥rios de dados
3. Download de m√∫ltiplos formatos

**Exemplo:**
```python
# Spider automaticamente:
# 1. Converte URLs blob para raw
# 2. Busca em diret√≥rios "data", "dataset"
# 3. Download de todos os arquivos encontrados
```

---

## üîç Detalhes T√©cnicos

### AnatelSpider - Parsing React/Next.js

**Desafio:** Data Basis usa React, ent√£o HTML renderizado n√£o cont√©m todos os dados.

**Abordagem:**
1. Parse de scripts JavaScript que cont√™m `__NEXT_DATA__`
2. Extra√ß√£o de JSON com informa√ß√µes do dataset
3. Fallback para API Data Basis

**C√≥digo:**
```python
# Buscar JSON embutido em scripts
scripts = response.css('script::text').getall()
for script in scripts:
    if '__NEXT_DATA__' in script:
        json_match = re.search(r'__NEXT_DATA__.*?({.+?})', script)
        if json_match:
            data = json.loads(json_match.group(1))
            # Processar dados do dataset
```

---

### SpringerSpider - Detec√ß√£o de Links de Dados

**Abordagem:**
1. Parse de texto completo do artigo
2. Busca por padr√µes de URLs (regex)
3. Filtragem por extens√µes relevantes (.csv, .zip, .json)

**C√≥digo:**
```python
# Buscar padr√µes de URLs de dados
text_content = response.css('body').get()
data_urls = re.findall(
    r'https?://[^\s<>"\'{}|\\^`\[\]]+\.(?:csv|zip|json|parquet)',
    text_content
)
```

---

## üìù Metadados e Logging

### Arquivo de Metadados

**Localiza√ß√£o:** `data/raw/download_metadata.json`

**Estrutura:**
```json
{
  "anatel_mobile_brazil": [
    {
      "file_path": "data/raw/anatel_mobile_brazil/mobile_data.csv",
      "url": "https://data-basis.org/...",
      "filename": "mobile_data.csv",
      "size": 58234,
      "file_type": "csv",
      "download_date": "2025-10-31T21:30:00",
      "spider": "anatel"
    }
  ]
}
```

### Logs

**Localiza√ß√£o:** `data/raw/scrapy.log`

**Informa√ß√µes:**
- Requests feitos
- Downloads bem-sucedidos
- Erros e warnings
- Estat√≠sticas de scraping

---

## üß™ Testes

### Testar Spider Individual

```bash
# Testar Anatel
python scripts/run_scrapy_spider.py anatel \
  --dataset-id test_anatel \
  --url "https://data-basis.org/dataset/d3c86a88-d9a4-4c0-bdec-08ab61e8f63c"

# Verificar arquivo baixado
ls -lh data/raw/test_anatel/

# Verificar metadados
cat data/raw/download_metadata.json | jq '.test_anatel'
```

### Testar Todos os Spiders

```bash
# Executar todos
python src/scrapy/run_all_spiders.py

# Verificar resultados
cat data/raw/download_metadata.json
```

---

## üîß Troubleshooting

### Problema 1: Spider n√£o encontra arquivos

**Solu√ß√£o:**
- Verificar se o site mudou estrutura HTML
- Ajustar seletores CSS/XPath
- Adicionar fallbacks adicionais

### Problema 2: Rate Limiting

**Solu√ß√£o:**
- Aumentar `DOWNLOAD_DELAY`
- Habilitar `AUTOTHROTTLE`
- Reduzir `CONCURRENT_REQUESTS`

### Problema 3: Encoding Issues

**Solu√ß√£o:**
- Especificar encoding no spider
- Usar `response.encoding` para detec√ß√£o autom√°tica
- Configurar `FEED_EXPORT_ENCODING` nas settings

---

## üöÄ Pr√≥ximos Passos

### Melhorias Planejadas

1. **Scrapy Middleware Personalizado**
   - Rota√ß√£o de User-Agents
   - Proxy support
   - Retry inteligente

2. **Spider Universal**
   - Spider gen√©rico que detecta tipo de site
   - Auto-configura√ß√£o baseada em URL

3. **Integra√ß√£o com Pipeline Principal**
   - Chamar spiders do pipeline de download
   - Processamento autom√°tico ap√≥s download

4. **Monitoramento**
   - Dashboard de status dos spiders
   - Alertas para falhas
   - Estat√≠sticas de sucesso

---

## ‚úÖ Checklist de Implementa√ß√£o

- [x] AnatelSpider implementado
- [x] InternetAbertaSpider implementado
- [x] SpringerSpider implementado
- [x] GitHubSpider implementado
- [x] Items definidos
- [x] Pipelines implementados
- [x] Settings configurados
- [x] Scripts de execu√ß√£o criados
- [x] Documenta√ß√£o completa

---

**Status:** ‚úÖ **SCRAPY COMPLETO E PRONTO PARA USO**

---

**Nova Corrente Grand Prix SENAI - Demand Forecasting System**


