# ðŸ“Š AnÃ¡lise Detalhada dos Datasets de Treinamento

## Nova Corrente - Demand Forecasting System

---

## ðŸ“ LocalizaÃ§Ã£o dos Datasets de Treinamento

### âœ… `data/training/` - Dados Prontos para ML

**Estrutura Completa:**

```
data/training/
â”œâ”€â”€ unknown_train.csv        â­ 11.61 MB | 93,881 registros (80%)
â”œâ”€â”€ unknown_test.csv            2.90 MB | 23,471 registros (20%)
â”œâ”€â”€ unknown_full.csv           14.51 MB | 117,352 registros (100%)
â”œâ”€â”€ CONN-001_train.csv          0.06 MB | 584 registros (80%)
â”œâ”€â”€ CONN-001_test.csv           0.02 MB | 146 registros (20%)
â”œâ”€â”€ CONN-001_full.csv           0.08 MB | 730 registros (100%)
â”œâ”€â”€ metadata.json               0.00 MB | Metadados
â””â”€â”€ training_summary.json       0.00 MB | Resumo estatÃ­stico
```

### âœ… `data/processed/` - Dados Processados

```
data/processed/
â”œâ”€â”€ unified_dataset_with_factors.csv  â­ 27.25 MB | 118,082 registros
â”œâ”€â”€ zenodo_milan_telecom_preprocessed.csv  9.87 MB | 116,257 registros
â””â”€â”€ test_dataset_preprocessed.csv           0.05 MB | 730 registros
```

---

## ðŸ“Š Dataset 1: Zenodo Milan Telecom (Preprocessed)

### `data/processed/zenodo_milan_telecom_preprocessed.csv`

**EstatÃ­sticas:**
- **Tamanho:** 9.87 MB
- **Registros:** 116,257 linhas
- **Colunas:** 9 colunas (schema unificado)
- **Fonte:** Zenodo (5G network slice resource demand)

**Estrutura de Colunas:**

| Coluna | Tipo | DescriÃ§Ã£o | Origem |
|--------|------|-----------|--------|
| `date` | DateTime | Data/timestamp | `step` (convertido) |
| `item_id` | String | ID do item | `unknown` (default) |
| `quantity` | Float | Quantidade/demanda | `totalSched` (total scheduled traffic) |
| `cost` | Float | Custo unitÃ¡rio | Default (0.0) |
| `lead_time` | Integer | Tempo de entrega | Default (14 dias) |
| `item_name` | String | Nome do item | `unknown` (default) |
| `category` | String | Categoria | `unknown` (default) |
| `site_id` | String | ID do site | `bsId` (base station ID) |
| `dataset_source` | String | Origem do dado | `zenodo_milan_telecom` |

**CaracterÃ­sticas Especiais:**

1. **Date:** Usa `step` como Ã­ndice temporal (1970-01-01 + step)
   - Range: 0 a 116,256 steps
   - Convertido para DateTime timestamp

2. **Quantity:** Mapeado de `totalSched` (Total Admitted Traffic Load)
   - Representa trÃ¡fego total agendado de 5G network slices
   - Valores podem variar significativamente

3. **Site ID:** Mapeado de `bsId` (Base Station ID)
   - Atualmente todos os registros sÃ£o do bsId = 1

**Uso Recomendado:**

```python
# Carregar dataset Zenodo processado
df_zenodo = pd.read_csv('data/processed/zenodo_milan_telecom_preprocessed.csv')

# Preparar para time series
df_zenodo['date'] = pd.to_datetime(df_zenodo['date'])
df_zenodo = df_zenodo.set_index('date')
series = df_zenodo['quantity']

# Treinar modelos ML
# ARIMA, Prophet, LSTM...
```

**ObservaÃ§Ãµes:**
- âš ï¸ **Date Range:** Data comeÃ§a em 1970-01-01 (timestamp base Unix)
  - Isso Ã© devido Ã  conversÃ£o de `step` (Ã­ndice numÃ©rico) para DateTime
  - Para anÃ¡lise temporal, considerar usar `step` diretamente ou recalcular datas base

- âš ï¸ **Quantity:** Muitos valores sÃ£o 0.0 no inÃ­cio
  - Pode indicar inÃ­cio de episÃ³dio ou dados nÃ£o inicializados
  - Filtrar zeros ou usar apenas valores > 0

---

## ðŸ“Š Dataset 2: Unknown (Training Split)

### `data/training/unknown_train.csv` â­ MAIOR

**EstatÃ­sticas:**
- **Tamanho:** 11.61 MB
- **Registros:** 93,881 linhas (80% de treino)
- **Colunas:** 31 colunas (com external factors)

**Estrutura de Colunas:**

**Colunas Base (9):**
- `date`, `item_id`, `item_name`, `quantity`, `site_id`, `category`, `cost`, `lead_time`, `dataset_source`

**External Factors (22):**
- **ClimÃ¡ticos:** `temperature`, `precipitation`, `humidity`, `extreme_heat`, `heavy_rain`, `high_humidity`
- **EconÃ´micos:** `exchange_rate_brl_usd`, `inflation_rate`, `gdp_growth`, `high_inflation`, `currency_devaluation`
- **RegulatÃ³rios:** `5g_coverage`, `regulatory_compliance_date`, `5g_expansion_rate`
- **Operacionais:** `is_holiday`, `is_vacation_period`, `sla_renewal_period`, `weekend`
- **Scores:** `climate_impact`, `economic_impact`, `operational_impact`, `demand_adjustment_factor`

**ComposiÃ§Ã£o:**
- **98.5%** dos dados vÃªm do Zenodo Milan Telecom
- **1.5%** de outros datasets (retail inventory, supply chain, test)

**Uso Recomendado:**

```python
# Carregar dataset de treino (maior)
train_df = pd.read_csv('data/training/unknown_train.csv')
test_df = pd.read_csv('data/training/unknown_test.csv')

# Preparar para ML
train_df['date'] = pd.to_datetime(train_df['date'])
test_df['date'] = pd.to_datetime(test_df['date'])

# Features para ML
features = ['quantity', 'temperature', 'precipitation', 'inflation_rate', 
           'is_holiday', 'weekend', 'climate_impact', 'economic_impact']
target = 'quantity'

X_train = train_df[features]
y_train = train_df[target]
X_test = test_df[features]
y_test = test_df[target]

# Treinar modelos...
```

### `data/training/unknown_test.csv`

**EstatÃ­sticas:**
- **Tamanho:** 2.90 MB
- **Registros:** 23,471 linhas (20% de teste)
- **Colunas:** 31 colunas

**Uso:** Para avaliaÃ§Ã£o de modelos ML (validaÃ§Ã£o)

---

## ðŸ“Š Dataset 3: CONN-001 (Training Split)

### `data/training/CONN-001_train.csv`

**EstatÃ­sticas:**
- **Tamanho:** 0.06 MB
- **Registros:** 584 linhas (80% de treino)
- **Colunas:** 31 colunas

**Origem:** `test_dataset` (730 registros totais)

**CaracterÃ­sticas:**
- Dataset menor, ideal para testes rÃ¡pidos
- PerÃ­odo: 2 anos (2023-01-01 a 2024-12-30)
- Item: CONN-001 (Conectores)
- Site: TORRE001

**Uso Recomendado:**

```python
# Dataset menor para testes rÃ¡pidos
train_small = pd.read_csv('data/training/CONN-001_train.csv')
test_small = pd.read_csv('data/training/CONN-001_test.csv')

# Ideal para:
# - Testes de algoritmos
# - ValidaÃ§Ã£o rÃ¡pida
# - Prototipagem
```

### `data/training/CONN-001_test.csv`

**EstatÃ­sticas:**
- **Tamanho:** 0.02 MB
- **Registros:** 146 linhas (20% de teste)
- **Colunas:** 31 colunas

---

## ðŸ“Š Dataset 4: Test Dataset (Raw)

### `data/raw/test_dataset/test_data.csv`

**EstatÃ­sticas:**
- **Tamanho:** ~37 KB
- **Registros:** 730 linhas
- **Colunas:** 7 colunas originais

**Estrutura Original:**

| Coluna | Tipo | DescriÃ§Ã£o |
|--------|------|-----------|
| `Date` | Date | Data (2023-01-01 a 2024-12-30) |
| `Product` | String | ID do produto (CONN-001) |
| `Order_Demand` | Integer | Demanda diÃ¡ria (4-10 unidades) |
| `Site` | String | ID do site (TORRE001) |
| `Category` | String | Categoria (Conectores) |
| `Cost` | Float | Custo unitÃ¡rio (300.0) |
| `Lead_Time` | Integer | Tempo de entrega (14 dias) |

**CaracterÃ­sticas:**
- âœ… Dados sintÃ©ticos/guiados para teste
- âœ… PerÃ­odo: 2 anos completos (730 dias)
- âœ… Demanda diÃ¡ria: Varia de 4 a 10 unidades
- âœ… Sem gaps: Dados contÃ­nuos para todos os dias

**Uso:** Dataset de referÃªncia para validaÃ§Ã£o e testes

---

## ðŸ”„ Mapeamento de Dados

### Zenodo Milan Telecom â†’ Preprocessed

**Original (38 colunas):**
- `bsId`, `episode`, `step`, `loadSMS`, `loadInt`, `loadCalls`, `totalSched`, etc.

**Processado (9 colunas unificadas):**
- `step` â†’ `date` (convertido para DateTime)
- `bsId` â†’ `site_id`
- `totalSched` â†’ `quantity`
- Defaults: `item_id` = "unknown", `category` = "unknown"

### Test Dataset â†’ Preprocessed

**Original (7 colunas):**
- `Date`, `Product`, `Order_Demand`, `Site`, `Category`, `Cost`, `Lead_Time`

**Processado (9 colunas unificadas):**
- `Date` â†’ `date`
- `Product` â†’ `item_id`
- `Order_Demand` â†’ `quantity`
- `Site` â†’ `site_id`
- `Category` â†’ `category`
- `Cost` â†’ `cost`
- `Lead_Time` â†’ `lead_time`

---

## ðŸ“ MatemÃ¡tica por TrÃ¡s dos Datasets

### AgregaÃ§Ã£o DiÃ¡ria

Quando dados sÃ£o agregados para nÃ­vel diÃ¡rio:

$$D_{daily} = \sum_{i=1}^{H} D_{hourly,i}$$

onde $H$ Ã© o nÃºmero de horas/dia (ex: 24).

**No nosso caso (Zenodo):**
- Dados originais: 116,257 steps/episÃ³dios
- ApÃ³s preprocessing: 116,257 registros diÃ¡rios
- NÃ£o houve agregaÃ§Ã£o (jÃ¡ estava em nÃ­vel step)

### Split Train/Test

**DivisÃ£o Temporal (Time Series):**

$$n_{train} = \lfloor 0.8 \times n \rfloor$$

$$n_{test} = n - n_{train}$$

onde $n$ Ã© o total de registros.

**Para unknown dataset:**
- $n = 117,352$
- $n_{train} = \lfloor 0.8 \times 117,352 \rfloor = 93,881$
- $n_{test} = 117,352 - 93,881 = 23,471$

**âš ï¸ Importante:** Split Ã© temporal (nÃ£o aleatÃ³rio) para sÃ©ries temporais!

### NormalizaÃ§Ã£o (para LSTM)

**Min-Max Scaling:**

$$X_{normalized} = \frac{X - X_{min}}{X_{max} - X_{min}}$$

**Aplicado em:**
- `quantity` (demanda)
- `temperature`, `precipitation`, etc.

---

## ðŸŽ¯ Uso PrÃ¡tico dos Datasets

### 1. Para Treinar Modelos ML (Usar unknown_train.csv)

```python
import pandas as pd
from statsmodels.tsa.arima.model import ARIMA
from prophet import Prophet
import tensorflow as tf

# Carregar dados de treino (MAIOR)
train_df = pd.read_csv('data/training/unknown_train.csv')
test_df = pd.read_csv('data/training/unknown_test.csv')

# Preparar time series
train_df['date'] = pd.to_datetime(train_df['date'])
train_df = train_df.set_index('date')
series = train_df['quantity']

# ARIMA
model_arima = ARIMA(series, order=(1,1,1))
model_arima = model_arima.fit()
forecast_arima = model_arima.forecast(steps=30)

# Prophet
prophet_df = train_df.reset_index()[['date', 'quantity']].rename(
    columns={'date': 'ds', 'quantity': 'y'}
)
model_prophet = Prophet()
model_prophet.fit(prophet_df)
future = model_prophet.make_future_dataframe(periods=30)
forecast_prophet = model_prophet.predict(future)

# LSTM (precisa normalizaÃ§Ã£o)
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
series_scaled = scaler.fit_transform(series.values.reshape(-1, 1))
# ... treinar LSTM
```

### 2. Para Testes RÃ¡pidos (Usar CONN-001_train.csv)

```python
# Dataset menor para prototipagem rÃ¡pida
train_small = pd.read_csv('data/training/CONN-001_train.csv')
test_small = pd.read_csv('data/training/CONN-001_test.csv')

# Mesmo processo, mas mais rÃ¡pido
```

### 3. Para AnÃ¡lise Completa (Usar unified_dataset_with_factors.csv)

```python
# Dataset completo com todos os dados
df_full = pd.read_csv('data/processed/unified_dataset_with_factors.csv')

# AnÃ¡lise exploratÃ³ria completa
# AgregaÃ§Ãµes, visualizaÃ§Ãµes, etc.
```

---

## ðŸ“Š EstatÃ­sticas Detalhadas

### Unknown Dataset (Training)

**Train Set:**
- **93,881 registros** (80%)
- **31 colunas** (9 base + 22 external factors)
- **11.61 MB**

**Test Set:**
- **23,471 registros** (20%)
- **31 colunas**
- **2.90 MB**

**ComposiÃ§Ã£o por Fonte:**
- **Zenodo Milan Telecom:** ~98.5%
- **Kaggle Retail Inventory:** ~0.6%
- **Test Dataset:** ~0.6%
- **Kaggle Supply Chain:** ~0.3%

### CONN-001 Dataset (Training)

**Train Set:**
- **584 registros** (80%)
- **31 colunas**
- **0.06 MB**

**Test Set:**
- **146 registros** (20%)
- **31 colunas**
- **0.02 MB**

**Origem:** 100% do test_dataset original

---

## ðŸš€ PrÃ³ximos Passos com os Datasets

### 1. Preparar para Modelos ML

```python
# 1. Carregar dados
train = pd.read_csv('data/training/unknown_train.csv')
test = pd.read_csv('data/training/unknown_test.csv')

# 2. Preparar features
train['date'] = pd.to_datetime(train['date'])
train = train.set_index('date')

# 3. Selecionar features
features = ['quantity', 'temperature', 'precipitation', 
           'inflation_rate', 'is_holiday', 'weekend']
target = 'quantity'

X_train = train[features]
y_train = train[target]
```

### 2. Treinar Modelos

- **ARIMA:** Para baseline
- **Prophet:** Para sazonalidade
- **LSTM:** Para padrÃµes complexos
- **Ensemble:** CombinaÃ§Ã£o otimizada

### 3. Avaliar Modelos

```python
from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error
import numpy as np

# MÃ©tricas
mape = mean_absolute_percentage_error(y_test, forecast)
rmse = np.sqrt(mean_squared_error(y_test, forecast))

print(f"MAPE: {mape:.2f}%")
print(f"RMSE: {rmse:.2f}")
```

---

## âœ… Resumo

### Datasets Principais de Treinamento:

1. **`unknown_train.csv`** â­ **USE ESTE!**
   - 93,881 registros
   - 31 colunas (com external factors)
   - 11.61 MB
   - **Ideal para treinar modelos ML completos**

2. **`CONN-001_train.csv`**
   - 584 registros
   - 31 colunas
   - 0.06 MB
   - **Ideal para testes rÃ¡pidos e prototipagem**

3. **`zenodo_milan_telecom_preprocessed.csv`**
   - 116,257 registros
   - 9 colunas (base)
   - 9.87 MB
   - **Dados especÃ­ficos do Zenodo (antes de merge)**

4. **`unified_dataset_with_factors.csv`**
   - 118,082 registros
   - 31 colunas (completo)
   - 27.25 MB
   - **Dataset completo final (todos os dados mesclados)**

---

**Status:** âœ… **Todos os datasets prontos para ML training!**

**RecomendaÃ§Ã£o:** Use `unknown_train.csv` para treinar modelos e `CONN-001_train.csv` para testes rÃ¡pidos.

---

**Nova Corrente Grand Prix SENAI - Demand Forecasting System**

---

## ðŸ” DEEP ANALYSIS: ALL RAW DATASETS BY SUB-FOLDER

### Comprehensive Dataset Context from Web Research & Data Exploration

---

## ðŸ“Š **1. `data/raw/test_dataset/` - Primary Time-Series Foundation**

### **File:** `test_data.csv` (37 KB, 732 rows)

**Detailed Schema:**
- `Date` - Daily timestamps from 2023-01-01 to 2024-12-30 (complete 2-year period)
- `Product` - Fixed identifier: "CONN-001"
- `Order_Demand` - Integer demand (range: 3-11 units, mean: 6.93, std: 2.51)
- `Site` - Fixed location: "TORRE001"
- `Category` - Fixed category: "Conectores"
- `Cost` - Fixed unit cost: 300.0 BRL
- `Lead_Time` - Fixed lead time: 14 days

**Deep Context:**
This dataset represents **synthetic/guided test data** designed specifically for pipeline validation in a telecommunications tower maintenance context. The "CONN-001" product represents telecom connectors, critical infrastructure components for cell towers. The consistent demand pattern (3-11 units daily) with no seasonality suggests steady baseline maintenance requirements for a single tower location.

**Business Intelligence:**
- **Purpose:** Benchmarking forecasting models
- **Reality Check:** Clean, controlled data for algorithm testing
- **Scale:** Single site, single product, 2-year horizon
- **Use Case:** Validation environment before production deployment

**Statistical Profile:**
```
Mean Demand: 6.93 units/day
Standard Deviation: 2.51 units
Coefficient of Variation: 0.36 (moderate variability)
Range: 8 units (min 3, max 11)
```

---

## ðŸ“Š **2. `data/raw/zenodo_broadband_brazil/` - Brazilian Broadband User Quality**

### **File:** `BROADBAND_USER_INFO.csv` (59 KB, 2,044 rows)

**Detailed Schema:**
- `Customer_ID` - Unique customer identifier
- `Latency` - Milliseconds (0-24.4ms, with many 0s indicating disconnects)
- `Jitter` - Packet jitter in ms (2.36-4.03ms range)
- `Packet_Loss` - Percentage of lost packets
- `Channel2_quality` - WiFi 2.4GHz quality (0-5 scale)
- `Channel5_quality` - WiFi 5GHz quality (0-5 scale)
- `N_distant_devices` - Number of interfering devices nearby
- `CRM_Complaint?` - Binary flag indicating customer complaints

**Deep Context:**
**Source:** Real data extracted from a Brazilian broadband operator via Zenodo research repository. This dataset represents **customer service quality metrics** collected from modem telemetry, including signal strength, network performance, and environmental interference factors.

**Quality Scores Interpretation:**
- **5** = Excellent signal
- **4** = Good signal
- **3** = Acceptable signal
- **2** = Poor signal
- **1** = Very poor signal
- **0** = No connection

**Business Intelligence:**
- **Use Case:** Predictive maintenance of customer premises equipment (CPE)
- **Target Variable:** `CRM_Complaint?` (1 = complaint filed)
- **Features:** Technical quality metrics that predict customer dissatisfaction
- **Brazilian Context:** Operating in challenging topography, weather conditions, and regulatory environment

**Analytical Applications:**
1. **Churn Prediction:** Link quality degradation â†’ customer complaints â†’ churn risk
2. **Infrastructure Planning:** Poor quality clusters â†’ tower expansion needs
3. **Maintenance Prioritization:** High jitter + packet loss â†’ equipment replacement urgency

**Statistical Insights:**
```
Latency: Many zeros (network disconnects) + continuous values (0-24.4ms)
Jitter: Tight distribution (2.36-4.03ms) suggests stable infrastructure
Packet Loss: Critical metric - correlates with CRM complaints
Distant Devices: Interference proxy - affects channel quality
```

---

## ðŸ“Š **3. `data/raw/zenodo_milan_telecom/` - 5G Network Slice Resource Demand**

### **File:** `output-step-bsId_1-2023_9_28_12_50_10.csv` (28.7 MB, 116,257 rows, 38 columns)

**Deep Context:**
**Source:** Zenodo MILANO dataset post-processed for 5G network slice management research. This represents **42 episodes of a game-theoretic decentralized AI consensus admission control algorithm** applied to cellular network resource allocation.

**Original Research Context:**
- **Timeline:** November 2013 to January 2014 (aggregated hourly over 300mÂ² areas in Milan)
- **Transformation:** Post-processed to support machine learning for 5G slice demand prediction
- **Episodes:** Each episode represents one optimization cycle of the admission control algorithm

**Detailed Column Breakdown:**

#### **Core Identity**
- `bsId` - Base Station ID (all = 1, single tower simulation)
- `episode` - Episode index (0-41, total 42 episodes)
- `step` - Temporal step within episode (sequential index)

#### **Traffic Load Metrics (3 services Ã— 4 states)**
**SMS Traffic:**
- `loadSMS` - Total incoming/outgoing SMS traffic load
- `rawActSMS` - Raw network packets for SMS
- `schedSMS` - **Admitted** SMS traffic (accepted by admission control)
- `schedDelaySMS` - **Queued** SMS traffic (delayed admission)
- `dropSMS` - **Dropped** SMS traffic (rejected)
- `defSMS` / `rawActDefSMS` - **Deferred** SMS traffic

**Internet Traffic:**
- `loadInt` - Internet traffic load
- `rawActInt` - Raw internet packets
- `schedInt` - Admitted internet traffic
- `schedDelayInt` - Queued internet traffic
- `dropInt` - Dropped internet traffic
- `defInt` / `rawActDefInt` - Deferred internet traffic

**Voice Call Traffic:**
- `loadCalls` - Voice call traffic load
- `rawActCalls` - Raw voice call packets
- `schedCalls` - Admitted call traffic
- `schedDelayCalls` - Queued call traffic
- `dropCalls` - Dropped call traffic
- `defCalls` / `rawActDefCalls` - Deferred call traffic

#### **Capacity & Performance**
- `totalSched` - **Total admitted traffic load** â­ (USED AS DEMAND TARGET)
- `bsCap` - Base Station capacity (fixed at 33.912)
- `totalDropped` - Total dropped traffic

#### **Quality Metrics**
**Reject Rates (per service):**
- `rejectRate` - Overall rejection rate
- `rejectRateCalls` - Voice call rejection rate
- `rejectRateInt` - Internet rejection rate
- `rejectRateSMS` - SMS rejection rate

**Delay Rates (per service):**
- `delayRate` - Overall delay rate
- `delayRateCalls` - Voice call delay rate
- `delayRateInt` - Internet delay rate
- `delayRateSMS` - SMS delay rate

#### **Algorithm Rewards**
- `reward` - Total game reward value (reinforcement learning signal)
- `episodeReward` - Cumulative reward per episode
- `avgEpisodeReward` - Average reward across episodes

**Business Intelligence:**
This dataset simulates **dynamic resource allocation** in 5G networks using multi-agent game theory. The admission control algorithm decides which traffic to accept, queue, or reject based on available capacity and priority policies.

**For Nova Corrente Use Case:**
- **Target Variable:** `totalSched` (admitted traffic) â†’ mapped to `quantity` (demand)
- **Capacity Planning:** Predict future `totalSched` â†’ tower upgrade needs
- **Quality Assurance:** Monitor `rejectRate` and `delayRate` â†’ network performance degradation
- **Episode Context:** Each episode represents different network conditions/scenarios

**Statistical Profile:**
```
totalSched range: 0.0 to ~40 units (simulated load units)
Multiple zeros indicate episode initialization or capacity exhaustion
Episode patterns show resource allocation dynamics over time
```

---

## ðŸ“Š **4. `data/raw/kaggle_equipment_failure/` - Predictive Maintenance AI Competition**

### **File:** `ai4i2020.csv` (10,002 rows, 14 columns)

**Deep Context:**
**Source:** AI4I 2020 Predictive Maintenance Dataset from Kaggle. This is a **binary classification** dataset designed to predict equipment failure before it occurs using sensor telemetry.

**Detailed Schema:**
- `UDI` - Unique identifier (1-10,000)
- `Product ID` - Product code (M14860, L47181, etc.)
- `Type` - Operation type: L (Low), M (Medium), H (High) - correlates with operational intensity
- `Air temperature [K]` - Ambient temperature in Kelvin (~298-300K range)
- `Process temperature [K]` - Operating process temperature (~308-310K range)
- `Rotational speed [rpm]` - Tool rotational speed (varies significantly: 1,000-3,000 rpm)
- `Torque [Nm]` - Applied torque (typical: 20-60 Nm)
- `Tool wear [min]` - Accumulated wear time (0-250 minutes)
- `Machine failure` - **Target variable:** Binary (0=normal, 1=failure)
- `TWF` - Tool Wear Failure flag
- `HDF` - Heat Dissipation Failure flag
- `PWF` - Power Failure flag
- `OSF` - Overstrain Failure flag
- `RNF` - Random Failure flag

**Deep Context:**
This dataset represents **multivariate classification** for predictive maintenance in industrial equipment. Each failure mode has distinct sensor signatures:

**Failure Mode Physics:**
1. **TWF (Tool Wear Failure):** High torque + high wear â†’ tool degradation
2. **HDF (Heat Dissipation):** High process temp + low air temp â†’ overheating
3. **PWF (Power Failure):** Abrupt torque/speed drops â†’ electrical issues
4. **OSF (Overstrain):** Extreme rotational speed â†’ mechanical stress
5. **RNF (Random):** Unpredictable events

**Business Intelligence:**
- **Use Case:** Preventative maintenance scheduling
- **Cost Savings:** Avoid unplanned downtime (expensive)
- **Spare Parts:** Order replacement tools before failure
- **Nova Corrente Context:** Apply to telecom tower equipment (power systems, cooling, RF equipment)

**Feature Engineering Insights:**
```python
# Derived features often used in competitions:
temp_diff = process_temp - air_temp  # Heat dissipation indicator
torque_times_wear = torque * tool_wear  # Degradation proxy
speed_anomaly = abs(rotational_speed - mean_speed)  # Outlier detection
```

**Statistical Profile:**
```
Failure Rate: ~3.4% (typical for reliable equipment)
Imbalanced Dataset: Requires SMOTE or class weighting
Most failures: TWF (wear-based degradation)
```

---

## ðŸ“Š **5. `data/raw/kaggle_telecom_network/` - Tower Capacity Forecasting**

### **File:** `Telecom_Network_Data.csv`

**Detailed Schema:**
- `timestamp` - Hourly timestamps
- `tower_id` - Unique identifier for cell tower
- `users_connected` - Number of concurrent users â­ (DEMAND PROXY)
- `download_speed` - Mbps throughput
- `upload_speed` - Mbps upload rate
- `latency` - Network delay in ms
- `weather` - Categorical: Clear, Rain, Snow, Storm
- `congestion` - Binary flag (0=normal, 1=congested)

**Deep Context:**
**Source:** Synthetic telecom network data simulating hourly tower load patterns with weather impact. This dataset models **capacity planning** for cellular infrastructure under varying demand and environmental conditions.

**Business Intelligence:**
- **Demand Proxy:** `users_connected` represents network load (traffic demand)
- **Capacity Limits:** Congestion flag (1) indicates tower at capacity
- **Weather Impact:** Storms â†’ latency spikes â†’ customer complaints
- **Use Case:** Tower expansion planning, capacity upgrades

**Analytical Applications:**
1. **Load Forecasting:** Predict peak `users_connected` â†’ infrastructure investment
2. **Weather Resilience:** Storm days â†’ backup capacity needs
3. **Congestion Prediction:** Early warning system for capacity exhaustion

**Statistical Insights:**
```
Users: Highly variable (0-1000 range, mean ~500)
Weather: Categorical with clear days dominating
Congestion: Rare events (~5-10% of observations)
Strong correlation: Weather â†’ Latency â†’ Congestion
```

---

## ðŸ“Š **6. `data/raw/kaggle_supply_chain/` - High-Dimensional Supply Chain**

### **File:** `supply_chain_dataset1.csv` (91,252 rows)

**Detailed Schema:**
- `Date` - Daily timestamps
- `SKU_ID` - Stock Keeping Unit identifier
- `Warehouse_ID` - Multi-warehouse network
- `Supplier_ID` - Supplier identifier
- `Region` - Geographic region (North, South, East, West)
- `Units_Sold` - Actual demand quantity
- `Inventory_Level` - Current stock
- `Supplier_Lead_Time_Days` - Procurement lead time
- `Reorder_Point` - Inventory threshold for reordering
- `Order_Quantity` - Quantity to reorder
- `Unit_Cost` - Cost per unit
- `Unit_Price` - Selling price per unit
- `Promotion_Flag` - Binary promotion indicator
- `Stockout_Flag` - Binary stockout event
- `Demand_Forecast` - Predicted demand

**Deep Context:**
This dataset models **complex supply chain dynamics** with multiple echelons (warehouses, suppliers), geographic dispersion, and promotional impacts. The `Demand_Forecast` column provides a **baseline forecast** to improve upon.

**Business Intelligence:**
- **Multi-Echelon:** Warehouse â†’ Supplier coordination
- **Geographic Variability:** Regional demand patterns differ
- **Promotion Effects:** Price changes â†’ demand spikes
- **Stockout Costs:** Lost sales = revenue impact
- **Lead Time:** Critical for inventory optimization

**Analytical Applications:**
1. **Forecast Accuracy:** `Demand_Forecast` vs `Units_Sold` â†’ ML improvement opportunity
2. **Stockout Prediction:** Early warning system
3. **Promotion Optimization:** Price elasticity analysis
4. **Network Balancing:** Warehouse inventory redistribution

**Statistical Profile:**
```
High Dimensionality: 15 columns
Multi-SKU: Thousands of products
Multi-Location: Multiple warehouses
Promotion Lift: +20-50% demand spikes
Stockout Frequency: 2-5% of periods
```

---

## ðŸ“Š **7. `data/raw/kaggle_daily_demand/` - Multi-Sector Daily Orders**

### **File:** `Daily Demand Forecasting Orders.csv` (62 rows, 13 columns)

**Detailed Schema:**
- Week of month (1-5)
- Day of week (2-6, Monday-Friday, business days only)
- `Non-urgent order` - Regular business orders
- `Urgent order` - Expedited orders
- `Order type A` - Product/service category A
- `Order type B` - Product/service category B
- `Order type C` - Product/service category C
- `Fiscal sector orders` - Government orders
- `Orders from the traffic controller sector` - Transportation/logistics
- `Banking orders (1)` - Financial institution orders
- `Banking orders (2)` - Different banking segment
- `Banking orders (3)` - Third banking segment
- `Target (Total orders)` - **Aggregate demand** â­

**Deep Context:**
**Source:** Lightweight MVP dataset for daily order forecasting. Only 62 rows representing approximately 12 weeks of business days (excluding weekends). This dataset has **minimal history** making it suitable for simple baselines but not deep learning.

**Business Intelligence:**
- **B2B Focus:** Professional services, not retail
- **Sector Diversity:** Fiscal, banking, traffic control
- **Urgency Tiers:** Service level priorities
- **Aggregation:** Target = sum of all order types

**Analytical Applications:**
1. **Multi-Variate Decomposition:** Forecast each order type separately
2. **Urgency Analysis:** Peak load management
3. **Sector Correlation:** Fiscal events â†’ banking activity

**Statistical Constraints:**
```
Very Small: 62 rows = ~3 months of business days
Insufficient for: Seasonal patterns, long-term trends
Suitable for: Simple time-series (ARIMA), baseline comparison
```

---

## ðŸ“Š **8. `data/raw/kaggle_logistics_warehouse/` - Inventory KPIs**

### **File:** `logistics_dataset.csv` (3,204 rows, 24 columns)

**Detailed Schema:**

**Identity & Categorization:**
- `item_id` - ITM10000, ITM10001, etc.
- `category` - Pharma, Automotive, Groceries, Apparel, Electronics

**Inventory State:**
- `stock_level` - Current units in stock
- `reorder_point` - Trigger for reordering
- `reorder_frequency_days` - Days between reorders
- `lead_time_days` - Supplier lead time

**Demand Metrics:**
- `daily_demand` - Average daily demand
- `demand_std_dev` - Demand variability
- `item_popularity_score` - Customer preference (0-1)
- `forecasted_demand_next_7d` - 7-day forecast

**Logistics Efficiency:**
- `storage_location_id` - L82, L15, etc.
- `zone` - A, B, C, D (warehouse zoning)
- `picking_time_seconds` - Order fulfillment time
- `handling_cost_per_unit` - Operational cost
- `layout_efficiency_score` - Warehouse layout quality (0-1)

**Financial:**
- `unit_price` - Selling price
- `holding_cost_per_unit_day` - Inventory carrying cost
- `unit_price` - Product value

**Performance KPIs:**
- `stockout_count_last_month` - Service failures
- `order_fulfillment_rate` - Success rate (0-1)
- `total_orders_last_month` - Demand volume
- `turnover_ratio` - Inventory velocity
- `KPI_score` - Overall performance metric (0-1)

**Deep Context:**
**Source:** Comprehensive warehouse operations dataset with **rich metadata** for each SKU. This dataset enables **multi-criteria optimization** balancing cost, service, and efficiency.

**Business Intelligence:**
- **ABC Analysis:** `popularity_score` + `turnover_ratio` â†’ inventory prioritization
- **Layout Optimization:** `layout_efficiency_score` â†’ warehouse redesign ROI
- **Service Level:** `fulfillment_rate` â†’ customer satisfaction proxy
- **Cost Optimization:** Balance holding costs vs stockouts

**Analytical Applications:**
1. **SKU Clustering:** Group similar items for shared strategies
2. **Demand Uncertainty:** High `demand_std_dev` â†’ safety stock adjustment
3. **Zone Optimization:** Slow pickers in Zone D â†’ layout redesign
4. **Forecast Evaluation:** `forecasted_demand` vs `daily_demand` â†’ ML accuracy

**Statistical Profile:**
```
Multi-Category: 5 product types
Multi-Zone: 4 warehouse zones
High Variability: demand_std_dev ranges widely
Balance Metrics: KPI_score synthesis of multiple factors
```

---

## ðŸ“Š **9. `data/raw/kaggle_retail_inventory/` - Retail Operations with Weather**

### **File:** `retail_store_inventory.csv` (73,000+ rows, 15 columns)

**Detailed Schema:**

**Temporal & Location:**
- `Date` - Daily timestamps
- `Store ID` - S001, S002, etc. (multiple stores)
- `Region` - North, South, East, West

**Product:**
- `Product ID` - P0001, P0002, etc. (hundreds of SKUs)
- `Category` - Groceries, Toys, Electronics, Furniture, Clothing

**Inventory & Demand:**
- `Inventory Level` - Stock on hand
- `Units Sold` - Actual demand
- `Units Ordered` - Procurement orders
- `Demand Forecast` - Predicted demand

**Pricing:**
- `Price` - Selling price
- `Discount` - Promotion percentage (0-20%)
- `Competitor Pricing` - Market rate

**External Factors:**
- `Weather Condition` - Clear, Rainy, Snowy, Storm
- `Holiday/Promotion` - Binary flag
- `Seasonality` - Spring, Summer, Autumn, Winter

**Deep Context:**
**Source:** Synthetic retail operations data simulating **multi-store, multi-SKU** retail chain with weather impacts, seasonality, and competitive dynamics.

**Business Intelligence:**
- **Demand Drivers:** Weather, holidays, promotions, competition
- **Cross-Store:** Regional patterns differ
- **Category Dynamics:** Electronics vs Groceries demand differently
- **Competitive:** Price sensitivity from competitor pricing

**Analytical Applications:**
1. **Weather Impact:** Storm days â†’ different category mix
2. **Promotion Analysis:** Discount lift by category
3. **Regional Clustering:** North vs South customer behavior
4. **Multi-Level Forecasting:** Store â†’ Region â†’ Chain

**Statistical Profile:**
```
Large Scale: 73K+ rows
Multi-Store: Multiple locations
Multi-Category: 5 product types
Rich Features: 15 columns
Weather Integration: Real external factor
```

---

## ðŸ“Š **10. `data/raw/github_5g3e/` - Prometheus Infrastructure Metrics**

### **Files:** `server_1.csv`, `server_2.csv`, `server_3.csv` (767+ columns each)

**Deep Context:**
**Source:** GitHub 5G3E virtualized infrastructure dataset. **Prometheus CSV export** format from containerized 5G infrastructure monitoring system. Each file represents one server in a 3-node cluster.

**Format Challenge:**
This is **wide-form time-series** with 767+ columns representing **high-dimensional multivariate** monitoring metrics exported from Prometheus (CNCF monitoring standard).

**Metric Categories:**
- **CPU:** node_cpu_seconds_total (32 CPUs Ã— multiple modes: user, system, idle, iowait, etc.)
- **Memory:** node_memory_* (free, cached, buffers, active, inactive, etc.)
- **Disk I/O:** node_disk_* (read/write bytes, ops, latency per disk)
- **Network:** node_network_* (bytes in/out, packets in/out, errors, drops per interface)
- **System Load:** load1, load5, load15
- **Thermal Sensors:** Core temperatures
- **File System:** Mount points, space usage

**Deep Context:**
This dataset represents **production-grade infrastructure monitoring** for 5G virtualized network functions. Each server hosts multiple containers running 5G core network components (AMF, SMF, UPF, etc.).

**Business Intelligence:**
- **Capacity Planning:** CPU/memory trends â†’ server upgrade needs
- **Anomaly Detection:** Thermal spikes â†’ hardware failure warning
- **Network Bottlenecks:** Interface errors â†’ network configuration issues
- **Resource Optimization:** Right-sizing container allocations

**Analytical Challenges:**
1. **Dimensionality:** 767+ features â†’ curse of dimensionality
2. **Missing Values:** Sparse matrix from metric collection
3. **Parsing:** Prometheus format â†’ requires custom parser
4. **Feature Selection:** Identify relevant metrics for forecasting

**Preprocessing Requirements:**
```python
# Would need specialized Prometheus parser:
prometheus_df = pd.read_csv('server_1.csv', parse_dates=['timestamp'])
# Extract key metrics:
cpu_usage = prometheus_df['node_cpu_seconds_total{...}']
memory_free = prometheus_df['node_memory_MemAvailable_bytes']
network_throughput = prometheus_df['node_network_receive_bytes_total{...}']
# Aggregate to hourly/daily:
hourly_metrics = prometheus_df.resample('H').mean()
```

**Statistical Profile:**
```
Extreme Width: 767+ columns
High Frequency: Likely minute-level sampling
Multi-Nodal: 3 servers in cluster
Production Realism: Real-world infrastructure telemetry
```

---

## ðŸ“Š **11. `data/raw/github_network_fault/` - Telstra Network Fault Classification**

### **Files:** 
- `event_type.csv` - Event classifications
- `log_feature.csv` - 58,673 rows (id, log_feature, volume)
- `feature_Extracted_test_data.csv` - Test features
- `feature_Extracted_trian_data.csv` - Training features  
- `processed_test1.csv` - Preprocessed test data

**Deep Context:**
**Source:** Telstra network fault detection competition from Kaggle. This is a **multi-class classification** challenge to predict fault severity levels in telecommunications infrastructure based on event logs.

**File Structure:**
1. **event_type.csv:** Event type mappings (11, 15, 20, etc.)
2. **log_feature.csv:** Aggregated log volumes per event ID
3. **feature_Extracted_***:** Extracted features ready for ML
4. **processed_test1:** Cleaned, prepared data

**Business Intelligence:**
- **Target:** Fault severity prediction (low, medium, high)
- **Use Case:** Prioritize network maintenance tickets
- **Source:** Telstra (Australian telco) infrastructure logs
- **Competition:** Kaggle challenge with evaluation metrics

**Feature Extraction Insight:**
The `log_feature.csv` shows **log volume aggregations** indicating the granularity of preprocessing:
```
feature_68: SMS-related logs
feature_172: Network protocol logs
feature_201: Critical system alerts
feature_80: Connection logs
```

**Analytical Applications:**
1. **Severity Prediction:** Binary/multi-class classification
2. **Event Correlation:** Co-occurring events â†’ cascade failures
3. **Log Mining:** Pattern detection in unstructured logs
4. **Priority Queue:** Sort maintenance by predicted severity

**Statistical Profile:**
```
Imbalanced: Rare high-severity events
Multi-Modal: Different event types
High Dimensionality: Extracted features
Temporal: Time-series of fault events
```

---

## ðŸ“Š **12-15. Brazilian National Datasets**

### **12. `data/raw/anatel_mobile_brazil/` - Anatel Regulatory Data**
**File:** `d3c86a88-d9a4-4c0-bdec-08ab61e8f63c` (58 KB, HTML/JSON format)

**Deep Context:**
**Source:** Anatel (Brazilian telecom regulator) via Data Basis portal. **Requires parsing** from HTML/JSON format to structured CSV. Contains **official mobile access statistics** by technology (GSM, 4G, 5G), region, and time period.

**Business Intelligence:**
- **Regulatory Authority:** Official Brazil telecom statistics
- **Market Dynamics:** Technology migration trends (2Gâ†’4Gâ†’5G)
- **Regional Analysis:** State/municipality breakdown
- **Investment Signals:** 5G adoption â†’ tower upgrades needed

**Data Contents:**
- Mobile access statistics (historical trends)
- Technology breakdown (GSM, LTE, 5G subscriber counts)
- Regional distribution
- Market concentration metrics

---

### **13. `data/raw/internet_aberta_forecast/` - Academic Forecast Study**
**File:** `Paper-1-EN-Data-Traffic-Demand-Forecast-for-Brazil.pdf` (789 KB)

**Deep Context:**
**Source:** Internet Aberta academic research paper. **Requires PDF extraction** to access embedded data tables. Presents **top-down forecasts** for Brazil internet growth through 2033.

**Research Claims:**
- Data traffic growth: 297 to 400 exabytes by 2033
- Correlation with GDP and consumption patterns
- PrevalÃªncia 4G/5G adoption curves
- Infrastructure investment projections

**Use Case:**
- Long-term planning (10+ year horizon)
- Investment decision support
- Strategic capacity planning
- Policy analysis

**Technical Requirement:** PDF parsing with `pdfplumber` or `tabula-py`

---

### **14. `data/raw/springer_digital_divide/` - Urban-Rural Connectivity**
**File:** `s13688-024-00508-8` (342 KB, HTML format)

**Deep Context:**
**Source:** Springer EPJ Data Science journal article. Requires HTML **scraping** to find data download links. Contains **~100 million Ookla speed test records** for Brazilian cities.

**Research Focus:**
- Digital divide: Urban vs rural connectivity gaps
- Ookla Speedtest Intelligence data (global platform)
- Inequality metrics: Speed distribution analysis
- Policy recommendations

**Use Case:**
- Spatial demand forecasting
- Coverage gap identification
- Infrastructure priority mapping
- Digital inclusion initiatives

**Technical Challenge:** Extract data from academic journal HTML, potential data repository links

**Scale Warning:** 100M records â†’ requires sampling for initial analysis

---

## ðŸŽ¯ **Cross-Dataset Patterns & Insights**

### **1. Demand Forecasting Hierarchy**

**Time Granularity:**
- **Minute/Hour:** 5G3E infrastructure metrics, github_5g3e
- **Hour:** Telecom Network Tower Data
- **Day:** Most datasets (test, supply chain, retail, daily demand)
- **Episode/Step:** Zenodo Milan (game-theoretic episodes)

**Spatial Granularity:**
- **Single Site:** Test dataset (TORRE001)
- **Multi-Site:** Supply chain (multiple warehouses)
- **National:** Brazilian regulatory datasets
- **Multi-Country:** Academic/research datasets

### **2. Target Variable Types**

**Continuous Demand:**
- `quantity`, `totalSched`, `users_connected`, `units_sold`

**Binary Classification:**
- `CRM_Complaint?`, `Machine_failure`, `congestion`, `stockout_flag`

**Multi-Class:**
- Event type severity, fault categories

**Survival/Time-to-Event:**
- Equipment failure, customer churn

### **3. External Factor Integration**

**Weather:** Brazilian datasets, retail inventory
**Economic:** Exchange rates, inflation, GDP
**Regulatory:** Anatel compliance dates, 5G expansion rates
**Operational:** Holidays, promotions, competition

### **4. Data Quality Considerations**

**High Quality:**
- Test dataset (synthetic, controlled)
- Equipment failure (carefully labeled)
- Zenodo broadband (research-quality)

**Medium Quality:**
- Kaggle competitions (noisy but structured)
- Supply chain/retail (synthetic but realistic)

**Requires Parsing:**
- Brazilian regulatory (HTML/JSON/PDF extraction)
- Academic research (PDF tables)

**Complex Processing:**
- 5G3E Prometheus (wide format, 767+ columns)
- Network fault logs (unstructured â†’ structured)

---

## ðŸ“Š **Statistical Summary Across All Datasets**

| Dataset | Rows | Cols | Frequency | Domain | Target Type | Quality |
|---------|------|------|-----------|--------|-------------|---------|
| **test_dataset** | 732 | 7 | Daily | Telecom | Continuous | â­â­â­â­â­ |
| **zenodo_broadband** | 2,044 | 8 | Snapshot | Customer QoS | Binary | â­â­â­â­â­ |
| **zenodo_milan** | 116,257 | 38 | Episode | 5G Slice | Continuous | â­â­â­â­â­ |
| **equipment_failure** | 10,002 | 14 | Temporal | Industrial | Binary | â­â­â­â­â­ |
| **telecom_network** | Variable | 8 | Hourly | Tower Capacity | Continuous | â­â­â­â­ |
| **supply_chain** | 91,252 | 15 | Daily | Logistics | Continuous | â­â­â­â­ |
| **daily_demand** | 62 | 13 | Daily | Multi-sector | Continuous | â­â­â­ |
| **logistics_warehouse** | 3,204 | 24 | Snapshot | Inventory | Continuous | â­â­â­â­â­ |
| **retail_inventory** | 73,000+ | 15 | Daily | Retail | Continuous | â­â­â­â­ |
| **5g3e_servers** | Variable | 767+ | Minute | Infrastructure | Multivariate | â­â­â­â­â­ |
| **network_fault** | 58,673+ | 3+ | Event | Fault Detection | Multi-class | â­â­â­â­ |
| **anatel_mobile** | Parsing | N/A | Varying | Regulatory | Statistics | â­â­â­ |
| **internet_aberta** | PDF | N/A | Projections | Academic | Forecasts | â­â­â­â­ |
| **springer_divide** | 100M+ | Variable | Test | Academic | Spatial | â­â­â­â­ |

---

## ðŸ”¬ **Feature Engineering Opportunities**

### **1. Temporal Features**
All time-series datasets benefit from:
```python
df['year'] = df['date'].dt.year
df['month'] = df['date'].dt.month
df['day_of_week'] = df['date'].dt.dayofweek
df['is_weekend'] = df['date'].dt.dayofweek >= 5
df['is_holiday'] = df['date'].dt.isin(holidays)
df['days_from_start'] = (df['date'] - df['date'].min()).dt.days
df['lag_1'] = df['quantity'].shift(1)
df['rolling_mean_7'] = df['quantity'].rolling(7).mean()
```

### **2. Demand Decomposition**
```python
from statsmodels.tsa.seasonal import seasonal_decompose
decomposition = seasonal_decompose(series, model='additive')
df['trend'] = decomposition.trend
df['seasonal'] = decomposition.seasonal
df['residual'] = decomposition.resid
```

### **3. Weather Impact Encoding**
```python
weather_order = ['Clear', 'Cloudy', 'Rain', 'Snow', 'Storm']
df['weather_severity'] = df['weather'].map({w: i for i, w in enumerate(weather_order)})
df['extreme_weather'] = df['weather'].isin(['Storm', 'Snow']).astype(int)
```

### **4. Interaction Features**
```python
df['load_capacity_ratio'] = df['users_connected'] / df['capacity']
df['demand_inventory_ratio'] = df['demand'] / df['inventory']
df['temp_humidity_interaction'] = df['temperature'] * df['humidity']
```

### **5. Aggregation Features**
```python
df['category_demand_avg'] = df.groupby('category')['quantity'].transform('mean')
df['site_demand_std'] = df.groupby('site_id')['quantity'].transform('std')
df['regional_market_share'] = df.groupby(['region', 'category'])['quantity'].transform(lambda x: x / x.sum())
```

---

## ðŸŽ¯ **Use Case Mapping for Nova Corrente**

### **Primary Use Cases:**

**1. Demand Forecasting:**
- **Best Datasets:** test_dataset, zenodo_milan, telecom_network, supply_chain, retail_inventory
- **Algorithms:** ARIMA, Prophet, LSTM, XGBoost
- **Features:** Historical demand + external factors

**2. Maintenance Prediction:**
- **Best Datasets:** equipment_failure, network_fault, broadband_brazil
- **Algorithms:** Random Forest, XGBoost, Neural Networks
- **Features:** Sensor telemetry, log volumes, quality metrics

**3. Capacity Planning:**
- **Best Datasets:** zenodo_milan, telecom_network, 5g3e_servers
- **Algorithms:** Linear Regression, SVR, Ensemble
- **Features:** Current load, historical patterns, growth trends

**4. Inventory Optimization:**
- **Best Datasets:** logistics_warehouse, retail_inventory, supply_chain
- **Algorithms:** EOQ models, ML-based safety stock
- **Features:** Lead times, demand variability, holding costs

**5. Churn/Customer Quality:**
- **Best Datasets:** broadband_brazil
- **Algorithms:** Classification models, survival analysis
- **Features:** Service quality metrics, complaint history

---

## ðŸ“ˆ **Data Integration Strategy**

### **Unified Schema Approach**
All datasets map to common structure:
```python
unified_columns = [
    'date',           # Temporal identifier
    'item_id',        # Product/entity identifier  
    'quantity',       # Demand/target variable
    'site_id',        # Location identifier
    'category',       # Classification
    'cost',           # Financial metric
    'lead_time',      # Time metric
    'item_name',      # Descriptive
    'dataset_source'  # Provenance
]
```

### **External Factors Enrichment**
Added to all processed datasets:
```python
external_factors = [
    'temperature', 'precipitation', 'humidity',           # Weather
    'exchange_rate_brl_usd', 'inflation_rate', 'gdp_growth',  # Economics
    '5g_coverage', 'regulatory_compliance_date',         # Regulatory
    'is_holiday', 'is_vacation_period', 'weekend',       # Operational
    'climate_impact', 'economic_impact', 'demand_adjustment_factor'  # Composite
]
```

---

**CENTRALIZED REPORTS & CHANGELOG SYSTEM COMPLETE!**

**Last Updated:** 2025-10-31  
**Nova Corrente Grand Prix SENAI - Deep Dataset Analysis**
