# ğŸ” DIAGNÃ“STICO PROFUNDO: DADOS & STORAGE - NOVA CORRENTE
## Status Atual vs. Arquitetura Alvo (Novembro 2025)

**VersÃ£o:** 2.0  
**Data:** Novembro 2025  
**Status:** ğŸ”´ CRÃTICO - 85% de Gap Arquitetural  
**ReferÃªncia:** [Roadmap de Engenharia de Dados](./DATA_ENGINEERING_ROADMAP_PT_BR.md)

---

## ğŸ“Š EXECUTIVE SUMMARY

### SituaÃ§Ã£o CrÃ­tica
O sistema atual opera com **arquitetura de dados bÃ¡sica (CSV + SQLite)** inadequada para produÃ§Ã£o. Faltam **85% dos componentes** da arquitetura moderna de Data Lakehouse planejada no roadmap.

### Impacto no NegÃ³cio
- âŒ **NÃ£o escala:** CSV files limitados a ~27MB, nÃ£o suportam TB de dados
- âŒ **Sem ACID:** ImpossÃ­vel garantir consistÃªncia transacional
- âŒ **Sem qualidade:** ValidaÃ§Ã£o manual, risco alto de dados incorretos
- âŒ **Sem governanÃ§a:** Zero rastreabilidade, lineage ou catalog
- âŒ **Performance ruim:** Queries de 30s+ (meta: <3s)

### Gap Arquitetural
| Componente | Planejado | Atual | Gap |
|------------|-----------|-------|-----|
| **Storage Layer** | MinIO/S3 + Delta Lake | CSV files | 100% |
| **Data Quality** | Great Expectations | Scripts manuais | 85% |
| **Orchestration** | Apache Airflow | Python scheduler bÃ¡sico | 80% |
| **Transformation** | dbt | Scripts Python ad-hoc | 100% |
| **Governance** | DataHub + Lineage | Nenhum | 100% |

---

## ğŸ—‚ï¸ INVENTÃRIO TÃ‰CNICO ATUAL (REALIDADE)

### 1. Storage & PersistÃªncia

#### âœ… O Que EXISTE
```
data/
â”œâ”€â”€ raw/ (37 items)                    # Dados brutos CSV
â”‚   â”œâ”€â”€ anatel_5g/
â”‚   â”œâ”€â”€ weather/
â”‚   â”œâ”€â”€ economic/
â”‚   â””â”€â”€ ... (33+ subdiretorios)
â”œâ”€â”€ processed/ (37 items)              # Dados processados CSV
â”‚   â”œâ”€â”€ unified_dataset_with_factors.csv (27MB, 118K rows)
â”‚   â”œâ”€â”€ feature_engineered_data.csv
â”‚   â””â”€â”€ ...
â”œâ”€â”€ training/ (8 items)                # Datasets ML
â”‚   â”œâ”€â”€ unknown_train.csv (93,881 rows)
â”‚   â””â”€â”€ unknown_test.csv (23,471 rows)
â””â”€â”€ registry/ (2 items)                # Metadata bÃ¡sico
```

**Problemas Identificados:**
- âŒ **CSV como storage primÃ¡rio** - nÃ£o escala, sem ACID
- âŒ **Sem particionamento** - queries sempre full scan
- âŒ **Sem compressÃ£o eficiente** - desperdiÃ§a storage
- âŒ **Sem versionamento** - impossÃ­vel rollback
- âŒ **Sem schema enforcement** - dados inconsistentes

#### âŒ O Que FALTA (Arquitetura Alvo)
```
MinIO/S3 + Delta Lake:
â”œâ”€â”€ bronze/                            # Raw data (Parquet + Delta)
â”‚   â”œâ”€â”€ year=2025/month=11/day=05/
â”‚   â”‚   â”œâ”€â”€ anatel_5g.parquet
â”‚   â”‚   â”œâ”€â”€ weather.parquet
â”‚   â”‚   â””â”€â”€ economic.parquet
â”œâ”€â”€ silver/                            # Cleaned data (Delta Lake)
â”‚   â”œâ”€â”€ stg_items/
â”‚   â”œâ”€â”€ stg_towers/
â”‚   â””â”€â”€ stg_forecasts/
â””â”€â”€ gold/                              # Analytics (Star Schema)
    â”œâ”€â”€ dim_items/
    â”œâ”€â”€ dim_towers/
    â”œâ”€â”€ dim_time/
    â”œâ”€â”€ fact_forecasts/
    â””â”€â”€ fact_inventory/
```

**Status:** ğŸ”´ **0% implementado**

---

### 2. Database & Schema

#### âœ… O Que EXISTE
```python
# backend/app/config.py
DATABASE_URL = "sqlite:///./data/nova_corrente.db"  # SQLite local

# backend/data/Nova_Corrente_ML_Ready_DB.sql (34.8KB)
# Schema inicial PostgreSQL (NÃƒO em uso)
CREATE TABLE Material (
    material_id INT PRIMARY KEY,
    descricao VARCHAR(255),
    familia VARCHAR(100),
    fornecedor_id INT
);
-- + outras 10+ tabelas
```

**Problemas:**
- âš ï¸ **SQLite em produÃ§Ã£o** - sem suporte adequado a concorrÃªncia
- âŒ **Schema SQL nÃ£o aplicado** - sem migraÃ§Ãµes (Alembic)
- âŒ **Sem ORM (SQLAlchemy)** - queries SQL manuais dispersas
- âŒ **NÃ£o Ã© Star Schema** - schema transacional, nÃ£o analÃ­tico
- âŒ **PostgreSQL config existe mas nÃ£o estÃ¡ em uso**

#### âŒ O Que FALTA
- âŒ PostgreSQL/MySQL em produÃ§Ã£o com pooling
- âŒ SQLAlchemy ORM com models declarativos
- âŒ Alembic para migraÃ§Ãµes versionadas
- âŒ Star Schema (dim_items, dim_towers, fact_forecasts)
- âŒ Ãndices otimizados para queries analÃ­ticas

**Status:** ğŸ”´ **40% implementado** (schema existe, nÃ£o aplicado)

---

### 3. ETL Pipelines & Orchestration

#### âœ… O Que EXISTE
```python
# backend/pipelines/orchestrator_service.py (7.9KB)
class OrchestratorService:
    def start_scheduler(self, time_str="02:00"):
        schedule.every().day.at(time_str).do(
            self.run_complete_pipeline
        )
        # Threading bÃ¡sico Python

# Pipelines implementados:
backend/pipelines/
â”œâ”€â”€ anatel_5g_etl.py          # âœ… 5G data extractor
â”œâ”€â”€ climate_etl.py            # âœ… Weather API
â”œâ”€â”€ economic_etl.py           # âœ… Economic API
â”œâ”€â”€ brazilian_calendar_etl.py # âœ… Calendar
â”œâ”€â”€ feature_calculation_etl.py # âœ… 73 features
â””â”€â”€ data_processing/ (19 scripts)
```

**Pontos Positivos:**
- âœ… Extractors funcionais para fontes externas
- âœ… Feature engineering implementado (73 features)
- âœ… Scheduler bÃ¡sico funciona para MVP

**Problemas CrÃ­ticos:**
- âŒ **Sem Apache Airflow** - orchestraÃ§Ã£o sem UI, retry, monitoring
- âŒ **Sem DAGs visuais** - impossÃ­vel debugar dependÃªncias
- âŒ **Sem retry automÃ¡tico** - falhas nÃ£o tratadas
- âŒ **Sem alerting** - falhas silenciosas
- âŒ **Logging bÃ¡sico** - sem agregaÃ§Ã£o/busca
- âŒ **Salvam em CSV** - nÃ£o em Bronze/Silver layers

#### âŒ O Que FALTA (Airflow)
```python
# dags/extract_bronze_dag.py (NÃƒO EXISTE)
from airflow import DAG
from airflow.operators.python import PythonOperator

with DAG('extract_bronze', schedule='@daily') as dag:
    extract_weather = PythonOperator(...)
    extract_economic = PythonOperator(...)
    extract_5g = PythonOperator(...)
    
    [extract_weather, extract_economic, extract_5g] >> validate_bronze
```

**Status:** ğŸŸ¡ **20% implementado** (scheduler existe, Airflow nÃ£o)

---

### 4. Data Transformation (dbt)

#### âŒ Status Atual: **0% implementado**

**TransformaÃ§Ãµes Atuais (Python ad-hoc):**
```python
# backend/pipelines/data_processing/
â”œâ”€â”€ data_aggregation.py
â”œâ”€â”€ data_cleaning.py
â”œâ”€â”€ feature_engineering.py
â”œâ”€â”€ time_series_preparation.py
â””â”€â”€ ... (15+ scripts duplicados)
```

**Problemas:**
- âŒ **CÃ³digo duplicado** - mesmas transformaÃ§Ãµes em mÃºltiplos arquivos
- âŒ **Sem versionamento** - mudanÃ§as nÃ£o rastreadas
- âŒ **Sem testes** - qualidade nÃ£o validada
- âŒ **Sem documentaÃ§Ã£o automÃ¡tica** - tribal knowledge
- âŒ **DifÃ­cil manutenÃ§Ã£o** - refatorar = quebrar tudo

#### âŒ O Que FALTA (dbt)
```yaml
# dbt_nova_corrente/models/staging/stg_items.sql (NÃƒO EXISTE)
SELECT
  item_id,
  TRIM(LOWER(item_name)) as item_name,
  CAST(price AS DECIMAL(10,2)) as price,
  -- ValidaÃ§Ãµes automÃ¡ticas via dbt tests
FROM {{ source('bronze', 'raw_items') }}
WHERE item_id IS NOT NULL
```

**BenefÃ­cios Perdidos:**
- âŒ SQL versionado como cÃ³digo
- âŒ Testes automÃ¡ticos (not_null, unique, relationships)
- âŒ DocumentaÃ§Ã£o HTML gerada automaticamente
- âŒ Lineage visual de transformaÃ§Ãµes
- âŒ CI/CD integrado

---

### 5. Data Quality & Validation

#### âš ï¸ O Que EXISTE (BÃ¡sico)
```python
# backend/pipelines/monitoring/data_quality_report.py
# Scripts manuais de validaÃ§Ã£o
def check_nulls(df):
    return df.isnull().sum()

def check_duplicates(df):
    return df.duplicated().sum()
```

**Problemas:**
- âš ï¸ **ValidaÃ§Ã£o manual** - precisa executar script manualmente
- âŒ **Sem expectation suites** - regras nÃ£o documentadas
- âŒ **Sem alertas automÃ¡ticos** - falhas passam despercebidas
- âŒ **Sem histÃ³rico** - nÃ£o rastreia degradaÃ§Ã£o ao longo do tempo
- âŒ **Sem data docs** - qualidade nÃ£o reportada

#### âŒ O Que FALTA (Great Expectations)
```python
# great_expectations/expectations/items_suite.json (NÃƒO EXISTE)
{
  "expectations": [
    {
      "expectation_type": "expect_column_values_to_not_be_null",
      "kwargs": {"column": "item_id"}
    },
    {
      "expectation_type": "expect_column_values_to_be_unique",
      "kwargs": {"column": "item_id"}
    },
    {
      "expectation_type": "expect_column_values_to_be_between",
      "kwargs": {"column": "price", "min_value": 0, "max_value": 1000000}
    }
  ]
}
```

**Status:** ğŸŸ¡ **15% implementado** (validaÃ§Ã£o bÃ¡sica existe)

---

### 6. ML Ops & Model Management

#### âš ï¸ O Que EXISTE
```python
# backend/ml/models/ (implementado)
â”œâ”€â”€ prophet/
â”œâ”€â”€ arima/
â””â”€â”€ lstm/

# backend/services/ml_models/model_registry.py (bÃ¡sico)
class ModelRegistry:
    def save_model(self, model, path):
        with open(path, 'wb') as f:
            pickle.dump(model, f)  # Pickle local
```

**Pontos Positivos:**
- âœ… Modelos ML implementados (Prophet, ARIMA, LSTM)
- âœ… Model registry bÃ¡sico funciona

**Problemas:**
- âŒ **Sem MLflow** - sem tracking de experimentos
- âŒ **Pickle files** - versionamento manual, sem metadata
- âŒ **Sem experiment tracking** - impossÃ­vel comparar modelos
- âŒ **Sem model serving** - inferÃªncia nÃ£o padronizada
- âŒ **Sem monitoring** - drift nÃ£o detectado

#### âŒ O Que FALTA (MLflow)
```python
# NÃƒO EXISTE
import mlflow

with mlflow.start_run():
    mlflow.log_params({"horizon": 30, "seasonality": "weekly"})
    mlflow.log_metrics({"mape": 0.12, "rmse": 45.3})
    mlflow.sklearn.log_model(model, "prophet_model")
```

**Status:** ğŸŸ¡ **10% implementado** (models existem, MLflow nÃ£o)

---

### 7. Observability & Monitoring

#### âš ï¸ O Que EXISTE
```python
# backend/config/logging_config.py
import logging
from logging.handlers import RotatingFileHandler

# Logging bÃ¡sico para arquivos
logger = logging.getLogger('nova_corrente')
handler = RotatingFileHandler('logs/app.log', maxBytes=10MB)
```

**Problemas:**
- âš ï¸ **Logs em arquivos** - difÃ­cil agregar/buscar
- âŒ **Sem Prometheus/Grafana** - sem mÃ©tricas de sistema
- âŒ **Sem alerting** - problemas nÃ£o notificados
- âŒ **Sem tracing distribuÃ­do** - debugging difÃ­cil
- âŒ **Sem dashboards** - visibilidade zero

#### âŒ O Que FALTA
- âŒ Prometheus + Grafana para mÃ©tricas
- âŒ ELK/Loki para logs agregados
- âŒ OpenTelemetry para tracing
- âŒ Alertmanager para notificaÃ§Ãµes
- âŒ Dashboards de observabilidade

**Status:** ğŸŸ¡ **20% implementado** (logging bÃ¡sico)

---

### 8. Data Governance & Catalog

#### âŒ Status Atual: **0% implementado**

**O Que NÃƒO EXISTE:**
- âŒ Data Catalog (DataHub) - datasets nÃ£o descobrÃ­veis
- âŒ Lineage tracking - origem dos dados desconhecida
- âŒ Metadata management - sem documentaÃ§Ã£o centralizada
- âŒ Data ownership - responsÃ¡veis nÃ£o definidos
- âŒ Access policies - sem RBAC para dados
- âŒ Data versioning - mudanÃ§as nÃ£o rastreadas

**Impacto:**
- ğŸ”´ **Descoberta impossÃ­vel** - usuÃ¡rios nÃ£o encontram dados
- ğŸ”´ **Conformidade em risco** - nÃ£o atende LGPD/GDPR
- ğŸ”´ **Debugging difÃ­cil** - nÃ£o sabe de onde vem o dado
- ğŸ”´ **ColaboraÃ§Ã£o prejudicada** - conhecimento tribal

---

## ğŸ”´ GAPS CRÃTICOS IDENTIFICADOS

### GAP #1: Storage Layer (100% faltando)
**Atual:** CSV files (27MB, nÃ£o escalÃ¡vel)  
**Alvo:** MinIO/S3 + Delta Lake (TB-scale, ACID)  
**Impacto:** ğŸ”´ BLOQUEADOR - nada escala sem isso

### GAP #2: Data Quality (85% faltando)
**Atual:** Scripts manuais de validaÃ§Ã£o  
**Alvo:** Great Expectations automÃ¡tico  
**Impacto:** ğŸ”´ ALTO - dados ruins em produÃ§Ã£o

### GAP #3: Orchestration (80% faltando)
**Atual:** Python scheduler bÃ¡sico  
**Alvo:** Apache Airflow com DAGs  
**Impacto:** ğŸ”´ ALTO - operaÃ§Ãµes nÃ£o confiÃ¡veis

### GAP #4: Transformation (100% faltando)
**Atual:** Scripts Python ad-hoc  
**Alvo:** dbt com testes e docs  
**Impacto:** ğŸŸ¡ MÃ‰DIO - manutenÃ§Ã£o difÃ­cil

### GAP #5: ML Ops (90% faltando)
**Atual:** Pickle files manuais  
**Alvo:** MLflow tracking + registry  
**Impacto:** ğŸŸ¡ MÃ‰DIO - reprodutibilidade baixa

### GAP #6: Governance (100% faltando)
**Atual:** Nenhum catalog ou lineage  
**Alvo:** DataHub + metadata management  
**Impacto:** ğŸŸ¢ BAIXO - nice to have

---

## ğŸš¨ RISCOS CRÃTICOS

### Risco #1: Perda de Dados (ALTO)
**Causa:** CSV sem backup, versionamento ou ACID  
**Probabilidade:** Alta (80%)  
**Impacto:** CrÃ­tico - perda de dados histÃ³ricos  
**MitigaÃ§Ã£o:** TASK 1.1 (MinIO) + TASK 1.2 (Delta Lake) URGENTE

### Risco #2: Dados Incorretos em ProduÃ§Ã£o (ALTO)
**Causa:** Sem validaÃ§Ã£o automÃ¡tica (Great Expectations)  
**Probabilidade:** Alta (70%)  
**Impacto:** Alto - decisÃµes de negÃ³cio erradas  
**MitigaÃ§Ã£o:** TASK 2.3 (Great Expectations) necessÃ¡rio

### Risco #3: Falhas Silenciosas de Pipeline (MÃ‰DIO)
**Causa:** Orchestrator sem retry, monitoring, alerting  
**Probabilidade:** MÃ©dia (50%)  
**Impacto:** MÃ©dio - dados desatualizados  
**MitigaÃ§Ã£o:** TASK 1.4 (Airflow) resolve

### Risco #4: Impossibilidade de Escalar (ALTO)
**Causa:** SQLite + CSV nÃ£o suportam concorrÃªncia/volume  
**Probabilidade:** Certa (100%) quando crescer  
**Impacto:** CrÃ­tico - sistema para  
**MitigaÃ§Ã£o:** MigraÃ§Ã£o para PostgreSQL + Delta Lake

### Risco #5: Reprodutibilidade de ML Baixa (MÃ‰DIO)
**Causa:** Modelos em Pickle sem tracking (MLflow)  
**Probabilidade:** MÃ©dia (60%)  
**Impacto:** MÃ©dio - nÃ£o consegue replicar resultados  
**MitigaÃ§Ã£o:** TASK 4.1 (MLflow) recomendado

---

## ğŸ¯ PLANO DE AÃ‡ÃƒO PRIORITÃRIO

### ğŸ”¥ SPRINT 1: FUNDAÃ‡ÃƒO CRÃTICA (Semana 1-2)
**Objetivo:** Resolver bloqueadores crÃ­ticos de storage e orchestraÃ§Ã£o

#### TASK 1.1: Setup MinIO (Dia 1-2) ğŸ”´ URGENTE
**Status Atual:** âŒ 0%  
**Bloqueador:** CSV files (~27MB) nÃ£o escalam

**AÃ§Ãµes:**
1. Provisionar MinIO via Docker Compose
   ```yaml
   # docker-compose.yml (ADICIONAR)
   minio:
     image: minio/minio:latest
     ports:
       - "9000:9000"
       - "9001:9001"
     volumes:
       - ./data/minio:/data
     environment:
       MINIO_ROOT_USER: admin
       MINIO_ROOT_PASSWORD: minio123
     command: server /data --console-address ":9001"
   ```

2. Criar buckets Bronze/Silver/Gold
   ```python
   # scripts/setup_minio.py (CRIAR)
   from minio import Minio
   
   client = Minio("localhost:9000", 
                  access_key="admin",
                  secret_key="minio123",
                  secure=False)
   
   for bucket in ["bronze", "silver", "gold"]:
       if not client.bucket_exists(bucket):
           client.make_bucket(bucket)
   ```

3. Migrar dados CSV existentes â†’ MinIO
   ```python
   # scripts/migrate_csv_to_minio.py (CRIAR)
   import pandas as pd
   from minio import Minio
   
   # Migrar data/processed/*.csv â†’ bronze/
   for csv_file in Path("data/processed").glob("*.csv"):
       df = pd.read_csv(csv_file)
       parquet_buffer = df.to_parquet()
       client.put_object(
           "bronze",
           f"raw/{csv_file.stem}.parquet",
           parquet_buffer
       )
   ```

**CritÃ©rios de Aceite:**
- âœ… MinIO rodando em localhost:9000
- âœ… Buckets bronze/silver/gold criados
- âœ… Dados CSV migrados para Parquet em MinIO/bronze
- âœ… Script de teste upload/download funcionando

---

#### TASK 1.2: Implementar Delta Lake (Dia 3-7) ğŸ”´ URGENTE
**Status Atual:** âŒ 0%  
**Bloqueador:** Sem ACID, sem time travel, sem schema evolution

**AÃ§Ãµes:**
1. Instalar Delta Lake
   ```bash
   # requirements_delta.txt (CRIAR)
   delta-spark==2.4.0
   pyspark==3.4.1
   ```

2. Configurar Spark + Delta
   ```python
   # backend/config/delta_config.py (CRIAR)
   from pyspark.sql import SparkSession
   from delta import configure_spark_with_delta_pip
   
   def get_spark_session():
       builder = SparkSession.builder \
           .appName("nova_corrente") \
           .config("spark.sql.extensions", 
                   "io.delta.sql.DeltaSparkSessionExtension") \
           .config("spark.sql.catalog.spark_catalog", 
                   "org.apache.spark.sql.delta.catalog.DeltaCatalog")
       
       return configure_spark_with_delta_pip(builder).getOrCreate()
   ```

3. Migrar Parquet â†’ Delta Lake
   ```python
   # scripts/migrate_parquet_to_delta.py (CRIAR)
   from delta.tables import DeltaTable
   
   spark = get_spark_session()
   
   # Ler Parquet do Bronze
   df = spark.read.parquet("s3a://bronze/raw/unified_dataset.parquet")
   
   # Escrever como Delta com particionamento
   df.write \
       .format("delta") \
       .mode("overwrite") \
       .partitionBy("year", "month", "day") \
       .save("s3a://bronze/delta/unified_dataset")
   ```

4. Testar ACID transactions
   ```python
   # tests/test_delta_acid.py (CRIAR)
   def test_delta_time_travel():
       df_v0 = spark.read.format("delta").load("s3a://bronze/delta/...")
       df_v0.write.format("delta").mode("append").save(...)
       
       # Rollback para versÃ£o anterior
       df_v1 = spark.read.format("delta") \
           .option("versionAsOf", 0) \
           .load("s3a://bronze/delta/...")
       
       assert df_v0.count() != df_v1.count()
   ```

**CritÃ©rios de Aceite:**
- âœ… PySpark + Delta Lake instalados
- âœ… Bronze layer em Delta format
- âœ… Particionamento year/month/day funcionando
- âœ… ACID transactions testadas
- âœ… Time travel (versioning) testado

---

#### TASK 1.3: Setup dbt (Dia 8-12) ğŸ”´ CRÃTICO
**Status Atual:** âŒ 0%  
**Bloqueador:** TransformaÃ§Ãµes nÃ£o versionadas, sem testes

**AÃ§Ãµes:**
1. Criar projeto dbt
   ```bash
   cd backend/
   pip install dbt-spark
   dbt init dbt_nova_corrente
   ```

2. Configurar conexÃ£o Delta Lake
   ```yaml
   # dbt_nova_corrente/profiles.yml (CRIAR)
   nova_corrente:
     target: dev
     outputs:
       dev:
         type: spark
         method: thrift
         host: localhost
         port: 10000
         schema: silver
   ```

3. Migrar transformaÃ§Ãµes Python â†’ SQL
   ```sql
   -- dbt_nova_corrente/models/staging/stg_items.sql (CRIAR)
   WITH source AS (
     SELECT * FROM {{ source('bronze', 'raw_items') }}
   )
   
   SELECT
     item_id,
     TRIM(LOWER(item_name)) as item_name,
     TRIM(category) as category,
     CAST(price AS DECIMAL(10,2)) as price,
     CURRENT_TIMESTAMP() as _dbt_loaded_at
   FROM source
   WHERE item_id IS NOT NULL
   ```

4. Adicionar testes dbt
   ```yaml
   # dbt_nova_corrente/models/staging/schema.yml (CRIAR)
   version: 2
   
   models:
     - name: stg_items
       description: "Staging layer for items"
       columns:
         - name: item_id
           tests:
             - not_null
             - unique
         - name: price
           tests:
             - dbt_utils.accepted_range:
                 min_value: 0
                 max_value: 1000000
   ```

**CritÃ©rios de Aceite:**
- âœ… dbt project inicializado
- âœ… 5 staging models criados (stg_items, stg_towers, stg_weather, stg_economic, stg_5g)
- âœ… Testes dbt passando (not_null, unique, relationships)
- âœ… DocumentaÃ§Ã£o gerada (dbt docs generate)

---

#### TASK 1.4: Setup Airflow (Dia 8-12) ğŸ”´ CRÃTICO
**Status Atual:** âš ï¸ 20% (apenas Python scheduler)  
**Bloqueador:** Sem UI, retry, monitoring, alerting

**AÃ§Ãµes:**
1. Adicionar Airflow ao Docker Compose
   ```yaml
   # docker-compose.yml (ADICIONAR)
   airflow-webserver:
     image: apache/airflow:2.8.0
     ports:
       - "8080:8080"
     environment:
       AIRFLOW__CORE__EXECUTOR: CeleryExecutor
       AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
     volumes:
       - ./dags:/opt/airflow/dags
       - ./logs:/opt/airflow/logs
     command: webserver
   
   airflow-scheduler:
     image: apache/airflow:2.8.0
     environment:
       AIRFLOW__CORE__EXECUTOR: CeleryExecutor
     volumes:
       - ./dags:/opt/airflow/dags
     command: scheduler
   ```

2. Migrar orchestrator_service.py â†’ Airflow DAG
   ```python
   # dags/extract_bronze_dag.py (CRIAR)
   from airflow import DAG
   from airflow.operators.python import PythonOperator
   from datetime import datetime, timedelta
   
   default_args = {
       'owner': 'data-engineering',
       'depends_on_past': False,
       'start_date': datetime(2025, 11, 1),
       'email_on_failure': True,
       'email': ['alerts@novacorrente.com'],
       'retries': 3,
       'retry_delay': timedelta(minutes=5),
   }
   
   with DAG(
       'extract_bronze',
       default_args=default_args,
       schedule_interval='@daily',
       catchup=False
   ) as dag:
       
       extract_weather = PythonOperator(
           task_id='extract_weather',
           python_callable=climate_etl.extract
       )
       
       extract_economic = PythonOperator(
           task_id='extract_economic',
           python_callable=economic_etl.extract
       )
       
       extract_5g = PythonOperator(
           task_id='extract_5g',
           python_callable=anatel_5g_etl.extract
       )
       
       validate_bronze = PythonOperator(
           task_id='validate_bronze',
           python_callable=validate_bronze_data
       )
       
       # Dependencies
       [extract_weather, extract_economic, extract_5g] >> validate_bronze
   ```

3. Configurar alerting (Slack)
   ```python
   # dags/config/airflow_config.py (CRIAR)
   from airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator
   
   def send_slack_alert(context):
       alert = SlackWebhookOperator(
           task_id='slack_alert',
           http_conn_id='slack_webhook',
           message=f"âŒ DAG {context['dag'].dag_id} failed!",
           channel='#data-alerts'
       )
       return alert.execute(context=context)
   ```

**CritÃ©rios de Aceite:**
- âœ… Airflow UI acessÃ­vel em localhost:8080
- âœ… 3 DAGs criados (extract_bronze, bronze_to_silver, silver_to_gold)
- âœ… Retry automÃ¡tico funcionando (testar com falha proposital)
- âœ… Alertas Slack configurados
- âœ… orchestrator_service.py depreciado

---

### ğŸŸ¡ SPRINT 2: QUALIDADE & TRANSFORMAÃ‡ÃƒO (Semana 3-4)

#### TASK 2.1: Bronze Layer Refactoring (Dia 13-16)
**Refatorar extractors** para salvar em MinIO/Bronze (Parquet + Delta)

#### TASK 2.2: Silver Layer (Dia 17-21)
**Criar dbt staging models** com limpeza e validaÃ§Ã£o

#### TASK 2.3: Great Expectations (Dia 22-26)
**Implementar expectation suites** para validaÃ§Ã£o automÃ¡tica

---

## âœ… CRITÃ‰RIOS DE ACEITE FINAIS

### Fase 0: FundaÃ§Ã£o (Semana 1-2)
- [ ] MinIO rodando com buckets Bronze/Silver/Gold
- [ ] Delta Lake implementado com ACID transactions
- [ ] dbt project com 5 staging models rodando
- [ ] Airflow com 3 DAGs executando daily
- [ ] Dados CSV migrados para Delta Lake
- [ ] orchestrator_service.py depreciado

### Fase 1: Qualidade (Semana 3-4)
- [ ] Bronze layer ingerindo dados com particionamento
- [ ] Silver layer com dbt staging models materializados
- [ ] Great Expectations validando >95% qualidade
- [ ] Testes dbt passando (100% coverage)
- [ ] Data quality reports automÃ¡ticos

### Fase 2: Analytics (Semana 5-8)
- [ ] Gold layer com star schema (dim_items, dim_towers, fact_forecasts)
- [ ] Metabase conectado e dashboards criados
- [ ] dbt metrics funcionando
- [ ] Query performance <3s (P95)

---

**Documento atualizado:** Novembro 2025  
**VersÃ£o:** 2.0  
**Status:** ğŸ”´ CRÃTICO - 85% Gap Arquitetural  
**PrÃ³xima AÃ§Ã£o:** ğŸ”¥ TASK 1.1 (Setup MinIO) - COMEÃ‡AR HOJE!

**ReferÃªncias:**
- [Roadmap de Engenharia de Dados](./DATA_ENGINEERING_ROADMAP_PT_BR.md)
- [DiagnÃ³stico Completo](./COMPREHENSIVE_DATA_ENGINEERING_DIAGNOSTIC_PT_BR.md)
- [Constraints Globais](./clusters/GLOBAL_CONSTRAINTS_NO_ML_OPS_PT_BR.md)
