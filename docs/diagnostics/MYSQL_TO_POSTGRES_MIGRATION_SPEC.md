# âŒ MIGRAÃ‡ÃƒO MySQL â†’ PostgreSQL - CANCELADA
## Status: DEPRECATED - Arquitetura Alterada

**VersÃ£o:** 2.0 (Novembro 2025)  
**Status:** ğŸš« **CANCELADO** - NÃ£o aplicÃ¡vel ao escopo atual  
**Motivo:** Sistema atual usa SQLite, nÃ£o MySQL

---

## ğŸ”´ AVISO CRÃTICO

Este documento de migraÃ§Ã£o MySQL â†’ PostgreSQL estÃ¡ **CANCELADO** pelos seguintes motivos:

### 1ï¸âƒ£ Sistema Atual NÃ£o Usa MySQL
```python
# backend/app/config.py (REALIDADE ATUAL)
DATABASE_URL = "sqlite:///./data/nova_corrente.db"  # âœ… SQLite em uso

# backend/config/database_config.py (NÃƒO UTILIZADO)
# ConfiguraÃ§Ãµes MySQL existem mas NÃƒO estÃ£o integradas
MYSQL_URI = "mysql+pymysql://..."  # âŒ Nunca foi usado
```

### 2ï¸âƒ£ Nova Arquitetura: Data Lakehouse (Sem RDBMS Tradicional)
Segundo o [Roadmap de Engenharia de Dados](./DATA_ENGINEERING_ROADMAP_PT_BR.md), a arquitetura alvo Ã©:

```
âŒ NÃƒO FAZER: MySQL â†’ PostgreSQL (abordagem tradicional)

âœ… FAZER: SQLite â†’ MinIO/S3 + Delta Lake (arquitetura moderna)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ARQUITETURA ALVO (Data Lakehouse)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                             â”‚
â”‚  Storage: MinIO/S3 (objeto storage)        â”‚
â”‚  Format:  Delta Lake (ACID + versionamento)â”‚
â”‚  Layers:  Bronze â†’ Silver â†’ Gold           â”‚
â”‚  Transform: dbt (SQL)                       â”‚
â”‚  Orchestration: Airflow (DAGs)             â”‚
â”‚                                             â”‚
â”‚  PostgreSQL: APENAS para metadata/catalog   â”‚
â”‚  (Airflow metadata, MLflow tracking)        â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3ï¸âƒ£ PostgreSQL Tem Papel Diferente
No novo escopo, PostgreSQL serÃ¡ usado **SOMENTE** para:
- âœ… Airflow metadata database
- âœ… MLflow tracking backend
- âœ… Great Expectations validation store
- âŒ **NÃƒO** para dados de negÃ³cio (estes vÃ£o para Delta Lake)

---

## âœ… PLANO CORRETO: SQLite â†’ Delta Lake

### MigraÃ§Ã£o Real NecessÃ¡ria

#### Atual (SQLite - Inadequado)
```python
# backend/app/config.py
DATABASE_URL = "sqlite:///./data/nova_corrente.db"

# Problemas:
# âŒ Sem suporte adequado a concorrÃªncia
# âŒ NÃ£o escala para mÃºltiplos workers
# âŒ Sem ACID robusto
# âŒ Lock de arquivo inteiro
```

#### Alvo (Delta Lake - EscalÃ¡vel)
```python
# Dados de negÃ³cio â†’ Delta Lake (MinIO/S3)
from delta import DeltaTable
import pyspark

# Bronze Layer (raw data)
df.write \
  .format("delta") \
  .mode("overwrite") \
  .partitionBy("year", "month", "day") \
  .save("s3a://bronze/materials/")

# Silver Layer (cleaned via dbt)
# dbt models transformam Bronze â†’ Silver

# Gold Layer (star schema via dbt)
# Analytics-ready dimensional model
```

---

## ğŸ“‹ DOCUMENTO SUBSTITUTO

Este documento Ã© substituÃ­do por:

### 1. [DATA_STORAGE_DIAGNOSTIC_DEEP_DIVE.md](./DATA_STORAGE_DIAGNOSTIC_DEEP_DIVE.md)
**SeÃ§Ã£o:** TASK 1.1 (Setup MinIO) + TASK 1.2 (Implementar Delta Lake)  
**ConteÃºdo:**
- Provisionamento MinIO via Docker Compose
- MigraÃ§Ã£o CSV â†’ Parquet â†’ Delta Lake
- ConfiguraÃ§Ã£o PySpark + Delta Lake
- Scripts de migraÃ§Ã£o completos

### 2. [DATA_ENGINEERING_ROADMAP_PT_BR.md](./DATA_ENGINEERING_ROADMAP_PT_BR.md)
**SeÃ§Ã£o:** Fase 0 (Foundation)  
**ConteÃºdo:**
- Arquitetura Medallion (Bronze/Silver/Gold)
- Stack tecnolÃ³gico completo
- Plano de implementaÃ§Ã£o em sprints

### 3. PostgreSQL - Uso Limitado
PostgreSQL serÃ¡ configurado **SOMENTE** para:

```yaml
# docker-compose.yml (ATUALIZAR)
services:
  # PostgreSQL para serviÃ§os de infraestrutura
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: nova_corrente_metadata
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow123
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
  
  # Airflow usa PostgreSQL
  airflow-webserver:
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow123@postgres/nova_corrente_metadata
  
  # MLflow usa PostgreSQL
  mlflow-server:
    environment:
      BACKEND_STORE_URI: postgresql://airflow:airflow123@postgres/mlflow
      ARTIFACT_ROOT: s3://mlflow-artifacts/

volumes:
  postgres_data:
```

---

## ğŸ¯ AÃ‡Ã•ES CORRETAS (Em vez deste documento)

### âœ… Sprint 1: Setup MinIO + Delta Lake
Executar [TASK 1.1](./DATA_STORAGE_DIAGNOSTIC_DEEP_DIVE.md#task-11-setup-minio-dia-1-2--urgente) e [TASK 1.2](./DATA_STORAGE_DIAGNOSTIC_DEEP_DIVE.md#task-12-implementar-delta-lake-dia-3-7--urgente)

### âœ… Sprint 2: Configurar PostgreSQL (Metadata Only)
```bash
# 1. Provisionar PostgreSQL (Docker Compose)
docker-compose up -d postgres

# 2. Criar databases separados
psql -h localhost -U airflow -d postgres -c "CREATE DATABASE airflow_metadata;"
psql -h localhost -U airflow -d postgres -c "CREATE DATABASE mlflow_tracking;"
psql -h localhost -U airflow -d postgres -c "CREATE DATABASE great_expectations;"

# 3. Executar migrations
airflow db init  # Cria schema Airflow
mlflow db upgrade  # Cria schema MLflow

# 4. NÃƒO migrar dados de negÃ³cio para PostgreSQL
# Dados vÃ£o para Delta Lake (MinIO/S3)
```

### âœ… Sprint 3: Depreciar SQLite
```python
# backend/app/config.py (ATUALIZAR)
# ANTES:
DATABASE_URL = "sqlite:///./data/nova_corrente.db"  # âŒ DEPRECAR

# DEPOIS:
# PostgreSQL APENAS para metadata de aplicaÃ§Ã£o (se necessÃ¡rio)
DATABASE_URL = os.getenv(
    "DATABASE_URL",
    "postgresql://app:app123@postgres/app_metadata"  # Metadata app
)

# Dados de negÃ³cio â†’ Delta Lake (nÃ£o RDBMS)
# Acessados via Spark/dbt, nÃ£o SQLAlchemy
```

---

## ğŸ“Š COMPARAÃ‡ÃƒO: Abordagem Antiga vs. Nova

| Aspecto | âŒ Antiga (Este Doc) | âœ… Nova (Roadmap) |
|---------|---------------------|-------------------|
| **Storage** | PostgreSQL RDBMS | MinIO/S3 + Delta Lake |
| **Formato** | Tabelas relacionais | Parquet + Delta (ACID) |
| **Escalabilidade** | Vertical (CPU/RAM) | Horizontal (object storage) |
| **Custos** | Alto (instÃ¢ncia dedicada) | Baixo (commodity storage) |
| **ACID** | PostgreSQL nativo | Delta Lake |
| **Versionamento** | Backup/restore | Time travel nativo |
| **Analytics** | Queries SQL diretas | dbt + Spark SQL |
| **Particionamento** | Limitado | Nativo (year/month/day) |
| **CompressÃ£o** | Limitada | 70% (Parquet + Snappy) |
| **TransformaÃ§Ã£o** | Stored procedures | dbt (versionado) |

---

## ğŸ—‘ï¸ CONTEÃšDO DEPRECADO

<details>
<summary>âš ï¸ ConteÃºdo original (manter para referÃªncia histÃ³rica)</summary>

- **Objetivo**: migrar do MySQL para PostgreSQL com zero perda de dados, mÃ­nimo downtime e controle total via `SQLAlchemy` + `Alembic`.
- **Escopo**: modelo de dados (DDL), migraÃ§Ã£o de dados, adequaÃ§Ãµes na aplicaÃ§Ã£o, testes, CI/CD, deploy e rollback.
- **Fases**:
  - **F0 â€“ Descoberta**: inventÃ¡rio completo do MySQL (schemas, tabelas, FKs, Ã­ndices, volumes, rotinas, grants).
  - **F1 â€“ Design**: DDL do PostgreSQL (tipos, constraints, Ã­ndices, sequences, FKs, particionamento quando aplicÃ¡vel).
  - **F2 â€“ MigraÃ§Ã£o**: escolha de estratÃ©gia (bulk cutover ou incremental), scripts automatizados e validaÃ§Ãµes pÃ³s-carga.
  - **F3 â€“ App/ORM**: `SQLAlchemy` models, Alembic, refactors de queries especÃ­ficas de MySQL.
  - **F4 â€“ Testes/Perf**: integridade, checksums, regressÃ£o de queries e benchmarks.
  - **F5 â€“ Deploy/Cutover**: execuÃ§Ã£o controlada (migrations-first + canary/blue-green) e rollback testado.
- **PriorizaÃ§Ã£o imediata**: (A) inventÃ¡rio + mapeamento tipos + DDL exemplo; (B) script automatizado para 1 tabela; (C) plano de cutover + validaÃ§Ãµes.

---

## 2) Plano tÃ©cnico detalhado por fase (comandos e exemplos)

### Fase 0 â€” Descoberta e inventÃ¡rio

- **Artefatos necessÃ¡rios**
  - Credenciais MySQL (host, porta, usuÃ¡rio, senha, database).
  - Tamanho total do banco e por tabela; crescimento histÃ³rico.
  - DependÃªncias de aplicaÃ§Ã£o (quais serviÃ§os escrevem/leem diretamente).
- **Comandos (MySQL INFORMATION_SCHEMA)**
  - Colunas e tipos:
    ```sql
    SELECT TABLE_SCHEMA, TABLE_NAME, COLUMN_NAME, COLUMN_TYPE, IS_NULLABLE, COLUMN_DEFAULT,
           COLUMN_KEY, EXTRA, COLLATION_NAME
    FROM INFORMATION_SCHEMA.COLUMNS
    WHERE TABLE_SCHEMA = 'YOUR_DB'
    ORDER BY TABLE_NAME, ORDINAL_POSITION;
    ```
  - Chaves e FKs:
    ```sql
    SELECT CONSTRAINT_NAME, TABLE_NAME, COLUMN_NAME, REFERENCED_TABLE_NAME, REFERENCED_COLUMN_NAME
    FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE
    WHERE TABLE_SCHEMA='YOUR_DB' AND REFERENCED_TABLE_NAME IS NOT NULL;
    ```
  - Ãndices:
    ```sql
    SELECT TABLE_NAME, INDEX_NAME, NON_UNIQUE, COLUMN_NAME, SEQ_IN_INDEX
    FROM INFORMATION_SCHEMA.STATISTICS
    WHERE TABLE_SCHEMA='YOUR_DB'
    ORDER BY TABLE_NAME, INDEX_NAME, SEQ_IN_INDEX;
    ```
  - Tamanhos e linhas (estimativas):
    ```sql
    SELECT TABLE_NAME, ENGINE, TABLE_ROWS, DATA_LENGTH, INDEX_LENGTH
    FROM INFORMATION_SCHEMA.TABLES
    WHERE TABLE_SCHEMA='YOUR_DB'
    ORDER BY DATA_LENGTH DESC;
    ```
  - Rotinas/Views/Triggers:
    ```sql
    SELECT ROUTINE_TYPE, ROUTINE_NAME FROM INFORMATION_SCHEMA.ROUTINES WHERE ROUTINE_SCHEMA='YOUR_DB';
    SHOW FULL TABLES WHERE TABLE_TYPE = 'VIEW';
    SHOW TRIGGERS FROM YOUR_DB;
    ```
  - Slow queries (opcional): habilitar `slow_query_log` e usar `mysqldumpslow` ou `pt-query-digest`.
- **SaÃ­da esperada**: CSV/JSON com inventÃ¡rio completo (guardar em `docs/migration/mysql_inventory/`).

### Fase 1 â€” Design do schema PostgreSQL

- **Mapeamento de tipos (MySQL â†’ PostgreSQL)**
  - `TINYINT(1)` â†’ `BOOLEAN` (flags)
  - `TINYINT/SMALLINT/INT` unsigned â†’ `INTEGER`/`BIGINT` + `CHECK (col >= 0)` (ou `BIGINT` para evitar overflow)
  - `INT AUTO_INCREMENT` â†’ `GENERATED ALWAYS AS IDENTITY`
  - `VARCHAR(n)` â†’ `VARCHAR(n)` (revisar collation; usar `UTF8`)
  - `TEXT` â†’ `TEXT`
  - `DATETIME`/`TIMESTAMP` â†’ `TIMESTAMP [WITH] TIME ZONE` (preferir `WITH` se hÃ¡ fusos)
  - `ENUM` â†’ `CREATE TYPE ... AS ENUM` ou `VARCHAR` + `CHECK`
  - `JSON` â†’ `JSONB` (com Ã­ndices GIN quando necessÃ¡rio)
  - `DOUBLE`/`FLOAT` â†’ `DOUBLE PRECISION`/`REAL`
  - `DECIMAL(p,s)` â†’ `NUMERIC(p,s)`
- **DDL exemplo (PostgreSQL)**
  ```sql
  CREATE TABLE public.material (
    id           BIGINT GENERATED ALWAYS AS IDENTITY PRIMARY KEY,
    code         VARCHAR(64) NOT NULL UNIQUE,
    family       VARCHAR(64) NOT NULL,
    supplier     VARCHAR(128),
    created_at   TIMESTAMP WITH TIME ZONE DEFAULT now(),
    updated_at   TIMESTAMP WITH TIME ZONE DEFAULT now()
  );

  CREATE TABLE public.series (
    material_id  BIGINT NOT NULL REFERENCES public.material(id) ON DELETE CASCADE,
    ts_date      DATE NOT NULL,
    demand      NUMERIC(18,4),
    stock       NUMERIC(18,4),
    lead_time   INTEGER,
    features    JSONB,
    PRIMARY KEY (material_id, ts_date)
  );

  CREATE INDEX series_ts_date_idx ON public.series(ts_date);
  CREATE INDEX series_features_gin ON public.series USING GIN (features);
  ```
- **Particionamento (quando Ãºtil)**
  - Por data (range) em tabelas de sÃ©ries temporais grandes:
    ```sql
    CREATE TABLE public.series (
      material_id BIGINT NOT NULL,
      ts_date DATE NOT NULL,
      ...,
      PRIMARY KEY (material_id, ts_date)
    ) PARTITION BY RANGE (ts_date);
    ```

### Fase 2 â€” EstratÃ©gia de migraÃ§Ã£o

- **OpÃ§Ã£o A: Bulk cutover (downtime curto)**
  - Pausar gravaÃ§Ãµes, exportar, importar, validar, apontar app para Postgres, liberar gravaÃ§Ãµes.
- **OpÃ§Ã£o B: Incremental (quase zero downtime)**
  - ReplicaÃ§Ã£o lÃ³gica (Debezium â†’ Kafka â†’ sink Postgres) ou `pgloader` com `--with data only` + delta final.
- **Ferramentas e scripts**
  - `pgloader`:
    ```bash
    pgloader mysql://USER:PASS@MYSQL_HOST/YOUR_DB \
            postgresql://PGUSER:PGPASS@PG_HOST/YOUR_DB \
            --with "workers = 8, concurrency = 4" \
            --with "prefetch rows = 10000" \
            --with data only
    ```
  - `mysqldump` â†’ `psql`:
    ```bash
    mysqldump -h MYSQL_HOST -u USER -p --routines --no-tablespaces --default-character-set=utf8mb4 YOUR_DB > dump.sql
    psql postgresql://PGUSER:PGPASS@PG_HOST/YOUR_DB -f dump.sql
    ```
  - ETL incremental (Python) para tabelas crÃ­ticas e reconciliaÃ§Ã£o.
- **ValidaÃ§Ãµes pÃ³s-carga**
  - Contagens por tabela, checksums por chunks, amostras linha a linha.
  - RegressÃ£o de queries-chaves e latÃªncia P95.

### Fase 3 â€” AplicaÃ§Ã£o/ORM

- **`SQLAlchemy` + `Alembic`**
  - Adicionar modelos ORM e criar `alembic/` com `env.py` e `versions/0001_initial.py`.
  - `DATABASE_URL` de produÃ§Ã£o: `postgresql+psycopg://USER:PASS@HOST:5432/DB`.
- **Refactors MySQLâ†’Postgres**
  - `INSERT ... ON DUPLICATE KEY` â†’ `INSERT ... ON CONFLICT (col) DO UPDATE ...`
  - DiferenÃ§as em `GROUP BY`, collation e `LIKE` vs `ILIKE`.
  - Full-text search: `MATCH ... AGAINST` â†’ `to_tsvector`/`ts_rank`.

### Fase 4 â€” Testes e performance

- **Integridade**: FKs vÃ¡lidas, contagens, checksums.
- **Performance**: `EXPLAIN ANALYZE`, Ã­ndices sugeridos, `pgbench`.
- **ManutenÃ§Ã£o**: `VACUUM (ANALYZE)`, `REINDEX` quando necessÃ¡rio.

### Fase 5 â€” Deploy e cutover

- **PrÃ©-cutover**: snapshot/backup MySQL, congelar writes (se Bulk), aquecer Postgres, rodar migrations.
- **Cutover**: executar migraÃ§Ã£o, validar (contagens, checksums), apontar app, monitorar.
- **Rollback**: critÃ©rios de gatilho e script de retorno para MySQL (se necessÃ¡rio) ou reexecuÃ§Ã£o de versÃ£o anterior.

---

## 3) Exemplos de scripts e comandos (prontos para adaptaÃ§Ã£o)

- `scripts/mysql_inventory.sql` (coleta inventÃ¡rio; ver consultas Fase 0).
- `scripts/generate_pg_ddl.py` (protÃ³tipo):
  ```python
  # LÃª inventÃ¡rio JSON do MySQL e gera DDL Postgres com mapping tipos
  # Uso: python scripts/generate_pg_ddl.py --inventory docs/migration/mysql_inventory/schema.json --out ddl/postgres_schema.sql
  ```
- `scripts/migrate_table_pgloader.sh`:
  ```bash
  #!/usr/bin/env bash
  set -euo pipefail
  SRC="mysql://$MYSQL_USER:$MYSQL_PASS@$MYSQL_HOST/$MYSQL_DB"
  DST="postgresql://$PG_USER:$PG_PASS@$PG_HOST/$PG_DB"
  TABLE="$1" # ex.: material
  pgloader <<EOF
  LOAD DATABASE
      FROM $SRC
      INTO $DST
  WITH include drop, create no tables, materialize view no tables,
       workers = 8, concurrency = 4, prefetch rows = 10000
  CAST type datetime to timestamp with time zone drop default using zero-dates-to-null,
       type json to jsonb using identity
  ALTER SCHEMA 'YOUR_DB' RENAME TO 'public'
  SET work_mem to '256MB', maintenance_work_mem to '512MB'
  EXCLUDING TABLE NAMES MATCHING 'tmp_.*'
  INCLUDING ONLY TABLE NAMES MATCHING "$TABLE"
  BEFORE LOAD DO
      $$ ALTER TABLE IF EXISTS public.$TABLE DISABLE TRIGGER ALL; $$,
  AFTER LOAD DO
      $$ ALTER TABLE IF EXISTS public.$TABLE ENABLE TRIGGER ALL; $$;
  EOF
  ```
- `scripts/verify_migration.py` (amostra):
  ```python
  # Compara counts e checksums por chunks entre MySQL e Postgres para uma tabela
  # Uso: python scripts/verify_migration.py --table material --chunk-size 100000
  ```
- Alembic inicial:
  ```bash
  alembic init alembic
  alembic revision -m "initial schema" --autogenerate
  alembic upgrade head
  ```
- CI (ex.: `.github/workflows/migrate.yml`):
  ```yaml
  # job: spin Postgres, run alembic upgrade, run verify tests
  ```

---

## 4) Checklist de aceitaÃ§Ã£o

- Alembic aplicado em staging, `alembic current` = `head`.
- Contagens por tabela iguais (Â± tolerÃ¢ncia justificada) e checksums por amostra/chunks batem.
- Testes de integraÃ§Ã£o da aplicaÃ§Ã£o OK com Postgres.
- Queries crÃ­ticas com performance aceitÃ¡vel (P95 dentro da meta) e Ã­ndices ajustados.
- Plano de cutover/rollback testado (dry run) e documentaÃ§Ã£o assinada.
- Observabilidade ativa (mÃ©tricas/logs/alertas por 72h pÃ³s-cutover).

---

## 5) Prioridade imediata (A/B/C)

- **A â€” InventÃ¡rio + mapeamento + DDL**
  - Rodar `scripts/mysql_inventory.sql` e salvar inventÃ¡rio.
  - Gerar DDL-alvo inicial com `generate_pg_ddl.py` (ou manualmente para 2-3 tabelas crÃ­ticas).
- **B â€” Script de conversÃ£o (tabela exemplo)**
  - Rodar `migrate_table_pgloader.sh material` e validar contagens/checksums.
- **C â€” Plano de cutover + validaÃ§Ãµes**
  - Definir janela/downtime alvo; ensaiar em staging.
  - Documentar passos e critÃ©rios de rollback.

---

## 6) InformaÃ§Ãµes faltantes (coletar automaticamente)

- Acesso MySQL (HOST, PORT, USER, PASS, DB) e volume de dados por tabela.
- Tabelas e relaÃ§Ãµes crÃ­ticas para a aplicaÃ§Ã£o.
- Lista de queries crÃ­ticas para benchmark.

Comandos de coleta: ver consultas de Fase 0; salvar CSV/JSON em `docs/migration/mysql_inventory/`.

---

## 7) AdaptaÃ§Ã£o ao projeto atual

- O backend hoje usa `DATABASE_URL` e ainda nÃ£o estÃ¡ integrado a MySQL; a migraÃ§Ã£o focarÃ¡ em padronizar `PostgreSQL` como alvo final.
- Introduzir `SQLAlchemy` + `Alembic` no app e mover `DATABASE_URL` para Postgres via `.env` e `docker-compose`.
- Criar `alembic/versions/0001_initial.py` com as tabelas mÃ­nimas (materials, series, forecasts, pipeline_runs, feature_snapshots).

---

## 8) Comandos prontos (placeholders para parametrizar)

```bash
# VariÃ¡veis
export MYSQL_HOST=... MYSQL_DB=... MYSQL_USER=... MYSQL_PASS=...
export PG_HOST=... PG_DB=... PG_USER=... PG_PASS=...

# InventÃ¡rio
mysql -h $MYSQL_HOST -u $MYSQL_USER -p$MYSQL_PASS -e "SOURCE scripts/mysql_inventory.sql" > docs/migration/mysql_inventory/inventory_$(date +%F).txt

# GeraÃ§Ã£o DDL (exemplo)
python scripts/generate_pg_ddl.py --inventory docs/migration/mysql_inventory/schema.json --out ddl/postgres_schema.sql

# CriaÃ§Ã£o schema no Postgres
psql postgresql://$PG_USER:$PG_PASS@$PG_HOST/$PG_DB -f ddl/postgres_schema.sql

# MigraÃ§Ã£o de 1 tabela (material)
bash scripts/migrate_table_pgloader.sh material

# VerificaÃ§Ã£o
python scripts/verify_migration.py --table material --chunk-size 100000

# Alembic
alembic upgrade head
```

---

</details>

---

## ğŸ“š REFERÃŠNCIAS ATUALIZADAS

### Documentos de DiagnÃ³stico VÃ¡lidos:
1. **[DATA_ENGINEERING_ROADMAP_PT_BR.md](./DATA_ENGINEERING_ROADMAP_PT_BR.md)**
   - Arquitetura completa Data Lakehouse
   - Sprint 1-4 com tasks detalhadas
   - Stack tecnolÃ³gico: MinIO, Delta Lake, dbt, Airflow

2. **[DATA_STORAGE_DIAGNOSTIC_DEEP_DIVE.md](./DATA_STORAGE_DIAGNOSTIC_DEEP_DIVE.md)**
   - InventÃ¡rio tÃ©cnico atual (SQLite, CSV)
   - Gaps crÃ­ticos identificados
   - Plano de aÃ§Ã£o com cÃ³digo completo

3. **[COMPREHENSIVE_DATA_ENGINEERING_DIAGNOSTIC_PT_BR.md](./COMPREHENSIVE_DATA_ENGINEERING_DIAGNOSTIC_PT_BR.md)**
   - DiagnÃ³stico completo (85% gap)
   - AnÃ¡lise top-down e bottom-up
   - Riscos e mitigaÃ§Ãµes

4. **[GLOBAL_CONSTRAINTS_NO_ML_OPS_PT_BR.md](./clusters/GLOBAL_CONSTRAINTS_NO_ML_OPS_PT_BR.md)**
   - PolÃ­tica de ML Ops fora do deployment
   - Arquitetura de deployment leve
   - Precomputed results only

### CÃ³digo Real Existente:
```
backend/
â”œâ”€â”€ app/config.py                    # âœ… DATABASE_URL atual (SQLite)
â”œâ”€â”€ config/database_config.py        # âŒ MySQL config (nÃ£o usado)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ Nova_Corrente_ML_Ready_DB.sql  # Schema PostgreSQL (referÃªncia)
â”‚   â””â”€â”€ collectors/                  # Extractors ETL
â”œâ”€â”€ pipelines/
â”‚   â”œâ”€â”€ orchestrator_service.py      # âš ï¸ Scheduler bÃ¡sico (deprecar)
â”‚   â”œâ”€â”€ anatel_5g_etl.py            # âœ… Refatorar para Bronze layer
â”‚   â”œâ”€â”€ climate_etl.py              # âœ… Refatorar para Bronze layer
â”‚   â””â”€â”€ economic_etl.py             # âœ… Refatorar para Bronze layer

data/
â”œâ”€â”€ processed/                       # âœ… Migrar para MinIO/Bronze
â”‚   â””â”€â”€ unified_dataset_with_factors.csv  # 27MB
â”œâ”€â”€ raw/                            # âœ… Migrar para MinIO/Bronze
â”‚   â”œâ”€â”€ anatel_5g/
â”‚   â”œâ”€â”€ weather/
â”‚   â””â”€â”€ economic/
â””â”€â”€ training/                       # âœ… Migrar para MinIO/Silver
    â”œâ”€â”€ unknown_train.csv
    â””â”€â”€ unknown_test.csv
```

---

## âœ… PRÃ“XIMOS PASSOS IMEDIATOS

### 1. Abandonar MigraÃ§Ã£o MySQL â†’ PostgreSQL
- âŒ NÃ£o implementar nenhum script deste documento
- âŒ NÃ£o criar infra MySQL
- âŒ NÃ£o migrar para PostgreSQL como storage principal

### 2. Seguir Roadmap Data Lakehouse
- âœ… **TASK 1.1:** Setup MinIO (Dia 1-2)
- âœ… **TASK 1.2:** Implementar Delta Lake (Dia 3-7)
- âœ… **TASK 1.3:** Setup dbt (Dia 8-12)
- âœ… **TASK 1.4:** Setup Airflow (Dia 8-12)

### 3. PostgreSQL Apenas para Metadata
- âœ… Provisionar PostgreSQL para Airflow metadata
- âœ… Provisionar PostgreSQL para MLflow tracking
- âœ… **NÃƒO** usar PostgreSQL para dados de negÃ³cio

---

**Documento atualizado:** Novembro 2025  
**VersÃ£o:** 2.0  
**Status:** ğŸš« **CANCELADO** - SubstituÃ­do por arquitetura Data Lakehouse  
**AÃ§Ã£o Imediata:** Seguir [DATA_STORAGE_DIAGNOSTIC_DEEP_DIVE.md](./DATA_STORAGE_DIAGNOSTIC_DEEP_DIVE.md)

**IMPORTANTE:** Este documento foi mantido apenas para **referÃªncia histÃ³rica**. A decisÃ£o arquitetural foi revisada e o sistema seguirÃ¡ a abordagem moderna de Data Lakehouse (MinIO + Delta Lake) ao invÃ©s de RDBMS tradicional (MySQL/PostgreSQL).
