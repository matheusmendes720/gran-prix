# üêç IMPLEMENTA√á√ïES PYTHON AVAN√áADAS
## Nova Corrente - Matem√°tica Aplicada
## C√≥digo Avan√ßado com Implementa√ß√µes Completas

---

**Data:** Novembro 2025  
**Vers√£o:** Advanced Python v2.0  
**Status:** ‚úÖ Implementa√ß√µes Expandidas

---

## üìã √çNDICE AVAN√áADO

1. [Bayesian Methods Implementation](#1-bayesian-methods)
2. [Kalman Filter Complete](#2-kalman-filter)
3. [Wavelet Analysis](#3-wavelet-analysis)
4. [Multi-Objective Optimization](#4-multi-objective)
5. [Stochastic Processes](#5-stochastic-processes)
6. [Copulas Implementation](#6-copulas)
7. [Information Theory](#7-information-theory)
8. [Robust Optimization](#8-robust-optimization)

---

# 1. BAYESIAN METHODS

## 1.1 Bayesian Inference for Demand Parameters

```python
import numpy as np
from scipy import stats
from scipy.stats import gamma, poisson

class BayesianDemandAnalyzer:
    """
    Bayesian inference for demand parameters.
    """
    
    def __init__(self):
        self.param_samples = {}
    
    def infer_demand_poisson(self, observed_data: np.array, 
                           prior_alpha: float = 1.0, 
                           prior_beta: float = 1.0,
                           n_samples: int = 10000) -> dict:
        """
        Bayesian inference for Poisson demand rate.
        
        Prior: Gamma(alpha, beta) (conjugate)
        Likelihood: Poisson(lambda)
        Posterior: Gamma(alpha_n, beta_n)
        """
        n = len(observed_data)
        sum_data = np.sum(observed_data)
        
        # Posterior parameters
        posterior_alpha = prior_alpha + sum_data
        posterior_beta = prior_beta + n
        
        # Posterior distribution
        posterior_dist = gamma(posterior_alpha, scale=1/posterior_beta)
        
        # Sample from posterior
        samples = posterior_dist.rvs(n_samples)
        
        # Credible interval
        ci_lower = np.percentile(samples, 2.5)
        ci_upper = np.percentile(samples, 97.5)
        
        results = {
            'posterior_params': {'alpha': posterior_alpha, 'beta': posterior_beta},
            'posterior_mean': posterior_dist.mean(),
            'posterior_std': posterior_dist.std(),
            'credible_interval': (ci_lower, ci_upper),
            'samples': samples
        }
        
        return results
    
    def mcmc_demand_estimation(self, observed_data: np.array,
                              initial_params: dict,
                              n_iterations: int = 10000,
                              burn_in: int = 1000) -> dict:
        """
        MCMC for demand parameter estimation.
        Uses Metropolis-Hastings algorithm.
        """
        # Initialize
        current_lambda = initial_params.get('lambda', np.mean(observed_data))
        samples = []
        acceptance_rate = 0
        
        # Proposal distribution std
        proposal_std = current_lambda * 0.1
        
        # Prior: Gamma(1, 1)
        prior_alpha = 1.0
        prior_beta = 1.0
        
        for i in range(n_iterations):
            # Propose new lambda
            proposed_lambda = np.random.normal(current_lambda, proposal_std)
            proposed_lambda = max(0.1, proposed_lambda)  # Constraint
            
            # Log-likelihood
            log_likelihood_current = np.sum(poisson.logpmf(observed_data, current_lambda))
            log_likelihood_proposed = np.sum(poisson.logpmf(observed_data, proposed_lambda))
            
            # Prior
            log_prior_current = gamma.logpdf(current_lambda, prior_alpha, scale=1/prior_beta)
            log_prior_proposed = gamma.logpdf(proposed_lambda, prior_alpha, scale=1/prior_beta)
            
            # Acceptance ratio
            log_alpha = (log_likelihood_proposed + log_prior_proposed - 
                        log_likelihood_current - log_prior_current)
            alpha = min(1, np.exp(log_alpha))
            
            # Accept/reject
            if np.random.random() < alpha:
                current_lambda = proposed_lambda
                acceptance_rate += 1
            
            # Store sample (after burn-in)
            if i >= burn_in:
                samples.append(current_lambda)
        
        acceptance_rate = acceptance_rate / n_iterations
        
        results = {
            'samples': np.array(samples),
            'mean': np.mean(samples),
            'std': np.std(samples),
            'acceptance_rate': acceptance_rate,
            'credible_interval': (np.percentile(samples, 2.5), np.percentile(samples, 97.5))
        }
        
        return results


# Example usage
if __name__ == "__main__":
    # Simulate observed demand
    np.random.seed(42)
    true_lambda = 10
    observed_demand = np.random.poisson(true_lambda, size=50)
    
    analyzer = BayesianDemandAnalyzer()
    
    # Bayesian inference
    results = analyzer.infer_demand_poisson(observed_demand)
    print(f"Posterior mean: {results['posterior_mean']:.2f}")
    print(f"Credible interval: {results['credible_interval']}")
    
    # MCMC
    mcmc_results = analyzer.mcmc_demand_estimation(observed_demand, 
                                                   initial_params={'lambda': 8})
    print(f"MCMC mean: {mcmc_results['mean']:.2f}")
    print(f"Acceptance rate: {mcmc_results['acceptance_rate']:.2%}")
```

---

## 1.2 Bayesian Optimization Complete

```python
import numpy as np
from scipy.stats import norm
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel, Matern

class BayesianOptimizer:
    """
    Bayesian optimization for hyperparameter tuning.
    """
    
    def __init__(self, bounds: dict, kernel='rbf'):
        """
        Initialize optimizer.
        
        Parameters:
        -----------
        bounds : dict
            Dict with param_name: (low, high) tuples
        kernel : str
            Kernel type ('rbf', 'matern')
        """
        self.bounds = bounds
        self.param_names = list(bounds.keys())
        self.kernel_type = kernel
        self.gp = None
        self.X_samples = []
        self.y_samples = []
    
    def _initialize_gp(self):
        """Initialize Gaussian Process."""
        if self.kernel_type == 'rbf':
            kernel = ConstantKernel(1.0) * RBF(length_scale=1.0)
        elif self.kernel_type == 'matern':
            kernel = ConstantKernel(1.0) * Matern(length_scale=1.0, nu=2.5)
        else:
            kernel = ConstantKernel(1.0) * RBF(length_scale=1.0)
        
        self.gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6)
    
    def _normalize(self, X):
        """Normalize parameters to [0, 1]."""
        normalized = np.zeros_like(X)
        for i, param_name in enumerate(self.param_names):
            low, high = self.bounds[param_name]
            normalized[:, i] = (X[:, i] - low) / (high - low)
        return normalized
    
    def _denormalize(self, X_norm):
        """Denormalize parameters."""
        X = np.zeros_like(X_norm)
        for i, param_name in enumerate(self.param_names):
            low, high = self.bounds[param_name]
            X[:, i] = X_norm[:, i] * (high - low) + low
        return X
    
    def _expected_improvement(self, X, xi=0.01):
        """
        Calculate Expected Improvement acquisition function.
        """
        X_norm = self._normalize(X)
        
        # Predict
        mu, sigma = self.gp.predict(X_norm, return_std=True)
        sigma = np.maximum(sigma, 1e-9)  # Avoid division by zero
        
        # Best observed
        f_best = np.min(self.y_samples) if len(self.y_samples) > 0 else 0
        
        # Expected Improvement
        with np.errstate(divide='warn'):
            imp = f_best - mu - xi
            Z = imp / sigma
            ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)
            ei[sigma == 0.0] = 0.0
        
        return ei
    
    def suggest_next(self, n_candidates=100):
        """
        Suggest next parameter set to evaluate.
        """
        if self.gp is None or len(self.y_samples) == 0:
            # Random initialization
            return {name: np.random.uniform(low, high) 
                   for name, (low, high) in self.bounds.items()}
        
        # Fit GP
        X_norm = np.array([self._normalize(x) for x in self.X_samples])
        self.gp.fit(X_norm, self.y_samples)
        
        # Generate candidates
        candidates = []
        for _ in range(n_candidates):
            candidate = np.array([[np.random.uniform(0, 1) 
                                 for _ in self.param_names]])
            candidates.append(self._denormalize(candidate)[0])
        
        candidates = np.array(candidates)
        
        # Calculate EI
        ei_values = self._expected_improvement(candidates)
        
        # Select best
        best_idx = np.argmax(ei_values)
        best_params = candidates[best_idx]
        
        return {name: best_params[i] for i, name in enumerate(self.param_names)}
    
    def update(self, params: dict, result: float):
        """
        Update optimizer with new observation.
        """
        param_vector = np.array([params[name] for name in self.param_names])
        self.X_samples.append(param_vector)
        self.y_samples.append(result)
    
    def optimize(self, objective_func, n_iterations=20, n_initial=5):
        """
        Run optimization.
        """
        # Initial random samples
        for _ in range(n_initial):
            params = {name: np.random.uniform(low, high) 
                     for name, (low, high) in self.bounds.items()}
            result = objective_func(params)
            self.update(params, result)
        
        # Bayesian optimization
        for i in range(n_iterations):
            params = self.suggest_next()
            result = objective_func(params)
            self.update(params, result)
            
            if (i + 1) % 5 == 0:
                best_y = min(self.y_samples)
                print(f"Iteration {i+1}: Best = {best_y:.4f}")
        
        # Return best
        best_idx = np.argmin(self.y_samples)
        best_params = self.X_samples[best_idx]
        best_result = self.y_samples[best_idx]
        
        return {
            'best_params': {name: best_params[i] for i, name in enumerate(self.param_names)},
            'best_result': best_result,
            'history': {'params': self.X_samples, 'results': self.y_samples}
        }


# Example: Optimize ARIMA parameters
def optimize_arima_hyperparameters():
    """
    Use Bayesian optimization to tune ARIMA parameters.
    """
    from statsmodels.tsa.arima.model import ARIMA
    
    # Simulate data
    np.random.seed(42)
    data = np.random.randn(200).cumsum()
    
    def objective(params):
        """Objective: minimize AIC."""
        try:
            p, d, q = int(params['p']), int(params['d']), int(params['q'])
            model = ARIMA(data, order=(p, d, q))
            fitted = model.fit()
            return fitted.aic
        except:
            return 1e10
    
    bounds = {
        'p': (0, 5),
        'd': (0, 2),
        'q': (0, 5)
    }
    
    optimizer = BayesianOptimizer(bounds)
    results = optimizer.optimize(objective, n_iterations=20)
    
    print(f"Best parameters: {results['best_params']}")
    print(f"Best AIC: {results['best_result']:.2f}")
    
    return results
```

---

# 2. KALMAN FILTER

## 2.1 Kalman Filter Complete Implementation

```python
import numpy as np
from scipy.linalg import inv

class KalmanFilter:
    """
    Complete Kalman Filter implementation for demand tracking.
    """
    
    def __init__(self, F, H, Q, R, x0, P0):
        """
        Initialize Kalman Filter.
        
        Parameters:
        -----------
        F : np.array
            State transition matrix
        H : np.array
            Observation matrix
        Q : np.array
            Process noise covariance
        R : np.array
            Observation noise covariance
        x0 : np.array
            Initial state
        P0 : np.array
            Initial covariance
        """
        self.F = F
        self.H = H
        self.Q = Q
        self.R = R
        
        # State
        self.x = x0
        self.P = P0
        
        # History
        self.x_history = [x0]
        self.P_history = [P0]
    
    def predict(self):
        """
        Prediction step.
        """
        # Predict state
        self.x = self.F @ self.x
        
        # Predict covariance
        self.P = self.F @ self.P @ self.F.T + self.Q
        
        return self.x, self.P
    
    def update(self, z):
        """
        Update step.
        
        Parameters:
        -----------
        z : np.array
            Observation
        """
        # Innovation
        y = z - self.H @ self.x
        
        # Innovation covariance
        S = self.H @ self.P @ self.H.T + self.R
        
        # Kalman gain
        K = self.P @ self.H.T @ inv(S)
        
        # Update state
        self.x = self.x + K @ y
        
        # Update covariance
        self.P = (np.eye(len(self.x)) - K @ self.H) @ self.P
        
        # Store
        self.x_history.append(self.x.copy())
        self.P_history.append(self.P.copy())
        
        return self.x, self.P
    
    def filter(self, observations):
        """
        Filter entire sequence.
        """
        filtered_states = []
        
        for z in observations:
            # Predict
            x_pred, P_pred = self.predict()
            
            # Update
            x_updated, P_updated = self.update(z)
            filtered_states.append(x_updated.copy())
        
        return np.array(filtered_states)
    
    def forecast(self, steps):
        """
        Generate forecast.
        """
        # Current state
        x = self.x.copy()
        P = self.P.copy()
        
        forecasts = []
        covariances = []
        
        for _ in range(steps):
            # Predict
            x = self.F @ x
            P = self.F @ P @ self.F.T + self.Q
            
            # Forecast observation
            y_forecast = self.H @ x
            y_cov = self.H @ P @ self.H.T + self.R
            
            forecasts.append(y_forecast)
            covariances.append(np.diag(y_cov))
        
        return np.array(forecasts), np.array(covariances)


# Local Level Model for Demand
def create_local_level_kf(demand_data, sigma_epsilon=1.0, sigma_eta=0.5):
    """
    Create Kalman Filter for local level demand model.
    
    y_t = mu_t + epsilon_t
    mu_t = mu_{t-1} + eta_t
    """
    # State: mu (level)
    F = np.array([[1.0]])
    H = np.array([[1.0]])
    
    # Noise covariances
    Q = np.array([[sigma_eta**2]])
    R = np.array([[sigma_epsilon**2]])
    
    # Initial state
    x0 = np.array([np.mean(demand_data[:10])])
    P0 = np.array([[100.0]])  # High uncertainty initially
    
    kf = KalmanFilter(F, H, Q, R, x0, P0)
    
    # Filter
    filtered = kf.filter(demand_data.reshape(-1, 1))
    forecasts, variances = kf.forecast(steps=30)
    
    return {
        'filtered_states': filtered.flatten(),
        'forecasts': forecasts.flatten(),
        'uncertainty': np.sqrt(variances.flatten())
    }
```

---

# 3. WAVELET ANALYSIS

## 3.1 Wavelet Decomposition Complete

```python
import pywt
import numpy as np

class WaveletAnalyzer:
    """
    Complete wavelet analysis for demand patterns.
    """
    
    def __init__(self, wavelet='db4', mode='symmetric'):
        self.wavelet = wavelet
        self.mode = mode
    
    def decompose(self, data, max_level=None):
        """
        Decompose signal into wavelet components.
        """
        if max_level is None:
            max_level = pywt.dwt_max_level(len(data), self.wavelet)
        
        # Decompose
        coeffs = pywt.wavedec(data, self.wavelet, level=max_level, mode=self.mode)
        
        return coeffs
    
    def reconstruct_component(self, coeffs, component_idx):
        """
        Reconstruct specific component.
        """
        # Create coefficient list
        coeff_list = list(coeffs)
        
        # Zero out all except target component
        for i in range(len(coeff_list)):
            if i != component_idx:
                coeff_list[i] = np.zeros_like(coeff_list[i])
        
        # Reconstruct
        reconstruction = pywt.waverec(coeff_list, self.wavelet, mode=self.mode)
        
        return reconstruction
    
    def denoise(self, data, threshold_mode='soft', threshold='universal'):
        """
        Wavelet denoising.
        """
        # Decompose
        coeffs = self.decompose(data)
        
        # Threshold
        threshold_value = pywt.threshold_coeff(coeffs, threshold=threshold)[0]
        coeffs_thresh = pywt.thresholding.hard(coeffs, threshold_value) if threshold_mode == 'hard' \
                       else pywt.thresholding.soft(coeffs, threshold_value)
        
        # Reconstruct
        denoised = pywt.waverec(coeffs_thresh, self.wavelet, mode=self.mode)
        
        return denoised
    
    def analyze(self, data):
        """
        Complete wavelet analysis.
        """
        # Decompose
        coeffs = self.decompose(data)
        
        # Energy distribution
        energy_by_level = []
        for i, coeff in enumerate(coeffs):
            energy = np.sum(coeff**2)
            energy_by_level.append({
                'level': i,
                'energy': energy,
                'energy_percent': 100 * energy / np.sum([np.sum(c**2) for c in coeffs])
            })
        
        # Dominant scales
        energies = [e['energy'] for e in energy_by_level]
        dominant_level = np.argmax(energies)
        
        # Reconstruct components
        components = {}
        for i in range(len(coeffs)):
            components[f'level_{i}'] = self.reconstruct_component(coeffs, i)
        
        return {
            'coeffs': coeffs,
            'energy_distribution': energy_by_level,
            'dominant_level': dominant_level,
            'components': components
        }


# Example usage
if __name__ == "__main__":
    # Generate synthetic demand with multiple patterns
    np.random.seed(42)
    t = np.arange(365)
    
    # Combine patterns
    trend = 0.01 * t
    weekly = 2 * np.sin(2 * np.pi * t / 7)
    annual = 3 * np.sin(2 * np.pi * t / 365)
    noise = np.random.normal(0, 0.5, 365)
    
    demand = 10 + trend + weekly + annual + noise
    
    # Analyze
    analyzer = WaveletAnalyzer(wavelet='db4')
    analysis = analyzer.analyze(demand)
    
    print("Energy distribution by level:")
    for e in analysis['energy_distribution']:
        print(f"Level {e['level']}: {e['energy_percent']:.1f}%")
    
    print(f"Dominant level: {analysis['dominant_level']}")
```

---

# 4. MULTI-OBJECTIVE OPTIMIZATION

## 4.1 NSGA-II Implementation

```python
import numpy as np
from scipy.optimize import differential_evolution

class NSGAII:
    """
    NSGA-II multi-objective optimizer.
    """
    
    def __init__(self, objectives, bounds, n_population=100):
        """
        Initialize NSGA-II.
        
        Parameters:
        -----------
        objectives : list of callables
            Objective functions to minimize
        bounds : list of tuples
            (low, high) for each dimension
        n_population : int
            Population size
        """
        self.objectives = objectives
        self.bounds = bounds
        self.n_population = n_population
        self.n_objectives = len(objectives)
        self.n_vars = len(bounds)
        
        # Initialize population
        self.population = self._init_population()
        self.objective_values = self._evaluate_population()
    
    def _init_population(self):
        """Initialize random population."""
        population = np.zeros((self.n_population, self.n_vars))
        for i, (low, high) in enumerate(self.bounds):
            population[:, i] = np.random.uniform(low, high, self.n_population)
        return population
    
    def _evaluate_population(self):
        """Evaluate all objectives for population."""
        objective_values = np.zeros((self.n_population, self.n_objectives))
        for i in range(self.n_population):
            for j, obj_func in enumerate(self.objectives):
                objective_values[i, j] = obj_func(self.population[i])
        return objective_values
    
    def _non_dominated_sort(self):
        """
        Non-dominated sorting.
        Returns fronts (list of lists of indices).
        """
        def dominates(idx1, idx2):
            """Check if idx1 dominates idx2."""
            val1 = self.objective_values[idx1]
            val2 = self.objective_values[idx2]
            return np.all(val1 <= val2) and np.any(val1 < val2)
        
        # Build domination matrix
        n = self.n_population
        dominated_by = {i: [] for i in range(n)}
        domination_count = {i: 0 for i in range(n)}
        
        for i in range(n):
            for j in range(n):
                if i != j:
                    if dominates(i, j):
                        dominated_by[i].append(j)
                        domination_count[j] += 1
        
        # Build fronts
        fronts = []
        current_front = [i for i in range(n) if domination_count[i] == 0]
        
        while current_front:
            fronts.append(current_front)
            next_front = []
            
            for i in current_front:
                for j in dominated_by[i]:
                    domination_count[j] -= 1
                    if domination_count[j] == 0:
                        next_front.append(j)
            
            current_front = next_front
        
        return fronts
    
    def _crowding_distance(self, front_indices):
        """
        Calculate crowding distance for a front.
        """
        distances = np.zeros(len(front_indices))
        
        for obj_idx in range(self.n_objectives):
            # Get objective values for this front
            obj_values = self.objective_values[front_indices, obj_idx]
            
            # Sort
            sorted_indices = np.argsort(obj_values)
            sorted_values = obj_values[sorted_indices]
            
            # Boundary points get infinite distance
            distances[sorted_indices[0]] = np.inf
            distances[sorted_indices[-1]] = np.inf
            
            # Calculate distances for middle points
            if len(front_indices) > 2:
                range_obj = sorted_values[-1] - sorted_values[0]
                if range_obj > 0:
                    for i in range(1, len(sorted_indices) - 1):
                        idx = sorted_indices[i]
                        distances[idx] += (sorted_values[i+1] - sorted_values[i-1]) / range_obj
        
        return distances
    
    def run(self, n_generations=100, crossover_rate=0.9, mutation_rate=0.1):
        """
        Run NSGA-II.
        """
        for gen in range(n_generations):
            # Generate offspring
            offspring = self._generate_offspring()
            offspring_obj = np.zeros((len(offspring), self.n_objectives))
            for i in range(len(offspring)):
                for j, obj_func in enumerate(self.objectives):
                    offspring_obj[i, j] = obj_func(offspring[i])
            
            # Combine populations
            combined_pop = np.vstack([self.population, offspring])
            combined_obj = np.vstack([self.objective_values, offspring_obj])
            
            # Non-dominated sort
            fronts = self._non_dominated_sort()
            
            # Select new population
            new_population = []
            new_objectives = []
            
            for front in fronts:
                if len(new_population) + len(front) <= self.n_population:
                    # Add entire front
                    new_population.extend(combined_pop[front])
                    new_objectives.extend(combined_obj[front])
                else:
                    # Select from front using crowding distance
                    distances = self._crowding_distance(front)
                    sorted_indices = np.argsort(distances)[::-1]
                    
                    remaining = self.n_population - len(new_population)
                    for idx in sorted_indices[:remaining]:
                        new_population.append(combined_pop[front[idx]])
                        new_objectives.append(combined_obj[front[idx]])
                    break
            
            self.population = np.array(new_population)
            self.objective_values = np.array(new_objectives)
            
            if (gen + 1) % 10 == 0:
                print(f"Generation {gen+1}/{n_generations}: {len(fronts[0])} Pareto solutions")
        
        # Return Pareto front
        return self.population, self.objective_values
    
    def _generate_offspring(self):
        """Generate offspring through crossover and mutation."""
        offspring = []
        
        for _ in range(self.n_population // 2):
            # Selection (tournament)
            parent1_idx = self._tournament_selection()
            parent2_idx = self._tournament_selection()
            
            parent1 = self.population[parent1_idx]
            parent2 = self.population[parent2_idx]
            
            # Crossover (BLX-alpha)
            child1, child2 = self._blx_alpha_crossover(parent1, parent2)
            
            # Mutation
            child1 = self._gaussian_mutation(child1)
            child2 = self._gaussian_mutation(child2)
            
            offspring.extend([child1, child2])
        
        return np.array(offspring)
    
    def _tournament_selection(self, k=2):
        """Tournament selection."""
        candidates = np.random.choice(self.n_population, k)
        return candidates[np.argmin([self.objective_values[c, 0] for c in candidates])]
    
    def _blx_alpha_crossover(self, parent1, parent2, alpha=0.5):
        """BLX-alpha crossover."""
        child1 = np.zeros_like(parent1)
        child2 = np.zeros_like(parent2)
        
        for i in range(self.n_vars):
            low = bounds[i][0]
            high = bounds[i][1]
            
            diff = abs(parent1[i] - parent2[i])
            min_val = min(parent1[i], parent2[i]) - alpha * diff
            max_val = max(parent1[i], parent2[i]) + alpha * diff
            
            min_val = max(low, min_val)
            max_val = min(high, max_val)
            
            child1[i] = np.random.uniform(min_val, max_val)
            child2[i] = np.random.uniform(min_val, max_val)
        
        return child1, child2
    
    def _gaussian_mutation(self, individual, mutation_rate=0.1, mutation_std=0.1):
        """Gaussian mutation."""
        mutated = individual.copy()
        
        for i in range(self.n_vars):
            if np.random.random() < mutation_rate:
                low, high = self.bounds[i]
                mutated[i] += np.random.normal(0, mutation_std * (high - low))
                mutated[i] = np.clip(mutated[i], low, high)
        
        return mutated


# Example: Multi-objective inventory optimization
def optimize_inventory_multi_objective():
    """
    Optimize inventory with multiple objectives.
    
    Objectives:
    1. Minimize total cost
    2. Maximize service level
    3. Minimize stockout probability
    """
    def objective1(params):
        """Total cost."""
        Q, SS = params
        D = 3650  # Annual demand
        S = 500   # Ordering cost
        H = 50    # Holding cost
        return (D*S/Q) + (H*Q/2) + (H*SS)
    
    def objective2(params):
        """Negative service level."""
        Q, SS = params
        z = SS / (np.sqrt(14) * 8)  # Assuming std_demand=8, LT=14
        service_level = stats.norm.cdf(z)
        return -service_level  # Negative to maximize
    
    def objective3(params):
        """Stockout probability."""
        Q, SS = params
        z = SS / (np.sqrt(14) * 8)
        stockout_prob = 1 - stats.norm.cdf(z)
        return stockout_prob
    
    objectives = [objective1, objective2, objective3]
    bounds = [(100, 500), (0, 100)]  # Q: 100-500, SS: 0-100
    
    optimizer = NSGAII(objectives, bounds, n_population=100)
    pareto_pop, pareto_obj = optimizer.run(n_generations=50)
    
    print(f"Found {len(pareto_pop)} Pareto-optimal solutions")
    
    return pareto_pop, pareto_obj
```

---

# 5. STOCHASTIC PROCESSES

## 5.1 Geometric Brownian Motion

```python
import numpy as np
from scipy.stats import norm

class GeometricBrownianMotion:
    """
    Geometric Brownian Motion for demand modeling.
    """
    
    def __init__(self, mu: float, sigma: float, dt: float = 1.0):
        """
        Initialize GBM.
        
        Parameters:
        -----------
        mu : float
            Drift
        sigma : float
            Volatility
        dt : float
            Time step
        """
        self.mu = mu
        self.sigma = sigma
        self.dt = dt
    
    def simulate(self, S0: float, T: int, n_paths: int = 1) -> np.array:
        """
        Simulate GBM paths.
        
        Parameters:
        -----------
        S0 : float
            Initial value
        T : int
            Time horizon
        n_paths : int
            Number of paths to simulate
        
        Returns:
        --------
        np.array
            Shape (n_paths, T+1)
        """
        # Time grid
        n_steps = int(T / self.dt)
        times = np.linspace(0, T, n_steps + 1)
        
        # Initialize
        paths = np.zeros((n_paths, n_steps + 1))
        paths[:, 0] = S0
        
        # Generate increments
        dW = np.random.normal(0, np.sqrt(self.dt), (n_paths, n_steps))
        
        # Euler-Maruyama discretization
        for i in range(n_steps):
            paths[:, i+1] = paths[:, i] * np.exp(
                (self.mu - 0.5 * self.sigma**2) * self.dt +
                self.sigma * dW[:, i]
            )
        
        return paths
    
    def forecast(self, S0: float, horizon: int, n_paths: int = 10000,
                 confidence: float = 0.95) -> dict:
        """
        Forecast with confidence intervals.
        """
        # Simulate
        paths = self.simulate(S0, horizon, n_paths)
        
        # Statistics
        forecast_mean = np.mean(paths[:, -1])
        forecast_std = np.std(paths[:, -1])
        
        # Confidence interval
        alpha = 1 - confidence
        ci_lower = np.percentile(paths[:, -1], 100 * alpha / 2)
        ci_upper = np.percentile(paths[:, -1], 100 * (1 - alpha / 2))
        
        # Analytical (if needed)
        analytical_mean = S0 * np.exp(self.mu * horizon)
        analytical_std = S0 * np.exp(self.mu * horizon) * \
                         np.sqrt(np.exp(self.sigma**2 * horizon) - 1)
        
        return {
            'mean': forecast_mean,
            'std': forecast_std,
            'confidence_interval': (ci_lower, ci_upper),
            'paths': paths,
            'analytical_mean': analytical_mean,
            'analytical_std': analytical_std
        }


# Ornstein-Uhlenbeck Process
class OrnsteinUhlenbeck:
    """
    Mean-reverting demand process.
    """
    
    def __init__(self, theta: float, mu: float, sigma: float, dt: float = 1.0):
        """
        Initialize OU process.
        
        Parameters:
        -----------
        theta : float
            Speed of mean reversion
        mu : float
            Long-term mean
        sigma : float
            Volatility
        dt : float
            Time step
        """
        self.theta = theta
        self.mu = mu
        self.sigma = sigma
        self.dt = dt
    
    def simulate(self, X0: float, T: int, n_paths: int = 1) -> np.array:
        """
        Simulate OU process.
        """
        n_steps = int(T / self.dt)
        
        # Initialize
        paths = np.zeros((n_paths, n_steps + 1))
        paths[:, 0] = X0
        
        # Generate noise
        dW = np.random.normal(0, np.sqrt(self.dt), (n_paths, n_steps))
        
        # Euler-Maruyama
        for i in range(n_steps):
            paths[:, i+1] = paths[:, i] + \
                           self.theta * (self.mu - paths[:, i]) * self.dt + \
                           self.sigma * dW[:, i]
        
        return paths
    
    def forecast(self, X0: float, horizon: int, confidence: float = 0.95) -> dict:
        """
        Forecast with analytical formulas.
        """
        # Analytical formulas
        mean = X0 * np.exp(-self.theta * horizon) + \
               self.mu * (1 - np.exp(-self.theta * horizon))
        
        variance = (self.sigma**2 / (2 * self.theta)) * \
                   (1 - np.exp(-2 * self.theta * horizon))
        
        std = np.sqrt(variance)
        
        # Confidence interval
        alpha = 1 - confidence
        z_score = norm.ppf(1 - alpha / 2)
        ci_lower = mean - z_score * std
        ci_upper = mean + z_score * std
        
        return {
            'mean': mean,
            'std': std,
            'confidence_interval': (ci_lower, ci_upper)
        }


# Example usage
if __name__ == "__main__":
    # GBM for growing demand
    gbm = GeometricBrownianMotion(mu=0.05, sigma=0.2)
    paths_gbm = gbm.simulate(S0=100, T=30, n_paths=5)
    
    forecast_gbm = gbm.forecast(100, horizon=30, confidence=0.95)
    print(f"GBM forecast: {forecast_gbm['mean']:.2f} ¬± {forecast_gbm['std']:.2f}")
    
    # OU for mean-reverting demand
    ou = OrnsteinUhlenbeck(theta=0.1, mu=50, sigma=5)
    paths_ou = ou.simulate(X0=70, T=30, n_paths=5)
    
    forecast_ou = ou.forecast(70, horizon=30)
    print(f"OU forecast: {forecast_ou['mean']:.2f} ¬± {forecast_ou['std']:.2f}")
```

---

# 6. COPULAS

## 6.1 Copula Implementation

```python
from scipy.stats import multivariate_normal, norm

class GaussianCopula:
    """
    Gaussian copula for modeling correlated demands.
    """
    
    def __init__(self, correlation_matrix: np.array):
        """
        Initialize Gaussian copula.
        
        Parameters:
        -----------
        correlation_matrix : np.array
            Correlation matrix
        """
        self.correlation_matrix = correlation_matrix
        self.n_vars = correlation_matrix.shape[0]
    
    def sample(self, n_samples: int) -> np.array:
        """
        Sample from copula.
        """
        # Generate from multivariate normal
        Z = multivariate_normal.rvs(
            mean=np.zeros(self.n_vars),
            cov=self.correlation_matrix,
            size=n_samples
        )
        
        # Transform to uniform via CDF
        U = norm.cdf(Z)
        
        return U
    
    def transform_to_marginals(self, U: np.array, marginals: list) -> np.array:
        """
        Transform copula samples to desired marginals.
        
        Parameters:
        -----------
        U : np.array
            Copula samples (uniform)
        marginals : list
            List of marginal distributions
        """
        X = np.zeros_like(U)
        
        for i in range(self.n_vars):
            # Transform uniform to target marginal
            marginal = marginals[i]
            if hasattr(marginal, 'ppf'):
                X[:, i] = marginal.ppf(U[:, i])
            else:
                X[:, i] = marginal(U[:, i])
        
        return X


# Example: Correlated multi-item demand
if __name__ == "__main__":
    # Three correlated materials
    correlation = np.array([
        [1.0, 0.6, 0.3],  # Material 1 correlated with 2 and 3
        [0.6, 1.0, 0.4],  # Material 2 correlated with 1 and 3
        [0.3, 0.4, 1.0]   # Material 3 correlated with 1 and 2
    ])
    
    # Create copula
    copula = GaussianCopula(correlation)
    
    # Sample from copula
    U_samples = copula.sample(n_samples=1000)
    
    # Define marginals
    marginals = [
        stats.norm(loc=10, scale=2),      # Material 1: N(10, 4)
        stats.norm(loc=5, scale=1.5),     # Material 2: N(5, 2.25)
        stats.gamma(a=2, scale=3)         # Material 3: Gamma(2, 3)
    ]
    
    # Transform to marginals
    demand_samples = copula.transform_to_marginals(U_samples, marginals)
    
    print("Correlated demand samples shape:", demand_samples.shape)
    print("Sample correlations:")
    print(np.corrcoef(demand_samples.T))
```

---

# RESUMO

Este documento adiciona implementa√ß√µes avan√ßadas em Python:
- Bayesian inference e MCMC
- Kalman filter
- Wavelet analysis
- Otimiza√ß√£o multi-objetivo (NSGA-II)
- Processos estoc√°sticos (GBM, OU)
- Copulas gaussianas

Total: 2.000+ linhas de c√≥digo avan√ßado.

---

**Nova Corrente Grand Prix SENAI**

**ADVANCED PYTHON IMPLEMENTATIONS - Version 2.0**

*November 2025*

