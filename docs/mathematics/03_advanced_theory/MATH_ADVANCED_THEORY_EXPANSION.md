# üìê MATEM√ÅTICA AVAN√áADA EXPANDIDA - TEORIA PROFUNDA
## Nova Corrente - Sistema de Previs√£o de Demanda
## An√°lise Matem√°tica Avan√ßada com Provas e Deriva√ß√µes Completas

---

**Data:** Novembro 2025  
**Vers√£o:** Advanced Theory v2.0  
**Status:** ‚úÖ An√°lise Profunda Expandida

---

## üìã √çNDICE EXPANDIDO

1. [Mathematical Foundations](#1-mathematical-foundations)
2. [Probability Theory Deep Dive](#2-probability-theory-deep-dive)
3. [Time Series Mathematics](#3-time-series-mathematics)
4. [Optimization Theory](#4-optimization-theory)
5. [Bayesian Methods](#5-bayesian-methods)
6. [Stochastic Processes](#6-stochastic-processes)
7. [Information Theory](#7-information-theory)
8. [Advanced Risk Metrics](#8-advanced-risk-metrics)

---

# 1. MATHEMATICAL FOUNDATIONS

## 1.1 Linear Algebra for Demand Forecasting

### Matrix Representations

**Time Series as Vector:**
$$\mathbf{D} = \begin{pmatrix}
D_1 \\
D_2 \\
D_3 \\
\vdots \\
D_n
\end{pmatrix} \in \mathbb{R}^n$$

**Feature Matrix for ML:**
$$\mathbf{X} = \begin{pmatrix}
1 & t_1 & \sin(2\pi t_1/7) & \cos(2\pi t_1/7) & \ldots \\
1 & t_2 & \sin(2\pi t_2/7) & \cos(2\pi t_2/7) & \ldots \\
\vdots & \vdots & \vdots & \vdots & \ddots \\
1 & t_n & \sin(2\pi t_n/7) & \cos(2\pi t_n/7) & \ldots
\end{pmatrix}$$

**OLS Solution:**
$$\hat{\boldsymbol{\beta}} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}$$

**For ill-conditioned matrices, use SVD:**
$$\mathbf{X} = \mathbf{U} \boldsymbol{\Sigma} \mathbf{V}^T$$

where:
- $\mathbf{U}$: $n \times n$ orthogonal (left singular vectors)
- $\boldsymbol{\Sigma}$: $n \times p$ diagonal (singular values)
- $\mathbf{V}$: $p \times p$ orthogonal (right singular vectors)

**Regularized Solution (Ridge):**
$$\hat{\boldsymbol{\beta}}_{ridge} = (\mathbf{X}^T \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^T \mathbf{y}$$

---

### Eigenvalue Decomposition for PCA

**Covariance Matrix:**
$$\mathbf{C} = \frac{1}{n-1} \mathbf{X}^T \mathbf{X}$$

**Eigendecomposition:**
$$\mathbf{C} = \mathbf{Q} \boldsymbol{\Lambda} \mathbf{Q}^T$$

where $\boldsymbol{\Lambda}$ contains eigenvalues $\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_p$.

**Principal Components:**
$$\mathbf{Z} = \mathbf{X} \mathbf{Q}$$

**Variance Explained:**
$$\text{Var explained}_k = \frac{\lambda_k}{\sum_{i=1}^{p} \lambda_i}$$

---

## 1.2 Calculus for Optimization

### Gradient Descent Derivation

**Objective Function:**
$$J(\boldsymbol{\theta}) = \frac{1}{2n} \sum_{i=1}^{n} (y_i - f(\mathbf{x}_i; \boldsymbol{\theta}))^2$$

**Gradient:**
$$\nabla_{\boldsymbol{\theta}} J = -\frac{1}{n} \sum_{i=1}^{n} (y_i - f(\mathbf{x}_i; \boldsymbol{\theta})) \nabla_{\boldsymbol{\theta}} f$$

**Update Rule:**
$$\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \alpha \nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta}_t)$$

**Convergence Condition:**
$$\|\nabla J\| < \epsilon \text{ or } J(\boldsymbol{\theta}_{t+1}) - J(\boldsymbol{\theta}_t) < \delta$$

---

### Second-Order Methods

**Newton's Method:**
$$\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \mathbf{H}^{-1} \nabla J$$

where $\mathbf{H}$ is the Hessian:
$$H_{ij} = \frac{\partial^2 J}{\partial \theta_i \partial \theta_j}$$

**Hessian-Free (Quasi-Newton):**
$$\mathbf{B}_{t+1} = \mathbf{B}_t + \frac{\mathbf{y}_t \mathbf{y}_t^T}{\mathbf{y}_t^T \mathbf{s}_t} - \frac{\mathbf{B}_t \mathbf{s}_t \mathbf{s}_t^T \mathbf{B}_t}{\mathbf{s}_t^T \mathbf{B}_t \mathbf{s}_t}$$

where $\mathbf{s}_t = \boldsymbol{\theta}_{t+1} - \boldsymbol{\theta}_t$, $\mathbf{y}_t = \nabla J_{t+1} - \nabla J_t$.

---

# 2. PROBABILITY THEORY DEEP DIVE

## 2.1 Advanced Distributions

### Compound Poisson Process

**For High-Variability Demand:**

$$D(t) = \sum_{i=1}^{N(t)} X_i$$

where:
- $N(t) \sim \text{Poisson}(\lambda t)$
- $X_i \sim \text{i.i.d.}$ with distribution $G$

**Moment Generating Function:**
$$M_{D(t)}(s) = e^{\lambda t (M_X(s) - 1)}$$

**Mean and Variance:**
$$\mathbb{E}[D(t)] = \lambda t \mu_X$$
$$\text{Var}(D(t)) = \lambda t (\sigma_X^2 + \mu_X^2)$$

---

### General Extreme Value (GEV) Distribution

**For Extreme Demand Events:**

**PDF:**
$$f(x; \mu, \sigma, \xi) = \begin{cases}
\frac{1}{\sigma} \left[1 + \xi\left(\frac{x-\mu}{\sigma}\right)\right]^{-1/\xi - 1} \exp\left\{-\left[1 + \xi\left(\frac{x-\mu}{\sigma}\right)\right]^{-1/\xi}\right\} & \xi \neq 0 \\
\frac{1}{\sigma} \exp\left(-\frac{x-\mu}{\sigma} - e^{-\frac{x-\mu}{\sigma}}\right) & \xi = 0
\end{cases}$$

**Parameters:**
- $\mu$: location parameter
- $\sigma > 0$: scale parameter
- $\xi$: shape parameter (tail index)

**Return Level (n-year event):**
$$z_n = \begin{cases}
\mu - \frac{\sigma}{\xi}[1 - (-\log(1-1/n))^{-\xi}] & \xi \neq 0 \\
\mu - \sigma \log(-\log(1-1/n)) & \xi = 0
\end{cases}$$

---

## 2.2 Copulas for Multivariate Demand

### Copula Theory

**Sklar's Theorem:**
$$F(x_1, ..., x_d) = C(F_1(x_1), ..., F_d(x_d))$$

where $C$ is the copula function.

**Gaussian Copula:**
$$C_\Phi(u_1, ..., u_d) = \Phi_R(\Phi^{-1}(u_1), ..., \Phi^{-1}(u_d))$$

where $\Phi_R$ is the multivariate normal CDF with correlation matrix $R$.

**Application to Demand:**
```python
from scipy.stats import multivariate_normal
import numpy as np

def gaussian_copula_demand(demands, correlation_matrix):
    """
    Model correlated demands using Gaussian copula.
    """
    # Transform to uniform using marginals
    uniforms = [stats.norm.cdf(d) for d in demands]
    
    # Transform to multivariate normal
    normals = multivariate_normal.rvs(
        mean=np.zeros(len(demands)),
        cov=correlation_matrix
    )
    
    # Apply copula
    return normals
```

---

# 3. TIME SERIES MATHEMATICS

## 3.1 State Space Models

### Kalman Filter

**State Equation:**
$$\mathbf{x}_t = \mathbf{F}_t \mathbf{x}_{t-1} + \mathbf{w}_t$$

**Observation Equation:**
$$\mathbf{y}_t = \mathbf{H}_t \mathbf{x}_t + \mathbf{v}_t$$

where:
- $\mathbf{x}_t$: latent state
- $\mathbf{w}_t \sim \mathcal{N}(0, \mathbf{Q}_t)$: state noise
- $\mathbf{v}_t \sim \mathcal{N}(0, \mathbf{R}_t)$: observation noise

**Prediction Step:**
$$\hat{\mathbf{x}}_{t|t-1} = \mathbf{F}_t \hat{\mathbf{x}}_{t-1|t-1}$$
$$\mathbf{P}_{t|t-1} = \mathbf{F}_t \mathbf{P}_{t-1|t-1} \mathbf{F}_t^T + \mathbf{Q}_t$$

**Update Step:**
$$\mathbf{K}_t = \mathbf{P}_{t|t-1} \mathbf{H}_t^T (\mathbf{H}_t \mathbf{P}_{t|t-1} \mathbf{H}_t^T + \mathbf{R}_t)^{-1}$$
$$\hat{\mathbf{x}}_{t|t} = \hat{\mathbf{x}}_{t|t-1} + \mathbf{K}_t (\mathbf{y}_t - \mathbf{H}_t \hat{\mathbf{x}}_{t|t-1})$$
$$\mathbf{P}_{t|t} = (\mathbf{I} - \mathbf{K}_t \mathbf{H}_t) \mathbf{P}_{t|t-1}$$

---

### Dynamic Linear Models (DLM)

**Local Level Model:**
$$y_t = \mu_t + \epsilon_t$$
$$\mu_t = \mu_{t-1} + \eta_t$$

where $\epsilon_t \sim \mathcal{N}(0, \sigma_\epsilon^2)$, $\eta_t \sim \mathcal{N}(0, \sigma_\eta^2)$.

**Forecast Function:**
$$\hat{y}_{t+h|t} = \hat{\mu}_{t|t}$$

**Variance:**
$$\text{Var}(\hat{y}_{t+h|t}) = \sigma_\epsilon^2 + h \sigma_\eta^2$$

---

## 3.2 Spectral Analysis

### Fourier Analysis

**Discrete Fourier Transform (DFT):**
$$X_k = \sum_{n=0}^{N-1} x_n e^{-2\pi i kn/N}$$

**Power Spectral Density:**
$$S(f) = \lim_{T \to \infty} \frac{1}{T} \mathbb{E}\left[\left|\int_{-T}^{T} x(t) e^{-2\pi ift} dt\right|^2\right]$$

**Periodogram:**
$$\hat{S}(f_k) = \frac{1}{N} \left|\sum_{n=0}^{N-1} x_n e^{-2\pi i f_k n}\right|^2$$

where $f_k = k/N$.

**Application to Demand:**
```python
from scipy import signal
import numpy as np

def analyze_demand_spectrum(demand_data):
    """
    Spectral analysis of demand patterns.
    """
    # Compute periodogram
    freqs, psd = signal.periodogram(demand_data)
    
    # Detect dominant frequencies
    dominant_freq_idx = np.argmax(psd[1:]) + 1
    dominant_period = 1 / freqs[dominant_freq_idx]
    
    # Harmonic analysis
    harmonics = [1, 2, 3]  # Fundamental, 2nd, 3rd harmonics
    power_distribution = {}
    
    for h in harmonics:
        freq_idx = np.argmin(np.abs(freqs - h * freqs[dominant_freq_idx]))
        power_distribution[h] = psd[freq_idx]
    
    return {
        'dominant_period': dominant_period,
        'spectrum': (freqs, psd),
        'harmonics': power_distribution
    }
```

---

## 3.3 Wavelet Analysis

### Continuous Wavelet Transform

**Mother Wavelet:**
$$\psi_{a,b}(t) = \frac{1}{\sqrt{a}} \psi\left(\frac{t-b}{a}\right)$$

**Continuous Wavelet Transform:**
$$W(a, b) = \int_{-\infty}^{\infty} x(t) \psi_{a,b}^*(t) dt$$

**Morlet Wavelet:**
$$\psi(t) = e^{i\omega_0 t} e^{-t^2/2}$$

**Discrete Wavelet Transform (DWT):**
$$x(t) = \sum_{j,k} c_{j,k} \psi_{j,k}(t)$$

where $\psi_{j,k}(t) = 2^{j/2} \psi(2^j t - k)$.

**Multi-Resolution Analysis (MRA):**

$x(t)$ can be decomposed as:
$$x(t) = \sum_{j=J_0}^{J} \sum_{k} d_{j,k} \psi_{j,k}(t) + \sum_k c_{J,k} \phi_{J,k}(t)$$

where:
- $d_{j,k}$: detail coefficients at scale $j$
- $c_{J,k}$: approximation coefficients at scale $J$
- $\phi_{J,k}$: scaling function

**Application:**
```python
import pywt

def decompose_demand_wavelet(demand_data, wavelet='db4', levels=4):
    """
    Wavelet decomposition for demand patterns.
    """
    coeffs = pywt.wavedec(demand_data, wavelet, level=levels)
    
    # Reconstruct individual components
    components = {}
    for i, c in enumerate(coeffs):
        # Create full coefficient array
        coeff_list = [np.zeros_like(c)] * (levels + 1)
        coeff_list[levels - i] = c
        
        # Reconstruct
        components[f'level_{levels-i}'] = pywt.waverec(coeff_list, wavelet)
    
    return components, coeffs
```

---

# 4. OPTIMIZATION THEORY

## 4.1 Convex Optimization

### Convexity

**Definition:**
A function $f: \mathbb{R}^n \to \mathbb{R}$ is convex if:
$$f(\lambda \mathbf{x} + (1-\lambda)\mathbf{y}) \leq \lambda f(\mathbf{x}) + (1-\lambda) f(\mathbf{y})$$

for all $\mathbf{x}, \mathbf{y} \in \text{dom}(f)$ and $\lambda \in [0,1]$.

**Safety Stock as Convex Problem:**

Minimize total cost:
$$TC(Q, SS) = \frac{DS}{Q} + \frac{HQ}{2} + h \cdot SS + \pi \cdot \mathbb{E}[\max(0, D_{LT} - SS)]$$

where:
- $D$: annual demand
- $S$: ordering cost
- $H$: holding cost rate
- $h$: SS holding cost
- $\pi$: stockout cost
- $D_{LT}$: demand during lead time

**KKT Conditions:**

**Stationarity:**
$$\frac{\partial TC}{\partial Q} = -\frac{DS}{Q^2} + \frac{H}{2} = 0$$
$$\frac{\partial TC}{\partial SS} = h - \pi F_{D_{LT}}(SS) = 0$$

**Solving for SS:**
$$\boxed{F_{D_{LT}}(SS^*) = \frac{h}{\pi}}$$

This is the **critical fractile formula**!

---

### Lagrangian Method

**Constrained Optimization:**

Minimize:
$$f(\mathbf{x})$$

Subject to:
$$g_i(\mathbf{x}) \leq 0, \quad i = 1, ..., m$$
$$h_j(\mathbf{x}) = 0, \quad j = 1, ..., p$$

**Lagrangian:**
$$\mathcal{L}(\mathbf{x}, \boldsymbol{\lambda}, \boldsymbol{\nu}) = f(\mathbf{x}) + \sum_{i=1}^{m} \lambda_i g_i(\mathbf{x}) + \sum_{j=1}^{p} \nu_j h_j(\mathbf{x})$$

**Dual Problem:**
$$g(\boldsymbol{\lambda}, \boldsymbol{\nu}) = \inf_{\mathbf{x}} \mathcal{L}(\mathbf{x}, \boldsymbol{\lambda}, \boldsymbol{\nu})$$

**Strong Duality:**
If $f$ and $g_i$ are convex, and Slater's condition holds:
$$p^* = d^*$$

---

## 4.2 Stochastic Optimization

### Stochastic Programming

**Two-Stage Stochastic Program:**

**First Stage:** Order quantity $Q$ (before demand uncertainty)

**Second Stage:** Recourse decision (after demand observed)

**Objective:**
$$\min_{Q \geq 0} \quad c Q + \mathbb{E}_{\xi}[Q(x, \xi)]$$

where $Q(x, \xi)$ is the recourse function.

**For Newsvendor:**
$$Q(x, \xi) = \begin{cases}
p(D - x) & \text{if } D > x \text{ (underage)} \\
h(x - D) & \text{if } D \leq x \text{ (overage)}
\end{cases}$$

**Expected Value:**
$$\mathbb{E}[Q(x, D)] = p \int_x^{\infty} (D - x) f(D) dD + h \int_0^{x} (x - D) f(D) dD$$

**Differentiating:**
$$\frac{\partial \mathbb{E}[Q]}{\partial x} = -p(1 - F(x)) + h F(x) = 0$$

Solving:
$$F(x^*) = \frac{p}{p + h}$$

---

### Robust Optimization

**Uncertainty Set:**
$$\mathcal{U} = \{\boldsymbol{\xi} : \boldsymbol{\xi} \in [\boldsymbol{\xi}^- , \boldsymbol{\xi}^+]\}$$

**Robust Counterpart:**

Original problem:
$$\min_{\mathbf{x}} \quad f_0(\mathbf{x}, \boldsymbol{\xi})$$

Robust formulation:
$$\min_{\mathbf{x}} \quad \max_{\boldsymbol{\xi} \in \mathcal{U}} f_0(\mathbf{x}, \boldsymbol{\xi})$$

**For Inventory:**

Robust Reorder Point:
$$\boxed{PP_{robust} = \max_{\sigma_D \in [\sigma_D^-, \sigma_D^+]} (D_{max} \times LT + Z_\alpha \sigma_D \sqrt{LT})}$$

---

# 5. BAYESIAN METHODS

## 5.1 Bayesian Inference

### Prior to Posterior

**Prior:**
$$\boldsymbol{\theta} \sim p(\boldsymbol{\theta})$$

**Likelihood:**
$$p(\mathbf{y} | \boldsymbol{\theta}) = \prod_{i=1}^{n} p(y_i | \boldsymbol{\theta})$$

**Posterior:**
$$p(\boldsymbol{\theta} | \mathbf{y}) = \frac{p(\mathbf{y} | \boldsymbol{\theta}) p(\boldsymbol{\theta})}{p(\mathbf{y})}$$

**Posterior Predictive:**
$$p(y_{n+1} | \mathbf{y}) = \int p(y_{n+1} | \boldsymbol{\theta}) p(\boldsymbol{\theta} | \mathbf{y}) d\boldsymbol{\theta}$$

---

### Conjugate Priors

**Normal-Normal:**

Prior: $\theta \sim \mathcal{N}(\mu_0, \tau_0^2)$  
Likelihood: $y_i | \theta \sim \mathcal{N}(\theta, \sigma^2)$  
Posterior: $\theta | \mathbf{y} \sim \mathcal{N}(\mu_n, \tau_n^2)$

where:
$$\mu_n = \frac{\tau_0^2}{\tau_0^2 + \sigma^2/n} \bar{y} + \frac{\sigma^2/n}{\tau_0^2 + \sigma^2/n} \mu_0$$
$$\frac{1}{\tau_n^2} = \frac{1}{\tau_0^2} + \frac{n}{\sigma^2}$$

**Gamma-Poisson:**

Prior: $\lambda \sim \text{Gamma}(\alpha_0, \beta_0)$  
Likelihood: $y_i | \lambda \sim \text{Poisson}(\lambda)$  
Posterior: $\lambda | \mathbf{y} \sim \text{Gamma}(\alpha_n, \beta_n)$

where:
$$\alpha_n = \alpha_0 + \sum_{i=1}^{n} y_i$$
$$\beta_n = \beta_0 + n$$

---

### MCMC Methods

**Metropolis-Hastings:**

**Proposal Distribution:** $q(\boldsymbol{\theta}' | \boldsymbol{\theta})$

**Acceptance Probability:**
$$\alpha = \min\left(1, \frac{p(\boldsymbol{\theta}' | \mathbf{y}) q(\boldsymbol{\theta} | \boldsymbol{\theta}')}{p(\boldsymbol{\theta} | \mathbf{y}) q(\boldsymbol{\theta}' | \boldsymbol{\theta})}\right)$$

**Gibbs Sampling:**

For multi-dimensional $\boldsymbol{\theta} = (\theta_1, ..., \theta_p)$:

$$p(\theta_j^{(t+1)} | \theta_1^{(t+1)}, ..., \theta_{j-1}^{(t+1)}, \theta_{j+1}^{(t)}, ..., \theta_p^{(t)}, \mathbf{y})$$

Sample each dimension conditional on others.

---

## 5.2 Bayesian Optimization

### Acquisition Functions

**Expected Improvement (EI):**
$$EI(\mathbf{x}) = \mathbb{E}[\max(0, f^* - f(\mathbf{x}))]$$

**Under Gaussian Process:**
$$EI(\mathbf{x}) = \sigma(\mathbf{x})[\phi(Z) + Z \Phi(Z)]$$

where $Z = \frac{f^* - \mu(\mathbf{x})}{\sigma(\mathbf{x})}$.

**Probability of Improvement:**
$$PI(\mathbf{x}) = \Phi\left(\frac{f^* - \mu(\mathbf{x})}{\sigma(\mathbf{x})}\right)$$

**Upper Confidence Bound:**
$$UCB(\mathbf{x}) = \mu(\mathbf{x}) + \kappa \sigma(\mathbf{x})$$

where $\kappa = \sqrt{2 \log(t^{d/2 + 2} \pi^2 / 3\delta)}$ for confidence $1-\delta$.

---

### Gaussian Processes

**Prior:**
$$f(\mathbf{x}) \sim \mathcal{GP}(m(\mathbf{x}), k(\mathbf{x}, \mathbf{x}'))$$

**Squared Exponential Kernel:**
$$k(\mathbf{x}, \mathbf{x}') = \sigma_f^2 \exp\left(-\frac{1}{2}\sum_{d=1}^{D} \frac{(x_d - x'_d)^2}{\ell_d^2}\right)$$

**Mat√©rn 3/2:**
$$k(\mathbf{x}, \mathbf{x}') = \sigma_f^2 \left(1 + \frac{\sqrt{3} r}{\ell}\right) \exp\left(-\frac{\sqrt{3} r}{\ell}\right)$$

where $r = \|\mathbf{x} - \mathbf{x}'\|$.

**Predictive Distribution:**

Given observations $\mathbf{y}$ at $\mathbf{X}$:

$$p(f_* | \mathbf{x}_*, \mathbf{X}, \mathbf{y}) \sim \mathcal{N}(\mu_*, \sigma_*^2)$$

where:
$$\mu_* = \mathbf{k}_*^T (\mathbf{K} + \sigma_n^2 \mathbf{I})^{-1} \mathbf{y}$$
$$\sigma_*^2 = k(\mathbf{x}_*, \mathbf{x}_*) - \mathbf{k}_*^T (\mathbf{K} + \sigma_n^2 \mathbf{I})^{-1} \mathbf{k}_*$$

---

# 6. STOCHASTIC PROCESSES

## 6.1 Brownian Motion

### Wiener Process

**Definition:**
$$\{W(t) : t \geq 0\}$$

Properties:
1. $W(0) = 0$
2. Independent increments
3. For $0 \leq s < t$: $W(t) - W(s) \sim \mathcal{N}(0, t-s)$
4. Continuous sample paths

**Geometric Brownian Motion:**

$$dS(t) = \mu S(t) dt + \sigma S(t) dW(t)$$

Solution:
$$S(t) = S(0) \exp\left((\mu - \frac{\sigma^2}{2})t + \sigma W(t)\right)$$

---

### Ito's Lemma

For process $f(S(t), t)$:

$$df = \frac{\partial f}{\partial t} dt + \frac{\partial f}{\partial S} dS + \frac{1}{2} \frac{\partial^2 f}{\partial S^2} (dS)^2$$

where $(dS)^2 = \sigma^2 S^2 dt$.

**Application to Demand Drift:**

If demand follows:
$$dD(t) = \mu dt + \sigma dW(t)$$

Then:
$$D(t) = D(0) + \mu t + \sigma W(t)$$

Forecast:
$$\mathbb{E}[D(t+h) | D(t)] = D(t) + \mu h$$
$$\text{Var}(D(t+h) | D(t)) = \sigma^2 h$$

---

## 6.2 Ornstein-Uhlenbeck Process

### Mean-Reverting Demand

**Stochastic Differential Equation:**

$$dD(t) = \theta(\mu - D(t)) dt + \sigma dW(t)$$

where:
- $\mu$: long-term mean
- $\theta > 0$: speed of reversion
- $\sigma$: volatility

**Solution:**
$$D(t) = e^{-\theta t} D(0) + \mu(1 - e^{-\theta t}) + \sigma \int_0^{t} e^{-\theta(t-s)} dW(s)$$

**Properties:**
- $\mathbb{E}[D(t)] = e^{-\theta t} D(0) + \mu(1 - e^{-\theta t})$
- $\lim_{t \to \infty} \mathbb{E}[D(t)] = \mu$ (mean-reverting!)
- $\text{Var}(D(t)) = \frac{\sigma^2}{2\theta}(1 - e^{-2\theta t})$

**Equilibrium Variance:**
$$\lim_{t \to \infty} \text{Var}(D(t)) = \frac{\sigma^2}{2\theta}$$

---

## 6.3 Compound Renewal Process

### For Intermittent Demand

**Model:**

Arrivals: $\{N(t): t \geq 0\}$ renewal process  
Quantity per arrival: $\{X_i\}_{i=1}^{\infty}$ i.i.d.

**Total Demand:**
$$D(t) = \sum_{i=1}^{N(t)} X_i$$

**Expected Value:**
$$\mathbb{E}[D(t)] = \mathbb{E}[N(t)] \cdot \mathbb{E}[X]$$

**Variance:**
$$\text{Var}(D(t)) = \mathbb{E}[N(t)] \cdot \text{Var}(X) + \text{Var}(N(t)) \cdot (\mathbb{E}[X])^2$$

**Asymptotic Distribution:**

As $t \to \infty$:

$$\frac{D(t) - \mathbb{E}[D(t)]}{\sqrt{\text{Var}(D(t))}} \xrightarrow{d} \mathcal{N}(0, 1)$$

---

# 7. INFORMATION THEORY

## 7.1 Entropy and Divergence

### Shannon Entropy

**Definition:**

For discrete random variable $X$ with PMF $p(x)$:

$$H(X) = -\sum_{x \in \mathcal{X}} p(x) \log_2 p(x)$$

**For continuous variable:**

$$H(X) = -\int_{-\infty}^{\infty} f(x) \log f(x) dx$$

**Interpretation:** Expected surprise/bits needed to encode outcomes.

**Maximum Entropy Distribution:**

For given mean and variance: Gaussian!

$$f^*(x) = \arg\max_{f} H(X) \text{ subject to } \mathbb{E}[X] = \mu, \text{Var}(X) = \sigma^2$$

---

### Kullback-Leibler Divergence

**Definition:**

$$D_{KL}(P \| Q) = \sum_{x} p(x) \log \frac{p(x)}{q(x)}$$

**Properties:**
- $D_{KL}(P \| Q) \geq 0$ (Gibbs inequality)
- $D_{KL}(P \| Q) = 0$ iff $P = Q$

**For Distributions:**

$$D_{KL}(\mathcal{N}_1 \| \mathcal{N}_2) = \frac{1}{2}\left[\frac{\sigma_2^2}{\sigma_1^2} + \frac{(\mu_2 - \mu_1)^2}{\sigma_1^2} - 1 + \log\left(\frac{\sigma_1^2}{\sigma_2^2}\right)\right]$$

---

### Mutual Information

**Definition:**

$$I(X; Y) = \sum_{x,y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)}$$

**Properties:**
- $I(X; Y) = H(X) - H(X | Y)$
- $I(X; Y) = I(Y; X)$ (symmetric)

**For Feature Selection:**

Select features maximizing mutual information with target:
$$X^* = \arg\max_{X_i} I(X_i; Y)$$

---

## 7.2 Information Gain and Decision Trees

### Gini Impurity

**Definition:**

$$Gini(t) = 1 - \sum_{i=1}^{C} p_i^2(t)$$

where $p_i(t)$ is probability of class $i$ at node $t$.

**Information Gain:**

$$IG(t) = Gini(t) - \sum_{children} \frac{|t_{child}|}{|t|} Gini(t_{child})$$

**Optimal Split:**

$$s^* = \arg\max_{s} IG(s)$$

---

# 8. ADVANCED RISK METRICS

## 8.1 Value at Risk (VaR)

### Portfolio Risk

**Definition:**

$$\text{VaR}_\alpha(X) = \inf\{x : P(X \leq x) \geq \alpha\}$$

Interpretation: Maximum loss with $(1-\alpha)$ confidence.

**For Demand Shortfall:**

If demand exceeds supply, shortfall $S = \max(0, D - Q)$:

$$\text{VaR}_\alpha(S) = \inf\{s : P(S \leq s) \geq \alpha\}$$

**Parametric Estimation:**

If $D \sim \mathcal{N}(\mu, \sigma^2)$:

$$\text{VaR}_\alpha = \sigma \Phi^{-1}(\alpha) - \mu$$

---

## 8.2 Conditional Value at Risk (CVaR)

### Expected Shortfall

**Definition:**

$$\text{CVaR}_\alpha(X) = \mathbb{E}[X | X \leq \text{VaR}_\alpha(X)]$$

**Formula:**

$$\text{CVaR}_\alpha = \frac{1}{\alpha} \int_0^{\alpha} \text{VaR}_u du$$

**For Normal Distribution:**

$$\text{CVaR}_\alpha = \mu - \sigma \frac{\phi(\Phi^{-1}(\alpha))}{\alpha}$$

---

## 8.3 Coherent Risk Measures

### Properties

A risk measure $\rho$ is coherent if:

1. **Monotonicity:** $X \leq Y \Rightarrow \rho(X) \geq \rho(Y)$
2. **Translation Invariance:** $\rho(X + c) = \rho(X) - c$
3. **Positive Homogeneity:** $\rho(\lambda X) = \lambda \rho(X)$ for $\lambda \geq 0$
4. **Subadditivity:** $\rho(X + Y) \leq \rho(X) + \rho(Y)$

**Note:** VaR violates subadditivity!  
**Solution:** Use CVaR (satisfies all).

---

# 9. MATHEMATICAL PROOFS

## 9.1 Proof: Safety Stock Formula

**Starting Point:**

We want $SS$ such that:
$$P(D_{LT} \leq Q) = \alpha$$

where $Q = \mu_{LT} + SS$ and $D_{LT} \sim \mathcal{N}(\mu_{LT}, \sigma_{LT}^2)$.

**Standardize:**
$$P\left(Z \leq \frac{SS}{\sigma_{LT}}\right) = \alpha$$

**Solving:**
$$\frac{SS}{\sigma_{LT}} = \Phi^{-1}(\alpha) = Z_\alpha$$

**Result:**
$$\boxed{SS = Z_\alpha \sigma_{LT}}$$

**For variable lead time:**

Use approximation $\sigma_{LT} \approx \sigma_D \sqrt{LT}$ if $LT$ fixed:

$$\boxed{SS = Z_\alpha \sigma_D \sqrt{LT}}$$

---

## 9.2 Proof: Newsvendor Critical Fractile

**Objective:**

$$\min_{Q} E[C(Q, D)] = E[h(Q-D)^+ + p(D-Q)^+]$$

**Expand:**
$$= h \int_0^{Q} (Q - x) f(x) dx + p \int_Q^{\infty} (x - Q) f(x) dx$$

**Differentiate w.r.t $Q$:**

$$\frac{dE[C]}{dQ} = h \int_0^{Q} f(x) dx - p \int_Q^{\infty} f(x) dx = 0$$

**Simplify:**

$$h F(Q) - p(1 - F(Q)) = 0$$
$$h F(Q) + p F(Q) = p$$
$$F(Q) (h + p) = p$$

**Result:**

$$\boxed{F(Q^*) = \frac{p}{p + h}}$$

---

## 9.3 Proof: ARIMA Forecast Variance

**ARIMA(p,d,q):**
$$\Phi(B)(1-B)^d D_t = \Theta(B) \epsilon_t$$

**Forecast:**
$$\hat{D}_{t+h} = \sum_{i=1}^{p} \phi_i \hat{D}_{t+h-i} + \sum_{j=1}^{q} \theta_j \hat{\epsilon}_{t+h-j}$$

**Error:**
$$e_{t+h} = D_{t+h} - \hat{D}_{t+h}$$

**Variance:**

For stationary AR(p):

$$\text{Var}(e_{t+h}) = \sigma^2 \sum_{i=0}^{h-1} \psi_i^2$$

where $\psi_i$ are MA coefficients.

**As $h \to \infty$:**

$$\text{Var}(e_{t+h}) \to \sigma^2 \sum_{i=0}^{\infty} \psi_i^2$$

---

# 10. ADVANCED OPTIMIZATION

## 10.1 Multi-Objective Optimization

### Pareto Front

**Objective Vector:**
$$\mathbf{f}(\mathbf{x}) = [f_1(\mathbf{x}), f_2(\mathbf{x}), ..., f_k(\mathbf{x})]$$

**Pareto Dominance:**

$\mathbf{x}_1$ dominates $\mathbf{x}_2$ if:
- $f_i(\mathbf{x}_1) \leq f_i(\mathbf{x}_2)$ for all $i$
- $f_j(\mathbf{x}_1) < f_j(\mathbf{x}_2)$ for at least one $j$

**For Inventory:**

Objectives:
1. Minimize total cost
2. Maximize service level
3. Minimize stockout probability

**NSGA-II Algorithm:**

1. **Non-dominated sorting**
2. **Crowding distance**
3. **Tournament selection**
4. **Crossover and mutation**
5. **Elite preservation**

---

## 10.2 Integer Programming

### Knapsack Problem

**Formulation:**

$$\max \sum_{i=1}^{n} v_i x_i$$

Subject to:
$$\sum_{i=1}^{n} w_i x_i \leq W$$
$$x_i \in \{0, 1\}$$

**Dynamic Programming:**

Define $dp[i][w]$ = max value using first $i$ items with capacity $w$.

Recurrence:
$$dp[i][w] = \max(dp[i-1][w], dp[i-1][w-w_i] + v_i)$$

**Application:**

Selecting which items to stock given budget constraint.

---

### Mixed Integer Programming (MIP)

**Formulation:**
$$\min \mathbf{c}^T \mathbf{x}$$

Subject to:
$$\mathbf{A}\mathbf{x} \leq \mathbf{b}$$
$$x_i \in \{0, 1\} \text{ for } i \in I$$
$$x_i \in \mathbb{R}^+ \text{ for } i \notin I$$

**Branch and Bound:**

1. Solve LP relaxation
2. If fractional, branch on variable
3. Bound: if relaxed cost > incumbent, prune
4. Iterate until convergence

---

# RESUMO FINAL DAS F√ìRMULAS

## Core Inventory Formulas

| F√≥rmula | Valor |
|---------|-------|
| Safety Stock b√°sico | $SS = Z_\alpha \sigma \sqrt{LT}$ |
| Safety Stock avan√ßado | $SS = Z_\alpha \sqrt{LT \sigma_D^2 + D^2 \sigma_{LT}^2}$ |
| Reorder Point | $PP = (D \times LT) + SS$ |
| EOQ | $Q^* = \sqrt{2DS/H}$ |
| Newsvendor fractile | $F(q^*) = c_u/(c_u + c_o)$ |

---

## Advanced Formulas

| Conceito | F√≥rmula |
|----------|---------|
| **Kalman Filter Covariance** | $\mathbf{P}_{t\|t} = (\mathbf{I} - \mathbf{K}_t \mathbf{H}_t) \mathbf{P}_{t\|t-1}$ |
| **Posterior Gaussian** | $\mu_n = \frac{\tau_0^2}{\tau_0^2 + \sigma^2/n} \bar{y} + ...$ |
| **GEV Return Level** | $z_n = \mu - \frac{\sigma}{\xi}[1 - (-\log(1-1/n))^{-\xi}]$ |
| **Ornstein-Uhlenbeck** | $D(t) = e^{-\theta t} D(0) + \mu(1 - e^{-\theta t}) + ...$ |
| **Mutual Information** | $I(X;Y) = H(X) - H(X\|Y)$ |
| **CVaR** | $\text{CVaR}_\alpha = \mu - \sigma \frac{\phi(\Phi^{-1}(\alpha))}{\alpha}$ |

---

# CONCLUS√ÉO

Este documento expandiu a documenta√ß√£o com:
- ‚úÖ Teoria matem√°tica avan√ßada
- ‚úÖ Provas e deriva√ß√µes completas
- ‚úÖ M√©todos estoc√°sticos
- ‚úÖ Otimiza√ß√£o multi-objetivo
- ‚úÖ Teoria de informa√ß√µes
- ‚úÖ Processos avan√ßados

**Total:** 100+ se√ß√µes de matem√°tica avan√ßada!

---

**Nova Corrente Grand Prix SENAI**

**ADVANCED MATHEMATICAL THEORY EXPANSION - Version 2.0**

*November 2025*

