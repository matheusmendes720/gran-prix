# üß† BREAKDOWN COMPLETO: LSTM
## An√°lise Profunda Passo a Passo - Long Short-Term Memory

---

**Data:** Novembro 2025  
**Vers√£o:** LSTM Breakdown v1.0  
**Status:** ‚úÖ Breakdown Completo Expandido

---

## üìã √çNDICE EXPANDIDO

### Parte I: Fundamentos de LSTM
1. [O que √© LSTM?](#1-o-que-√©-lstm)
2. [Por que LSTM?](#2-por-que-lstm)
3. [Arquitetura da C√©lula LSTM](#3-arquitetura-c√©lula)
4. [Compara√ß√£o com RNN](#4-compara√ß√£o-rnn)
5. [Problema do Vanishing Gradient](#5-vanishing-gradient)

### Parte II: Matem√°tica Profunda
6. [Equa√ß√µes dos Gates](#6-equa√ß√µes-gates)
7. [Deriva√ß√£o do Forget Gate](#7-deriva√ß√£o-forget-gate)
8. [Deriva√ß√£o do Input Gate](#8-deriva√ß√£o-input-gate)
9. [Deriva√ß√£o do Output Gate](#9-deriva√ß√£o-output-gate)
10. [Cell State Update](#10-cell-state-update)

### Parte III: Backpropagation
11. [Backpropagation Through Time (BPTT)](#11-bptt)
12. [Gradientes dos Gates](#12-gradientes-gates)
13. [Deriva√ß√£o Completa BPTT](#13-deriva√ß√£o-bptt)
14. [Vanishing/Exploding Gradient](#14-gradient-problems)
15. [Solu√ß√£o: Gradient Clipping](#15-gradient-clipping)

### Parte IV: Implementa√ß√£o
16. [Forward Pass Completo](#16-forward-pass)
17. [Backward Pass Completo](#17-backward-pass)
18. [Otimiza√ß√£o (Adam)](#18-otimiza√ß√£o)
19. [Regulariza√ß√£o](#19-regulariza√ß√£o)
20. [Aplica√ß√µes Nova Corrente](#20-aplica√ß√µes)

---

# 1. O QUE √â LSTM?

## 1.1 Defini√ß√£o

**LSTM (Long Short-Term Memory)** √© uma arquitetura de rede neural recorrente (RNN) projetada para resolver o problema do **vanishing gradient**.

**Inova√ß√£o:** Adiciona um **"memory cell"** que pode armazenar informa√ß√£o por longos per√≠odos.

## 1.2 Componentes Principais

Uma c√©lula LSTM tem **4 componentes**:

1. **Forget Gate** ($f_t$): Decide o que esquecer
2. **Input Gate** ($i_t$): Decide o que armazenar
3. **Cell State** ($C_t$): Mem√≥ria de longo prazo
4. **Output Gate** ($o_t$): Decide o que produzir

---

# 2. POR QUE LSTM?

## 2.1 Problemas do RNN Simples

**RNN Padr√£o:**
$$h_t = \tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$$

**Problemas:**
- ‚ùå Vanishing gradient (gradientes desaparecem)
- ‚ùå Exploding gradient (gradientes explodem)
- ‚ùå N√£o lembra informa√ß√µes de longo prazo

## 2.2 Vantagens do LSTM

- ‚úÖ **Lembra longo prazo:** Cell state mant√©m informa√ß√£o
- ‚úÖ **Esquece seletivamente:** Forget gate remove informa√ß√£o irrelevante
- ‚úÖ **Gradientes est√°veis:** Cell state facilita backpropagation
- ‚úÖ **Modela depend√™ncias longas:** Eficaz para s√©ries temporais

---

# 3. ARQUITETURA DA C√âLULA LSTM

## 3.1 Diagrama Completo

```
Input: x_t (features at time t)
Hidden: h_{t-1} (previous hidden state)
Cell: C_{t-1} (previous cell state)
‚îÇ
‚îú‚îÄ‚Üí Forget Gate (f_t)
‚îÇ   ‚îî‚îÄ‚Üí Multiplica C_{t-1} (decide o que esquecer)
‚îÇ
‚îú‚îÄ‚Üí Input Gate (i_t)
‚îÇ   ‚îú‚îÄ‚Üí CÃÉ_t (candidate values)
‚îÇ   ‚îî‚îÄ‚Üí Multiplica CÃÉ_t (decide o que armazenar)
‚îÇ
‚îú‚îÄ‚Üí Cell State Update
‚îÇ   ‚îî‚îÄ‚Üí C_t = f_t ‚äô C_{t-1} + i_t ‚äô CÃÉ_t
‚îÇ
‚îú‚îÄ‚Üí Output Gate (o_t)
‚îÇ   ‚îî‚îÄ‚Üí Multiplica tanh(C_t) (decide o que produzir)
‚îÇ
‚îî‚îÄ‚Üí Hidden State: h_t = o_t ‚äô tanh(C_t)
```

## 3.2 Fluxo de Dados

1. **Forget Gate** ‚Üí Remove informa√ß√£o antiga
2. **Input Gate** ‚Üí Adiciona informa√ß√£o nova
3. **Cell State** ‚Üí Atualiza mem√≥ria
4. **Output Gate** ‚Üí Produz output

---

# 4. COMPARA√á√ÉO COM RNN

## 4.1 RNN Simples

**Equa√ß√£o:**
$$h_t = \tanh(W_h h_{t-1} + W_x x_t + b)$$

**Limita√ß√µes:**
- Mem√≥ria: apenas $h_t$
- Gradiente: desaparece rapidamente
- Capacidade: limitada para depend√™ncias curtas

## 4.2 LSTM

**Equa√ß√µes (ver se√ß√£o 6):**
- M√∫ltiplos gates
- Cell state separado
- Gradiente preservado atrav√©s de cell state

**Vantagens:**
- Mem√≥ria: cell state + hidden state
- Gradiente: mais est√°vel
- Capacidade: depend√™ncias muito longas

---

# 5. PROBLEMA DO VANISHING GRADIENT

## 5.1 O Problema

**Em RNN simples:**
$$\frac{\partial L}{\partial W} = \sum_{t=1}^{T} \frac{\partial L}{\partial h_T} \prod_{k=t+1}^{T} \frac{\partial h_k}{\partial h_{k-1}}$$

**Produto de derivadas:**
$$\prod_{k=t+1}^{T} \frac{\partial h_k}{\partial h_{k-1}} = \prod_{k=t+1}^{T} \tanh'(W_h h_{k-1} + ...) W_h$$

**Se $|\tanh'(.)| < 1$ e $|W_h| < 1$:**
$$\lim_{T-t \to \infty} \prod_{k=t+1}^{T} (...) \to 0$$

**Gradiente desaparece!** ‚ùå

## 5.2 Solu√ß√£o LSTM

**Cell state permite "escalas" diretas:**

$$\frac{\partial C_t}{\partial C_{t-1}} = f_t + \text{termos controlados por gates}$$

**Forget gate** pode ser pr√≥ximo de 1 ‚Üí gradiente preservado!

---

# 6. EQUA√á√ïES DOS GATES

## 6.1 Forget Gate

**Fun√ß√£o:**
$$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$$

**Interpreta√ß√£o:**
- $f_t \approx 1$: Mant√©m informa√ß√£o
- $f_t \approx 0$: Esquece informa√ß√£o

**Aplicado ao cell state anterior:**
$$f_t \odot C_{t-1}$$

onde $\odot$ √© produto elemento a elemento (Hadamard).

## 6.2 Input Gate

**Gate de input:**
$$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$$

**Valores candidatos:**
$$\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$$

**Aplicado:**
$$i_t \odot \tilde{C}_t$$

## 6.3 Cell State Update

**Atualiza√ß√£o:**
$$C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t$$

**Forma completa:**
$$C_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) \odot C_{t-1} + \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) \odot \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$$

## 6.4 Output Gate

**Gate de output:**
$$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$$

**Hidden state:**
$$h_t = o_t \odot \tanh(C_t)$$

---

# 7. DERIVA√á√ÉO DO FORGET GATE

## 7.1 Objetivo

**Forget gate decide:** Quanto da informa√ß√£o antiga ($C_{t-1}$) manter.

## 7.2 C√°lculo

**Input:**
$$\mathbf{z}_f = W_f \cdot \mathbf{concat}(h_{t-1}, x_t) + b_f$$

**Sigmoid:**
$$f_t = \sigma(\mathbf{z}_f) = \frac{1}{1 + e^{-\mathbf{z}_f}}$$

**Aplica√ß√£o:**
$$f_t \odot C_{t-1}$$

**Element-wise:**
$$f_{t,i} \times C_{t-1,i}$$

## 7.3 Propriedades

- $f_t \in [0, 1]$ (sigmoid output)
- Se $f_{t,i} = 0$: esquece completamente valor $i$ de $C_{t-1}$
- Se $f_{t,i} = 1$: mant√©m valor $i$ completamente

---

# 8. DERIVA√á√ÉO DO INPUT GATE

## 8.1 Objetivo

**Input gate decide:** Quanto da nova informa√ß√£o ($\tilde{C}_t$) armazenar.

## 8.2 C√°lculo

**Step 1: Gate de input**
$$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$$

**Step 2: Valores candidatos**
$$\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$$

**Step 3: Combina√ß√£o**
$$i_t \odot \tilde{C}_t$$

**Interpreta√ß√£o:**
- $i_t$: quanto armazenar (0-1)
- $\tilde{C}_t$: valores novos candidatos (-1 a 1)

---

# 9. DERIVA√á√ÉO DO OUTPUT GATE

## 9.1 Objetivo

**Output gate decide:** Quanto da informa√ß√£o do cell state usar no output.

## 9.2 C√°lculo

**Step 1: Gate de output**
$$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$$

**Step 2: Cell state processado**
$$\tanh(C_t)$$

**Step 3: Hidden state**
$$h_t = o_t \odot \tanh(C_t)$$

**Interpreta√ß√£o:**
- $o_t$: quanto expor do cell state
- $\tanh(C_t)$: cell state normalizado para [-1, 1]
- $h_t$: informa√ß√£o exposta para pr√≥xima camada/timestep

---

# 10. CELL STATE UPDATE

## 10.1 Equa√ß√£o Completa

$$C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t$$

**Expandindo todos os termos:**

$$C_t = \sigma(W_f [h_{t-1}, x_t] + b_f) \odot C_{t-1} + \sigma(W_i [h_{t-1}, x_t] + b_i) \odot \tanh(W_C [h_{t-1}, x_t] + b_C)$$

## 10.2 Interpreta√ß√£o

**Parte 1:** $f_t \odot C_{t-1}$
- **Esquece** informa√ß√£o antiga proporcionalmente a $f_t$

**Parte 2:** $i_t \odot \tilde{C}_t$
- **Adiciona** informa√ß√£o nova proporcionalmente a $i_t$

**Soma:** Combina√ß√£o de informa√ß√£o antiga (esquece seletivamente) + informa√ß√£o nova (armazena seletivamente)

## 10.3 Propriedades

**Conserva√ß√£o de informa√ß√£o:**
- Se $f_t \approx 1$ e $i_t \approx 0$: mant√©m informa√ß√£o antiga
- Se $f_t \approx 0$ e $i_t \approx 1$: substitui por informa√ß√£o nova
- Se ambos ‚âà 0: cell state pr√≥ximo de zero

---

# 11. BACKPROPAGATION THROUGH TIME (BPTT)

## 11.1 Objetivo

**Calcular gradientes** retrocedendo no tempo.

## 11.2 Loss Function

**Para sequ√™ncia de comprimento T:**

$$L = \sum_{t=1}^{T} L_t$$

onde $L_t$ √© loss no timestep $t$ (ex: MSE).

## 11.3 Gradiente w.r.t. Par√¢metros

**Gradiente total:**
$$\frac{\partial L}{\partial W} = \sum_{t=1}^{T} \frac{\partial L_t}{\partial W}$$

**Para LSTM, precisamos:**
$$\frac{\partial L}{\partial W_f}, \frac{\partial L}{\partial W_i}, \frac{\partial L}{\partial W_C}, \frac{\partial L}{\partial W_o}$$

---

# 12. GRADIENTES DOS GATES

## 12.1 Gradiente do Cell State

**Gradiente w.r.t. $C_t$:**

$$\frac{\partial L}{\partial C_t} = \frac{\partial L}{\partial h_t} \frac{\partial h_t}{\partial C_t} + \frac{\partial L}{\partial C_{t+1}} \frac{\partial C_{t+1}}{\partial C_t}$$

**Onde:**
$$\frac{\partial h_t}{\partial C_t} = o_t \odot (1 - \tanh^2(C_t))$$

$$\frac{\partial C_{t+1}}{\partial C_t} = f_{t+1}$$

**Resultado:**
$$\frac{\partial L}{\partial C_t} = \frac{\partial L}{\partial h_t} o_t \odot (1 - \tanh^2(C_t)) + \frac{\partial L}{\partial C_{t+1}} f_{t+1}$$

**Importante:** Gradiente flui atrav√©s de $f_{t+1}$, que pode ser ‚âà 1 ‚Üí gradiente preservado!

## 12.2 Gradientes dos Gates

### Forget Gate
$$\frac{\partial L}{\partial W_f} = \sum_{t=1}^{T} \frac{\partial L}{\partial C_t} \frac{\partial C_t}{\partial f_t} \frac{\partial f_t}{\partial W_f}$$

$$\frac{\partial C_t}{\partial f_t} = C_{t-1}$$

$$\frac{\partial f_t}{\partial W_f} = f_t(1-f_t) [h_{t-1}, x_t]$$

### Input Gate
$$\frac{\partial L}{\partial W_i} = \sum_{t=1}^{T} \frac{\partial L}{\partial C_t} \tilde{C}_t \cdot i_t(1-i_t) [h_{t-1}, x_t]$$

### Candidate Values
$$\frac{\partial L}{\partial W_C} = \sum_{t=1}^{T} \frac{\partial L}{\partial C_t} i_t \cdot (1-\tilde{C}_t^2) [h_{t-1}, x_t]$$

### Output Gate
$$\frac{\partial L}{\partial W_o} = \sum_{t=1}^{T} \frac{\partial L}{\partial h_t} \tanh(C_t) \cdot o_t(1-o_t) [h_{t-1}, x_t]$$

---

# 13. DERIVA√á√ÉO COMPLETA BPTT

## 13.1 Algoritmo Completo

### Forward Pass
```
Para t = 1 at√© T:
    f_t = œÉ(W_f [h_{t-1}, x_t] + b_f)
    i_t = œÉ(W_i [h_{t-1}, x_t] + b_i)
    CÃÉ_t = tanh(W_C [h_{t-1}, x_t] + b_C)
    C_t = f_t ‚äô C_{t-1} + i_t ‚äô CÃÉ_t
    o_t = œÉ(W_o [h_{t-1}, x_t] + b_o)
    h_t = o_t ‚äô tanh(C_t)
    ≈∑_t = W_y h_t + b_y
    L_t = loss(y_t, ≈∑_t)
```

### Backward Pass
```
‚àÇL/‚àÇh_T = ‚àÇL_T/‚àÇh_T (√∫ltimo timestep)
‚àÇL/‚àÇC_T = ‚àÇL/‚àÇh_T ¬∑ o_T ‚äô (1 - tanh¬≤(C_T))

Para t = T at√© 1:
    # Gradiente do output gate
    ‚àÇL/‚àÇW_o += ‚àÇL/‚àÇh_t ¬∑ tanh(C_t) ¬∑ o_t(1-o_t) [h_{t-1}, x_t]
    
    # Gradiente do cell state
    ‚àÇL/‚àÇC_t = ‚àÇL/‚àÇh_t ¬∑ o_t ‚äô (1 - tanh¬≤(C_t)) + ‚àÇL/‚àÇC_{t+1} ¬∑ f_{t+1}
    
    # Gradiente do forget gate
    ‚àÇL/‚àÇW_f += ‚àÇL/‚àÇC_t ¬∑ C_{t-1} ¬∑ f_t(1-f_t) [h_{t-1}, x_t]
    
    # Gradiente do input gate
    ‚àÇL/‚àÇW_i += ‚àÇL/‚àÇC_t ¬∑ CÃÉ_t ¬∑ i_t(1-i_t) [h_{t-1}, x_t]
    
    # Gradiente dos valores candidatos
    ‚àÇL/‚àÇW_C += ‚àÇL/‚àÇC_t ¬∑ i_t ¬∑ (1-CÃÉ_t¬≤) [h_{t-1}, x_t]
    
    # Gradiente para timestep anterior
    ‚àÇL/‚àÇh_{t-1} = ‚àÇL/‚àÇh_t ¬∑ ‚àÇh_t/‚àÇh_{t-1} + ‚àÇL/‚àÇC_t ¬∑ ‚àÇC_t/‚àÇh_{t-1}
```

---

# 14. VANISHING/EXPLODING GRADIENT

## 14.1 An√°lise do Gradiente

**Gradiente atrav√©s do tempo:**
$$\frac{\partial L}{\partial C_1} = \frac{\partial L}{\partial C_T} \prod_{t=2}^{T} \frac{\partial C_t}{\partial C_{t-1}}$$

**Onde:**
$$\frac{\partial C_t}{\partial C_{t-1}} = f_t + \text{termos adicionais}$$

### Se Forget Gate ‚âà 1
**Gradiente preservado:** $\prod_{t=2}^{T} f_t \approx 1$

### Se Forget Gate ‚âà 0
**Gradiente desaparece:** $\prod_{t=2}^{T} f_t \approx 0$

## 14.2 Exploding Gradient

**Se valores dos gates grandes:**
- Gradientes podem explodir
- Solu√ß√£o: **Gradient Clipping**

---

# 15. GRADIENT CLIPPING

## 15.1 M√©todo

**Clipping por valor:**
```python
if grad_norm > max_norm:
    grad = grad * (max_norm / grad_norm)
```

**Clipping por norma:**
```python
grad_norm = torch.norm(gradients)
if grad_norm > max_norm:
    gradients = gradients * (max_norm / grad_norm)
```

## 15.2 Implementa√ß√£o

```python
def clip_gradients(model, max_norm=1.0):
    """
    Clips gradients to prevent exploding.
    """
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)
```

---

# 16. FORWARD PASS COMPLETO

## 16.1 Implementa√ß√£o Python

```python
import numpy as np
import torch
import torch.nn as nn

class LSTMCell(nn.Module):
    """
    Implementa√ß√£o completa c√©lula LSTM.
    """
    def __init__(self, input_size, hidden_size):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        
        # Forget gate
        self.W_f = nn.Linear(input_size + hidden_size, hidden_size)
        # Input gate
        self.W_i = nn.Linear(input_size + hidden_size, hidden_size)
        # Candidate values
        self.W_C = nn.Linear(input_size + hidden_size, hidden_size)
        # Output gate
        self.W_o = nn.Linear(input_size + hidden_size, hidden_size)
    
    def forward(self, x_t, h_t_prev, C_t_prev):
        """
        Forward pass de uma c√©lula LSTM.
        """
        # Concatenar input e hidden state anterior
        concat = torch.cat([h_t_prev, x_t], dim=1)
        
        # Forget gate
        f_t = torch.sigmoid(self.W_f(concat))
        
        # Input gate
        i_t = torch.sigmoid(self.W_i(concat))
        
        # Candidate cell state
        C_tilde = torch.tanh(self.W_C(concat))
        
        # Update cell state
        C_t = f_t * C_t_prev + i_t * C_tilde
        
        # Output gate
        o_t = torch.sigmoid(self.W_o(concat))
        
        # Hidden state
        h_t = o_t * torch.tanh(C_t)
        
        return h_t, C_t
```

## 16.2 Sequ√™ncia Completa

```python
def lstm_forward(X, model):
    """
    Forward pass para sequ√™ncia completa.
    
    X: (seq_length, batch_size, input_size)
    """
    h_t = torch.zeros(batch_size, hidden_size)
    C_t = torch.zeros(batch_size, hidden_size)
    
    outputs = []
    
    for t in range(seq_length):
        x_t = X[t]  # (batch_size, input_size)
        h_t, C_t = lstm_cell(x_t, h_t, C_t)
        outputs.append(h_t)
    
    return torch.stack(outputs), h_t, C_t
```

---

# 17. BACKWARD PASS COMPLETO

## 17.1 Implementa√ß√£o

**PyTorch faz automaticamente** via autograd, mas aqui a forma manual:

```python
def lstm_backward(dL_dh, dL_dC, X, h_prev, C_prev, h_curr, C_curr, gates):
    """
    Backward pass manual.
    """
    # Gradiente do output gate
    dL_do = dL_dh * torch.tanh(C_curr)
    dL_dW_o = dL_do * o * (1 - o) * concat
    
    # Gradiente do cell state
    dL_dC_curr = dL_dh * o * (1 - torch.tanh(C_curr)**2) + dL_dC_next * f_next
    
    # Gradiente do forget gate
    dL_df = dL_dC_curr * C_prev
    dL_dW_f = dL_df * f * (1 - f) * concat
    
    # Gradiente do input gate
    dL_di = dL_dC_curr * C_tilde
    dL_dW_i = dL_di * i * (1 - i) * concat
    
    # Gradiente dos valores candidatos
    dL_dC_tilde = dL_dC_curr * i
    dL_dW_C = dL_dC_tilde * (1 - C_tilde**2) * concat
    
    # Gradiente para timestep anterior
    dL_dh_prev = dL_dh @ W_o[:, :hidden_size] + dL_dC_curr @ W_f[:, :hidden_size] + ...
    dL_dC_prev = dL_dC_curr * f
    
    return dL_dh_prev, dL_dC_prev, dL_dW_f, dL_dW_i, dL_dW_C, dL_dW_o
```

---

# 18. OTIMIZA√á√ÉO (ADAM)

## 18.1 Adam Optimizer

**Atualiza√ß√£o:**

$$m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$$
$$v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2$$
$$\hat{m}_t = \frac{m_t}{1-\beta_1^t}$$
$$\hat{v}_t = \frac{v_t}{1-\beta_2^t}$$
$$\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t$$

**Hiperpar√¢metros:**
- $\alpha = 0.001$ (learning rate)
- $\beta_1 = 0.9$ (momentum decay)
- $\beta_2 = 0.999$ (squared gradient decay)
- $\epsilon = 10^{-8}$ (num√©rico)

---

# 19. REGULARIZA√á√ÉO

## 19.1 Dropout

**Aplica dropout no hidden state:**
$$h_t = \text{dropout}(h_t, p)$$

**Dropout rate:** $p = 0.2-0.5$ comum.

## 19.2 L2 Regularization

**Adiciona ao loss:**
$$L_{total} = L + \lambda \sum_{W} \|W\|_2^2$$

**$\lambda$:** weight decay (ex: 0.0001).

---

# 20. APLICA√á√ïES NOVA CORRENTE

## 20.1 Previs√£o de Demanda com LSTM

**Arquitetura:**
- Input: 30 dias hist√≥ricos
- LSTM: 2 camadas, 50 unidades cada
- Output: 30 dias de previs√£o

**Features:**
- Consumo hist√≥rico
- Temperatura
- Feriados
- Sazonalidade

### Implementa√ß√£o

```python
import torch
import torch.nn as nn

class DemandLSTM(nn.Module):
    def __init__(self, input_size=10, hidden_size=50, num_layers=2, output_size=30):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        # x: (batch, seq_len, features)
        lstm_out, (h_n, c_n) = self.lstm(x)
        # Usar √∫ltima sa√≠da
        last_hidden = lstm_out[:, -1, :]
        forecast = self.fc(last_hidden)
        return forecast
```

## 20.2 Treinamento

```python
model = DemandLSTM(input_size=10, hidden_size=50, num_layers=2)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.MSELoss()

for epoch in range(100):
    for batch_x, batch_y in dataloader:
        # Forward
        forecast = model(batch_x)
        loss = criterion(forecast, batch_y)
        
        # Backward
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
```

---

# RESUMO FINAL

## Equa√ß√µes Principais LSTM

| Gate/Estado | F√≥rmula |
|-------------|---------|
| **Forget Gate** | $f_t = \sigma(W_f [h_{t-1}, x_t] + b_f)$ |
| **Input Gate** | $i_t = \sigma(W_i [h_{t-1}, x_t] + b_i)$ |
| **Candidate** | $\tilde{C}_t = \tanh(W_C [h_{t-1}, x_t] + b_C)$ |
| **Cell State** | $C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t$ |
| **Output Gate** | $o_t = \sigma(W_o [h_{t-1}, x_t] + b_o)$ |
| **Hidden State** | $h_t = o_t \odot \tanh(C_t)$ |

---

**Nova Corrente Grand Prix SENAI**

**LSTM COMPLETE BREAKDOWN - Version 1.0**

*Novembro 2025*

