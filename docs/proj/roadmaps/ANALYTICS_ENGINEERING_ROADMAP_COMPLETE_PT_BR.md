# üöÄ ROADMAP ANALYTICS ENGINEERING COMPLETO
## Nova Corrente - Previsibilidade de Demandas com IA

**Vers√£o:** 1.0  
**Data:** Novembro 2025  
**Status:** ‚úÖ Roadmap Completo para Implementa√ß√£o  
**Dura√ß√£o Estimada:** 16 semanas (4 meses)

---

## üìã √çNDICE

1. [Vis√£o Geral de Analytics Engineering](#visao-geral)
2. [Arquitetura de Dados Moderna](#arquitetura-dados)
3. [Modelagem de Dados](#modelagem-dados)
4. [Pipeline de Engenharia de Dados](#pipeline-engenharia)
5. [ML Ops Pipeline](#ml-ops)
6. [Qualidade e Observabilidade](#qualidade-observabilidade)
7. [Infraestrutura como C√≥digo](#infraestrutura-codigo)
8. [Analytics e BI](#analytics-bi)
9. [Governan√ßa de Dados](#governanca-dados)
10. [Roadmap de Implementa√ß√£o](#roadmap-implementacao)
11. [Tecnologias e Ferramentas](#tecnologias-ferramentas)
12. [M√©tricas de Sucesso](#metricas-sucesso)

---

<a name="visao-geral"></a>

## 1. üìä VIS√ÉO GERAL DE ANALYTICS ENGINEERING

### 1.1 O que √© Analytics Engineering?

**Defini√ß√£o:**
Analytics Engineering √© a disciplina que transforma dados brutos em insights acion√°veis atrav√©s de:
- Transforma√ß√£o de dados com c√≥digo (dbt, SQL)
- Versionamento e testes de dados
- Documenta√ß√£o automatizada
- CI/CD para analytics
- Self-service para usu√°rios de neg√≥cio

**Princ√≠pios Fundamentais:**

1. **C√≥digo-First:** Transforma√ß√µes em c√≥digo (n√£o GUI)
2. **Versionamento:** Git para todas transforma√ß√µes
3. **Testes:** Valida√ß√£o autom√°tica de qualidade
4. **Documenta√ß√£o:** Gerada automaticamente
5. **Repetibilidade:** Pipelines idempotentes
6. **Colabora√ß√£o:** Equipes trabalham juntas com c√≥digo

### 1.2 Stack Moderno de Analytics Engineering

**Camada de Transforma√ß√£o:**
- **dbt (data build tool):** Framework SQL para transforma√ß√µes
- **SQL:** Linguagem padr√£o de transforma√ß√£o
- **Python:** Para transforma√ß√µes complexas (UDFs)

**Camada de Armazenamento:**
- **Data Lakehouse:** Delta Lake ou Iceberg
- **Medallion Architecture:** Bronze ‚Üí Silver ‚Üí Gold
- **Cloud Data Warehouses:** Snowflake, BigQuery, Redshift

**Camada de Orquestra√ß√£o:**
- **Airflow/Prefect:** Orquestra√ß√£o de pipelines
- **dbt Cloud:** CI/CD para dbt
- **GitHub Actions:** CI/CD para c√≥digo

**Camada de BI:**
- **Metabase/Superset:** Self-service analytics
- **dbt Semantic Layer:** M√©tricas unificadas
- **Embed Analytics:** Integra√ß√£o em apps

### 1.3 Arquitetura Medallion (Bronze/Silver/Gold)

**Camada Bronze (Raw):**
```
- Dados brutos como chegam das fontes
- Sem transforma√ß√µes, apenas ingest√£o
- Schema evolu√ß√£o permitida
- Particionamento por data/hora
- Formato: Parquet/Delta/Iceberg
```

**Camada Silver (Cleaned):**
```
- Dados limpos e validados
- Schema aplicado
- Duplicatas removidas
- Tipos corrigidos
- Ready for analytics
```

**Camada Gold (Curated):**
```
- Modelos de neg√≥cio (star schema)
- M√©tricas pr√©-calculadas
- Agrega√ß√µes por dimens√µes
- Ready for BI tools
- Performance otimizado
```

### 1.4 Data Vault 2.0 para Escalabilidade

**Componentes:**

1. **Hubs (Entidades de Neg√≥cio):**
   - Item Hub, Torre Hub, Fornecedor Hub
   - Chaves de neg√≥cio √∫nicas
   - Timestamp de cria√ß√£o

2. **Links (Rela√ß√µes):**
   - Item-Torre Link
   - Item-Fornecedor Link
   - Torre-Contrato Link

3. **Satellites (Atributos Hist√≥ricos):**
   - Item Satellite (descri√ß√£o, categoria)
   - Torre Satellite (localiza√ß√£o, status)
   - Forecast Satellite (previs√µes hist√≥ricas)

**Vantagens:**
- Escalabilidade horizontal
- Auditoria completa (tudo hist√≥rico)
- Flexibilidade (novas fontes f√°ceis)
- Performance (particionamento)

---

<a name="arquitetura-dados"></a>

## 2. üèóÔ∏è ARQUITETURA DE DADOS MODERNA

### 2.1 Data Lakehouse

**Defini√ß√£o:**
Arquitetura que combina:
- **Data Lake:** Armazenamento barato e escal√°vel
- **Data Warehouse:** Performance e ACID transactions
- **Benef√≠cios:** Melhor dos dois mundos

**Implementa√ß√£o:**
- **Delta Lake:** ACID transactions sobre Parquet
- **Apache Iceberg:** Tabelas versionadas
- **Apache Hudi:** Upserts e deletes incrementais

**Para Nova Corrente:**
```
Data Lakehouse Stack:
‚îú‚îÄ‚îÄ Storage: S3/MinIO (objeto)
‚îú‚îÄ‚îÄ Format: Delta Lake (transa√ß√µes ACID)
‚îú‚îÄ‚îÄ Compute: Spark (processamento)
‚îú‚îÄ‚îÄ Catalog: Unity Catalog / Hive Metastore
‚îî‚îÄ‚îÄ Access: SQL (Spark SQL, Delta SQL)
```

### 2.2 Streaming e Batch

**Arquitetura Lambda:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Batch    ‚îÇ      ‚îÇ   Streaming  ‚îÇ
‚îÇ   (ETL)    ‚îÇ      ‚îÇ   (Real-time)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ                     ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ   Serving    ‚îÇ
         ‚îÇ    Layer     ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Batch Pipeline:**
- Execu√ß√£o di√°ria/semanal
- Transforma√ß√µes complexas
- Dados hist√≥ricos
- Ferramentas: Airflow, dbt

**Streaming Pipeline:**
- Tempo real (< 1 minuto lat√™ncia)
- Eventos cr√≠ticos (alertas)
- Kafka ‚Üí Flink ‚Üí Delta
- Ferramentas: Kafka, Flink, Kafka Connect

**Para Nova Corrente:**
- **Batch:** Previs√µes di√°rias, relat√≥rios semanais
- **Streaming:** Alertas cr√≠ticos, mudan√ßas de estoque em tempo real

### 2.3 Data Mesh (Microservi√ßos de Dados)

**Conceito:**
Arquitetura distribu√≠da onde:
- Cada dom√≠nio √© dono de seus dados
- Produtos de dados (n√£o apenas datasets)
- Infraestrutura compartilhada self-serve
- Governan√ßa federada

**Para Nova Corrente:**

**Dom√≠nios:**
```
‚îú‚îÄ‚îÄ Forecasting Domain
‚îÇ   ‚îú‚îÄ‚îÄ Forecast Data Products
‚îÇ   ‚îî‚îÄ‚îÄ Model Metrics Data Products
‚îÇ
‚îú‚îÄ‚îÄ Inventory Domain
‚îÇ   ‚îú‚îÄ‚îÄ Stock Levels Data Products
‚îÇ   ‚îî‚îÄ‚îÄ Reorder Points Data Products
‚îÇ
‚îú‚îÄ‚îÄ Procurement Domain
‚îÇ   ‚îú‚îÄ‚îÄ Purchase Orders Data Products
‚îÇ   ‚îî‚îÄ‚îÄ Supplier Performance Data Products
‚îÇ
‚îî‚îÄ‚îÄ Operations Domain
    ‚îú‚îÄ‚îÄ Tower Status Data Products
    ‚îî‚îÄ‚îÄ SLA Metrics Data Products
```

**Benef√≠cios:**
- Escalabilidade horizontal
- Autonomia das equipes
- Reutiliza√ß√£o de produtos
- Governan√ßa descentralizada

### 2.4 Diagrama Arquitetural Completo

```mermaid
graph TB
    subgraph "Data Sources"
        ERP[ERP System]
        WEATHER[Weather APIs]
        ANATEL[Anatel APIs]
        SUPPLIER[Supplier APIs]
    end
    
    subgraph "Ingestion Layer"
        BRONZE[Bronze Layer<br/>Raw Data]
        KAFKA[Kafka Streams<br/>Real-time Events]
    end
    
    subgraph "Processing Layer"
        SILVER[Silver Layer<br/>Cleaned Data]
        DBT[dbt Transformations]
        SPARK[Spark Processing]
    end
    
    subgraph "Storage Layer"
        DELTA[Delta Lake<br/>ACID Transactions]
        GOLD[Gold Layer<br/>Business Models]
    end
    
    subgraph "ML Layer"
        MLFLOW[MLflow<br/>Model Registry]
        FEATURES[Feature Store]
        INFERENCE[Model Serving]
    end
    
    subgraph "Analytics Layer"
        METABASE[Metabase<br/>Self-Service BI]
        SUPERSET[Superset<br/>Dashboards]
        API[REST API<br/>dbt Semantic Layer]
    end
    
    subgraph "Governance"
        DATAHUB[DataHub<br/>Catalog & Lineage]
        GREATEXP[Great Expectations<br/>Data Quality]
    end
    
    ERP & WEATHER & ANATEL & SUPPLIER --> BRONZE
    BRONZE --> KAFKA
    KAFKA --> SILVER
    BRONZE --> SILVER
    SILVER --> DBT
    DBT --> SPARK
    SPARK --> DELTA
    DELTA --> GOLD
    GOLD --> MLFLOW
    MLFLOW --> FEATURES
    FEATURES --> INFERENCE
    GOLD --> METABASE
    GOLD --> SUPERSET
    GOLD --> API
    
    SILVER --> GREATEXP
    GOLD --> DATAHUB
    INFERENCE --> DATAHUB
```

---

<a name="modelagem-dados"></a>

## 3. üìê MODELAGEM DE DADOS

### 3.1 Dimensional (Star Schema)

**Para Analytics de Neg√≥cio:**

```sql
-- Fact Table: Forecast Metrics
CREATE TABLE fact_forecast_metrics (
    forecast_id BIGINT,
    item_id STRING,
    date DATE,
    forecasted_demand DECIMAL(10,2),
    actual_demand DECIMAL(10,2),
    mape DECIMAL(5,2),
    created_at TIMESTAMP
);

-- Dimension: Items
CREATE TABLE dim_items (
    item_id STRING,
    item_name STRING,
    category STRING,
    supplier_id STRING,
    cost DECIMAL(10,2),
    valid_from DATE,
    valid_to DATE
);

-- Dimension: Towers
CREATE TABLE dim_towers (
    tower_id STRING,
    tower_name STRING,
    city STRING,
    state STRING,
    region STRING,
    sla_tier STRING,
    valid_from DATE,
    valid_to DATE
);

-- Dimension: Time
CREATE TABLE dim_time (
    date DATE,
    year INT,
    quarter INT,
    month INT,
    week INT,
    day_of_week INT,
    is_weekend BOOLEAN,
    is_holiday BOOLEAN,
    holiday_name STRING
);
```

**Vantagens:**
- Simples para BI tools
- Performance otimizado
- F√°cil de entender

### 3.2 Data Vault 2.0 (Hubs, Links, Satellites)

**Estrutura para Escalabilidade:**

```sql
-- Hub: Item (Business Key)
CREATE TABLE hub_item (
    item_hk STRING,  -- Hash of item_id
    item_id STRING,  -- Business Key
    load_dt TIMESTAMP
);

-- Link: Item-Tower Relationship
CREATE TABLE link_item_tower (
    item_tower_hk STRING,  -- Hash of (item_id, tower_id)
    item_hk STRING,
    tower_hk STRING,
    load_dt TIMESTAMP
);

-- Satellite: Item Attributes
CREATE TABLE sat_item (
    item_hk STRING,
    item_name STRING,
    category STRING,
    supplier_id STRING,
    cost DECIMAL(10,2),
    load_dt TIMESTAMP,
    load_end_dt TIMESTAMP,
    hash_diff STRING
);

-- Satellite: Forecast History
CREATE TABLE sat_forecast (
    item_hk STRING,
    forecast_date DATE,
    forecasted_demand DECIMAL(10,2),
    model_type STRING,
    confidence_interval_lower DECIMAL(10,2),
    confidence_interval_upper DECIMAL(10,2),
    load_dt TIMESTAMP,
    hash_diff STRING
);
```

**Views de Neg√≥cio (Presentation Layer):**
```sql
-- View: Current Item Details
CREATE VIEW vw_items_current AS
SELECT 
    h.item_id,
    s.item_name,
    s.category,
    s.supplier_id,
    s.cost
FROM hub_item h
INNER JOIN sat_item s ON h.item_hk = s.item_hk
WHERE s.load_end_dt IS NULL;  -- Current version
```

### 3.3 dbt Models e Testes

**Model: stg_items (Staging):**

```sql
-- models/staging/stg_items.sql
{{ config(materialized='view') }}

WITH source AS (
    SELECT * FROM {{ source('bronze', 'raw_items') }}
),

cleaned AS (
    SELECT
        item_id,
        TRIM(item_name) AS item_name,
        LOWER(category) AS category,
        supplier_id,
        CAST(cost AS DECIMAL(10,2)) AS cost,
        created_at
    FROM source
    WHERE item_id IS NOT NULL
)

SELECT * FROM cleaned
```

**Model: dim_items (Dimension):**

```sql
-- models/marts/dim_items.sql
{{ config(materialized='table') }}

WITH staging AS (
    SELECT * FROM {{ ref('stg_items') }}
),

final AS (
    SELECT
        item_id,
        item_name,
        category,
        supplier_id,
        cost,
        CURRENT_TIMESTAMP AS valid_from,
        NULL AS valid_to
    FROM staging
)

SELECT * FROM final
```

**Testes:**

```yaml
# models/schema.yml
version: 2

models:
  - name: dim_items
    columns:
      - name: item_id
        tests:
          - unique
          - not_null
      - name: item_name
        tests:
          - not_null
      - name: cost
        tests:
          - not_null
          - dbt_utils.accepted_range:
              min_value: 0
              max_value: 1000000
```

### 3.4 Schema Evolution

**Estrat√©gias:**

1. **Backward Compatible:**
   - Adicionar colunas opcionais
   - N√£o remover colunas (deprecated)
   - Sem mudan√ßas de tipo

2. **Schema Registry:**
   - Kafka Schema Registry
   - Delta Lake schema evolution autom√°tico
   - Valida√ß√£o autom√°tica

3. **Migration Scripts:**
   - Alembic (SQLAlchemy)
   - dbt migrations
   - Versionamento de schemas

---

<a name="pipeline-engenharia"></a>

## 4. üîÑ PIPELINE DE ENGENHARIA DE DADOS

### 4.1 Extra√ß√£o (Fivetran, Airbyte)

**Fivetran:**
- Conectores prontos (ERP, APIs)
- Schema evolution autom√°tico
- Transforma√ß√µes b√°sicas
- Custo: ~$500/m√™s por connector

**Airbyte (Open Source):**
- Self-hosted ou Cloud
- 200+ conectores
- Custom connectors (Python)
- Custo: Infraestrutura pr√≥pria

**Para Nova Corrente:**
```
Extraction Sources:
‚îú‚îÄ‚îÄ ERP System ‚Üí Airbyte (PostgreSQL)
‚îú‚îÄ‚îÄ Weather API ‚Üí Custom Python (cron)
‚îú‚îÄ‚îÄ Anatel API ‚Üí Custom Python (daily)
‚îú‚îÄ‚îÄ Supplier APIs ‚Üí Airbyte (REST)
‚îî‚îÄ‚îÄ Kafka ‚Üí Real-time ingestion
```

### 4.2 Transforma√ß√£o (dbt)

**dbt Project Structure:**
```
nova_corrente_dbt/
‚îú‚îÄ‚îÄ dbt_project.yml
‚îú‚îÄ‚îÄ profiles.yml
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ staging/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stg_items.sql
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stg_towers.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stg_forecasts.sql
‚îÇ   ‚îú‚îÄ‚îÄ intermediate/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ int_item_metrics.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ int_forecast_accuracy.sql
‚îÇ   ‚îî‚îÄ‚îÄ marts/
‚îÇ       ‚îú‚îÄ‚îÄ dim_items.sql
‚îÇ       ‚îú‚îÄ‚îÄ dim_towers.sql
‚îÇ       ‚îú‚îÄ‚îÄ fact_forecasts.sql
‚îÇ       ‚îî‚îÄ‚îÄ metrics/
‚îÇ           ‚îî‚îÄ‚îÄ forecast_accuracy.sql
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ assertions/
‚îÇ   ‚îî‚îÄ‚îÄ custom_tests/
‚îú‚îÄ‚îÄ macros/
‚îÇ   ‚îú‚îÄ‚îÄ reorder_point.sql
‚îÇ   ‚îî‚îÄ‚îÄ safety_stock.sql
‚îî‚îÄ‚îÄ snapshots/
    ‚îî‚îÄ‚îÄ snp_items.sql
```

**Exemplo: Macro Reorder Point**

```sql
-- macros/reorder_point.sql
{% macro calculate_reorder_point(
    avg_demand,
    lead_time,
    std_demand,
    service_level=0.95
) %}
    {% set z_score = 1.96 %}  -- 95% service level
    {% set safety_stock = z_score * std_demand * sqrt(lead_time) %}
    {{ avg_demand * lead_time + safety_stock }}
{% endmacro %}
```

**Uso:**
```sql
SELECT
    item_id,
    {{ calculate_reorder_point(
        avg_daily_demand,
        lead_time,
        std_daily_demand,
        0.95
    ) }} AS reorder_point
FROM {{ ref('int_item_metrics') }}
```

### 4.3 Carregamento (ELT)

**Padr√£o ELT:**
1. **Extract:** Dados brutos ‚Üí Bronze
2. **Load:** Bronze ‚Üí Silver (apenas schema)
3. **Transform:** Silver ‚Üí Gold (dbt transformations)

**Vantagens ELT vs ETL:**
- Flexibilidade (transforma√ß√µes podem mudar)
- Performance (processamento no warehouse)
- Escalabilidade (compute separado de storage)

**Pipeline Airflow:**

```python
# dags/nova_corrente_pipeline.py
from airflow import DAG
from airflow.providers.databricks.operators.databricks import DatabricksRunNowOperator
from airflow.operators.bash import BashOperator

dag = DAG(
    'nova_corrente_daily',
    schedule_interval='@daily',
    catchup=False
)

# Extract
extract_task = BashOperator(
    task_id='extract_data',
    bash_command='python scripts/extract_all_sources.py',
    dag=dag
)

# Load to Bronze
load_bronze = DatabricksRunNowOperator(
    task_id='load_bronze',
    job_id=12345,
    dag=dag
)

# Transform with dbt
run_dbt = BashOperator(
    task_id='run_dbt',
    bash_command='cd dbt && dbt run --profiles-dir .',
    dag=dag
)

# Run tests
test_dbt = BashOperator(
    task_id='test_dbt',
    bash_command='cd dbt && dbt test --profiles-dir .',
    dag=dag
)

extract_task >> load_bronze >> run_dbt >> test_dbt
```

### 4.4 Orquestra√ß√£o (Airflow, Prefect)

**Airflow:**
- Padr√£o da ind√∫stria
- UI robusta
- Muitos operators
- Escal√°vel

**Prefect:**
- Moderno (Python-first)
- UI elegante
- F√°cil de usar
- Boa documenta√ß√£o

**Para Nova Corrente:**
- **Recomenda√ß√£o:** Airflow (mais maduro, mais comunidade)
- **Alternativa:** Prefect (se time prefere Python)

---

<a name="ml-ops"></a>

## 5. ü§ñ ML OPS PIPELINE

### 5.1 MLflow

**Componentes:**

1. **Tracking:**
   - Experimentos, m√©tricas, par√¢metros
   - Artifacts (modelos, plots)
   - UI web

2. **Projects:**
   - C√≥digo versionado
   - Depend√™ncias
   - Execu√ß√£o reproduz√≠vel

3. **Models:**
   - Registry de modelos
   - Versionamento
   - Staging (dev/staging/prod)

4. **Model Serving:**
   - REST API
   - Batch inference
   - Real-time serving

**Exemplo:**
```python
import mlflow
import mlflow.prophet

with mlflow.start_run():
    # Log parameters
    mlflow.log_param("model_type", "prophet")
    mlflow.log_param("forecast_steps", 30)
    
    # Train model
    model = train_prophet_model(data)
    
    # Log metrics
    mape = evaluate_model(model, test_data)
    mlflow.log_metric("MAPE", mape)
    
    # Log model
    mlflow.prophet.log_model(model, "model")
    
    # Register to registry
    mlflow.register_model(
        "runs:/{run_id}/model",
        "NovaCorrenteForecast"
    )
```

### 5.2 Kubeflow

**Componentes:**
- Pipelines (orquestra√ß√£o ML)
- Katib (hyperparameter tuning)
- KFServing (model serving)
- Notebooks (Jupyter no K8s)

**Para Produ√ß√£o:**
- Kubernetes-native
- Escal√°vel
- Enterprise-grade

### 5.3 Versionamento de Modelos

**Estrat√©gia:**
```
Model Registry:
‚îú‚îÄ‚îÄ nova_corrente_forecast
‚îÇ   ‚îú‚îÄ‚îÄ v1 (Production)
‚îÇ   ‚îú‚îÄ‚îÄ v2 (Staging)
‚îÇ   ‚îî‚îÄ‚îÄ v3 (Development)
‚îÇ
‚îú‚îÄ‚îÄ nova_corrente_arima
‚îÇ   ‚îî‚îÄ‚îÄ v1 (Production)
‚îÇ
‚îî‚îÄ‚îÄ nova_corrente_ensemble
    ‚îî‚îÄ‚îÄ v1 (Production)
```

**Promo√ß√£o:**
- Dev ‚Üí Staging: CI/CD autom√°tico
- Staging ‚Üí Prod: Aprova√ß√£o manual
- Rollback: Um comando

### 5.4 Feature Stores

**Feast (Open Source):**
```python
# Define features
@feature_view(
    name="item_demand_features",
    entities=["item"],
    ttl=timedelta(days=90)
)
class ItemDemandFeatures:
    avg_daily_demand = Value(dtype=Float32)
    std_daily_demand = Value(dtype=Float32)
    lead_time = Value(dtype=Int32)
    created_at = Value(dtype=UnixTimestamp)
```

**Tecton (Managed):**
- Feature store gerenciado
- Transforma√ß√µes em tempo real
- Integra√ß√£o com MLflow

### 5.5 Model Serving

**Op√ß√µes:**

1. **MLflow Serving:**
   - REST API simples
   - Bom para batch
   - F√°cil deploy

2. **Seldon Core:**
   - Kubernetes-native
   - A/B testing
   - Canary deployments

3. **TorchServe:**
   - PyTorch otimizado
   - Batching autom√°tico
   - Performance

**Para Nova Corrente:**
- **Inicial:** MLflow Serving (simples)
- **Produ√ß√£o:** Seldon Core (K8s, A/B testing)

---

<a name="qualidade-observabilidade"></a>

## 6. ‚úÖ QUALIDADE E OBSERVABILIDADE

### 6.1 Great Expectations

**Expectations (Valida√ß√µes):**

```python
import great_expectations as ge

# Create expectation suite
suite = ge.dataset.SparkDFDataset(df)

# Expect columns to exist
suite.expect_column_to_exist("item_id")
suite.expect_column_to_exist("forecasted_demand")

# Expect values
suite.expect_column_values_to_be_between(
    "forecasted_demand",
    min_value=0,
    max_value=10000
)

# Expect uniqueness
suite.expect_column_values_to_be_unique("item_id")

# Save suite
suite.save_expectation_suite("forecasts_expectations.json")
```

**Data Docs (HTML):**
- Relat√≥rios autom√°ticos
- Valida√ß√£o visual
- Hist√≥rico de resultados

### 6.2 dbt Tests

**Built-in Tests:**
```yaml
models:
  - name: fact_forecasts
    columns:
      - name: item_id
        tests:
          - unique
          - not_null
          - relationships:
              to: ref('dim_items')
              field: item_id
```

**Custom Tests:**
```sql
-- tests/custom/assert_mape_threshold.sql
SELECT item_id, mape
FROM {{ ref('fact_forecasts') }}
WHERE mape > 20  -- Threshold: 20%
```

### 6.3 Data Profiling

**dbt Profiling:**
```bash
dbt run-operation generate_model_profiles
```

**Pandas Profiling:**
```python
import pandas_profiling

profile = df.profile_report()
profile.to_file("forecasts_profile.html")
```

### 6.4 Data Lineage

**dbt Lineage:**
```bash
dbt docs generate
dbt docs serve
```

**DataHub:**
- Lineage autom√°tico
- Catalog de dados
- Metadata management

### 6.5 Alerts

**Integra√ß√£o:**
- Slack notifications
- Email alerts
- PagerDuty (critical)
- Custom webhooks

**Exemplo:**
```python
# Alert on data quality failure
if not validation_result.success:
    send_slack_alert(
        channel="#data-alerts",
        message=f"Data quality failed: {validation_result.summary}"
    )
```

---

<a name="infraestrutura-codigo"></a>

## 7. üèóÔ∏è INFRAESTRUTURA COMO C√ìDIGO

### 7.1 Terraform

**Infraestrutura Cloud:**
```hcl
# terraform/aws/data_warehouse.tf
resource "aws_s3_bucket" "data_lake" {
  bucket = "nova-corrente-data-lake"
  
  versioning {
    enabled = true
  }
  
  lifecycle_rule {
    enabled = true
    expiration {
      days = 90
    }
  }
}

resource "aws_databricks_workspace" "main" {
  account_id     = var.databricks_account_id
  workspace_name = "nova-corrente-workspace"
  deployment_name = "nova-corrente"
  
  tags = {
    Environment = "production"
    Project     = "forecasting"
  }
}
```

### 7.2 Docker e Kubernetes

**Docker Compose (Desenvolvimento):**
```yaml
version: '3.8'
services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: nova_corrente
  
  airflow:
    image: apache/airflow:2.7.0
    depends_on:
      - postgres
  
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.7.0
    ports:
      - "5000:5000"
```

**Kubernetes (Produ√ß√£o):**
```yaml
# k8s/airflow-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-webserver
spec:
  replicas: 3
  selector:
    matchLabels:
      app: airflow-webserver
  template:
    metadata:
      labels:
        app: airflow-webserver
    spec:
      containers:
      - name: airflow
        image: apache/airflow:2.7.0
        resources:
          requests:
            memory: "2Gi"
            cpu: "1"
          limits:
            memory: "4Gi"
            cpu: "2"
```

### 7.3 CI/CD

**GitHub Actions:**
```yaml
# .github/workflows/dbt.yml
name: dbt CI
on: [push]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run dbt tests
        run: |
          cd dbt
          dbt deps
          dbt run --models staging
          dbt test
```

**dbt Cloud:**
- CI/CD nativo
- Jobs agendados
- Slack/Email notifications

### 7.4 Environments

**Estrutura:**
```
environments/
‚îú‚îÄ‚îÄ dev/
‚îÇ   ‚îú‚îÄ‚îÄ terraform/
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml
‚îÇ   ‚îî‚îÄ‚îÄ config/
‚îú‚îÄ‚îÄ staging/
‚îÇ   ‚îú‚îÄ‚îÄ terraform/
‚îÇ   ‚îú‚îÄ‚îÄ k8s/
‚îÇ   ‚îî‚îÄ‚îÄ config/
‚îî‚îÄ‚îÄ prod/
    ‚îú‚îÄ‚îÄ terraform/
    ‚îú‚îÄ‚îÄ k8s/
    ‚îî‚îÄ‚îÄ config/
```

---

<a name="analytics-bi"></a>

## 8. üìä ANALYTICS E BI

### 8.1 dbt + BI Tools

**dbt Semantic Layer:**
```yaml
# models/metrics.yml
version: 2

metrics:
  - name: forecast_accuracy
    label: "Forecast Accuracy"
    model: ref('fact_forecasts')
    calculation_method: average
    expression: |
      CASE 
        WHEN actual_demand > 0 
        THEN ABS(forecasted_demand - actual_demand) / actual_demand
        ELSE NULL
      END
    
  - name: total_forecasted_demand
    label: "Total Forecasted Demand"
    model: ref('fact_forecasts')
    calculation_method: sum
    expression: forecasted_demand
```

**Access via API:**
```python
from dbt_semantic_interfaces import SemanticManifest

manifest = SemanticManifest.load("metrics.yml")
metrics = manifest.list_metrics()
```

### 8.2 Metabase/Superset

**Metabase:**
- Self-service BI
- F√°cil de usar
- Query builder visual
- Embed analytics

**Superset:**
- Apache Superset (open source)
- Dashboard avan√ßados
- SQL Lab
- Alerts

**Para Nova Corrente:**
- **Metabase:** Usu√°rios de neg√≥cio (n√£o t√©cnicos)
- **Superset:** Analistas de dados (SQL)

### 8.3 Embed Analytics

**Metabase Embed:**
```javascript
<iframe
  src="https://metabase.novacorrente.com/embed/dashboard/123"
  frameBorder="0"
  width="800"
  height="600"
/>
```

### 8.4 Self-Service

**Capabilities:**
- Query builder (sem SQL)
- Saved queries
- Dashboards customizados
- Export Excel/PDF
- Scheduled reports

---

<a name="governanca-dados"></a>

## 9. üõ°Ô∏è GOVERNAN√áA DE DADOS

### 9.1 Cataloga√ß√£o (DataHub)

**Metadata:**
- Schemas, tables, columns
- Lineage (upstream/downstream)
- Ownership (data stewards)
- Tags (PII, sensitive)

**Integra√ß√£o:**
```python
from datahub.ingestion.api.common import RecordEnvelope

# Ingest metadata
from datahub.ingestion.graph.client import DatahubGraph

graph = DatahubGraph(ConfigDatahub())
graph.emit(metadata)
```

### 9.2 Lineage

**Automatic:**
- dbt lineage (SQL parsing)
- Airflow lineage (task dependencies)
- Spark lineage (Spark UI)

**Manual:**
- DataHub annotations
- Documentation links

### 9.3 Security e Compliance

**Row-Level Security:**
```sql
-- dbt macro
{% macro apply_rls(model_name) %}
  {% set user_email = var('user_email') %}
  
  {% if user_email %}
    ALTER TABLE {{ model_name }} 
    ENABLE ROW LEVEL SECURITY;
    
    CREATE POLICY user_policy ON {{ model_name }}
      FOR SELECT
      USING (owner_email = '{{ user_email }}');
  {% endif %}
{% endmacro %}
```

**Data Masking:**
```sql
-- PII columns masked
SELECT
    item_id,
    mask_pii(item_name) AS item_name,  -- Custom function
    cost
FROM dim_items
```

### 9.4 Documenta√ß√£o

**dbt Docs:**
- Auto-gerada de YAML
- Descriptions, tests, examples
- Acess√≠vel via web

**DataHub:**
- Rich text documentation
- Links, images
- Ownership, tags

---

<a name="roadmap-implementacao"></a>

## 10. üóìÔ∏è ROADMAP DE IMPLEMENTA√á√ÉO

### Fase 0: Foundation (Semanas 1-2)

**Objetivos:**
- Setup infraestrutura b√°sica
- Configurar dbt project
- Criar pipelines Bronze/Silver

**Entregas:**
- [ ] Terraform para AWS/GCP
- [ ] S3/Cloud Storage (Bronze layer)
- [ ] dbt project structure
- [ ] Airflow DAG b√°sico
- [ ] Documenta√ß√£o inicial

**Checkpoint:**
- Dados raw sendo ingeridos diariamente
- dbt models b√°sicos funcionando

### Fase 1: Data Foundation (Semanas 3-4)

**Objetivos:**
- Implementar Silver layer
- Criar modelos dbt staging
- Setup Data Quality (Great Expectations)

**Entregas:**
- [ ] Staging models (stg_items, stg_towers, stg_forecasts)
- [ ] Data quality tests
- [ ] Great Expectations suite
- [ ] Data profiling reports

**Checkpoint:**
- Dados limpos na Silver layer
- Data quality passando (95%+)

### Fase 2: Analytics Layer (Semanas 5-8)

**Objetivos:**
- Implementar Gold layer (star schema)
- Criar m√©tricas de neg√≥cio
- Setup BI tools

**Entregas:**
- [ ] Dimension models (dim_items, dim_towers, dim_time)
- [ ] Fact models (fact_forecasts, fact_inventory)
- [ ] dbt metrics (MAPE, forecast accuracy)
- [ ] Metabase/Superset configurado
- [ ] Dashboards b√°sicos

**Checkpoint:**
- Dashboards funcionando
- Usu√°rios de neg√≥cio acessando dados

### Fase 3: ML Ops (Semanas 9-12)

**Objetivos:**
- Setup MLflow
- Feature store
- Model serving

**Entregas:**
- [ ] MLflow tracking
- [ ] Model registry
- [ ] Feature store (Feast ou Tecton)
- [ ] Model serving (MLflow ou Seldon)
- [ ] A/B testing setup

**Checkpoint:**
- Modelos versionados e servidos
- Predictions em produ√ß√£o

### Fase 4: Advanced Features (Semanas 13-16)

**Objetivos:**
- Governan√ßa completa
- Streaming pipeline
- Otimiza√ß√µes

**Entregas:**
- [ ] DataHub catalog
- [ ] Streaming pipeline (Kafka + Flink)
- [ ] Performance optimization
- [ ] Advanced dashboards
- [ ] Self-service analytics

**Checkpoint:**
- Sistema completo em produ√ß√£o
- Todas m√©tricas sendo coletadas

---

<a name="tecnologias-ferramentas"></a>

## 11. üõ†Ô∏è TECNOLOGIAS E FERRAMENTAS

### 11.1 Stack Recomendado

**Camada de Armazenamento:**
- **Cloud:** AWS S3 / GCP Cloud Storage
- **Format:** Delta Lake
- **Compute:** Databricks / Spark on K8s

**Camada de Transforma√ß√£o:**
- **dbt:** Transforma√ß√µes SQL
- **Python:** UDFs complexas
- **Airflow:** Orquestra√ß√£o

**Camada de ML:**
- **MLflow:** Experiment tracking + Registry
- **Feast:** Feature store
- **Seldon Core:** Model serving

**Camada de BI:**
- **Metabase:** Self-service BI
- **Superset:** Advanced dashboards
- **dbt Semantic Layer:** Metrics API

**Camada de Governan√ßa:**
- **DataHub:** Catalog
- **Great Expectations:** Data quality
- **dbt Tests:** Validation

### 11.2 Alternativas

**Cloud Providers:**
- **AWS:** S3, Glue, Redshift, SageMaker
- **GCP:** Cloud Storage, BigQuery, Vertex AI
- **Azure:** ADLS, Synapse, Azure ML

**Compute:**
- **Databricks:** Managed Spark
- **Snowflake:** Data warehouse
- **BigQuery:** Serverless warehouse

**Orquestra√ß√£o:**
- **Airflow:** Padr√£o da ind√∫stria
- **Prefect:** Modern Python-first
- **Dagster:** Data-aware orchestration

### 11.3 Custos Estimados (Mensal)

**AWS Stack:**
```
S3 Storage (10TB):        $230
Databricks (Medium):      $1,500
Airflow (Managed):        $200
MLflow (Self-hosted):    $100
Metabase Cloud:           $85
DataHub (Self-hosted):   $150
---------------------------------
Total: ~$2,265/m√™s
```

**Open Source Stack (Self-hosted):**
```
Compute (K8s):           $500
Storage (S3):            $230
Monitoring:              $50
---------------------------------
Total: ~$780/m√™s
```

### 11.4 TCO (Total Cost of Ownership)

**Ano 1:**
- Infraestrutura: $27,000
- Ferramentas: $14,400
- Desenvolvimento: $120,000 (1 FTE)
- **Total: $161,400**

**Ano 2+ (Otimizado):**
- Infraestrutura: $15,000
- Ferramentas: $10,000
- Manuten√ß√£o: $60,000 (0.5 FTE)
- **Total: $85,000/ano**

---

<a name="metricas-sucesso"></a>

## 12. üìà M√âTRICAS DE SUCESSO

### 12.1 KPIs T√©cnicos

**Data Quality:**
- Test pass rate: >95%
- Data freshness: <1 hora
- Schema changes: <5%/m√™s

**Pipeline Performance:**
- Bronze ingestion: <30 min/dia
- Silver processing: <1 hora/dia
- Gold transformations: <30 min/dia
- **Total pipeline: <2 horas/dia**

**Model Performance:**
- MAPE: <15% (target)
- Inference latency: <100ms
- Model retraining: Semanal

### 12.2 KPIs de Neg√≥cio

**Previsibilidade:**
- Rupturas reduzidas: -60%
- Estoque otimizado: -20%
- Lead time accuracy: >85%

**Ado√ß√£o:**
- Usu√°rios ativos: 20+ usu√°rios
- Dashboards utilizados: 10+ dashboards
- Queries self-service: 100+/dia

**ROI:**
- Payback: 6-12 meses
- ROI Ano 1: 80-180%
- Economia estoque: R$ 500K+/ano

### 12.3 Dashboards de Monitoramento

**Operational Dashboard:**
- Pipeline status (Airflow)
- Data quality scores (Great Expectations)
- Model performance (MLflow)
- System health (Grafana)

**Business Dashboard:**
- Forecast accuracy (MAPE por item)
- Inventory levels vs Reorder Points
- Alert trends
- ROI metrics

---

## üéØ CONCLUS√ÉO

Este roadmap fornece uma base s√≥lida para implementar Analytics Engineering moderno na Nova Corrente:

‚úÖ **Arquitetura Medallion** (Bronze/Silver/Gold)  
‚úÖ **dbt para transforma√ß√µes** (c√≥digo versionado)  
‚úÖ **ML Ops completo** (MLflow, feature store)  
‚úÖ **Data Quality** (Great Expectations, dbt tests)  
‚úÖ **Self-Service BI** (Metabase/Superset)  
‚úÖ **Governan√ßa** (DataHub, lineage)  
‚úÖ **Roadmap de 16 semanas** (implementa√ß√£o incremental)

**Pr√≥ximos Passos:**
1. Revisar roadmap com stakeholders
2. Priorizar fases (start with Fase 0)
3. Alocar recursos (1-2 engenheiros)
4. Iniciar implementa√ß√£o

---

**Documento criado:** Novembro 2025  
**Vers√£o:** 1.0  
**Autor:** Equipe Grand Prix SENAI  
**Status:** ‚úÖ Roadmap Completo - Pronto para Implementa√ß√£o

**CENTRALIZED REPORTS & CHANGELOG SYSTEM COMPLETE!**

