# üèóÔ∏è ARQUITETURA T√âCNICA - DEEP DIVE
## Nova Corrente - Analytics Engineering

**Vers√£o:** 1.0  
**Data:** Novembro 2025  
**Status:** ‚úÖ Especifica√ß√£o T√©cnica Completa

---

## üìã VIS√ÉO GERAL ARQUITETURAL

### Stack Tecnol√≥gico Completo

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PRESENTATION LAYER                        ‚îÇ
‚îÇ  Metabase | Superset | dbt Semantic Layer API | Custom UI  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     ANALYTICS LAYER                         ‚îÇ
‚îÇ          Gold Layer (Star Schema) | dbt Metrics             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     PROCESSING LAYER                        ‚îÇ
‚îÇ  dbt Transformations | Spark Jobs | Python UDFs             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      STORAGE LAYER                          ‚îÇ
‚îÇ  Delta Lake (S3) | Databricks Tables | Unity Catalog        ‚îÇ
‚îÇ  Bronze (Raw) | Silver (Cleaned) | Gold (Curated)          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    ORCHESTRATION LAYER                      ‚îÇ
‚îÇ              Airflow | Prefect | dbt Cloud                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    INGESTION LAYER                          ‚îÇ
‚îÇ  Airbyte | Fivetran | Custom Python | Kafka Streams        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                       SOURCES                                ‚îÇ
‚îÇ  ERP | Weather APIs | Anatel | BACEN | Supplier APIs        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üóÑÔ∏è CAMADA DE ARMAZENAMENTO

### 1. Medallion Architecture

**Bronze Layer (Raw):**
```
s3://nova-corrente-data-lake-bronze/
‚îú‚îÄ‚îÄ erp/
‚îÇ   ‚îî‚îÄ‚îÄ year=2025/month=11/day=01/
‚îÇ       ‚îî‚îÄ‚îÄ data.parquet
‚îú‚îÄ‚îÄ weather/
‚îÇ   ‚îî‚îÄ‚îÄ year=2025/month=11/day=01/
‚îÇ       ‚îî‚îÄ‚îÄ data.json
‚îú‚îÄ‚îÄ economic/
‚îÇ   ‚îî‚îÄ‚îÄ year=2025/month=11/day=01/
‚îÇ       ‚îî‚îÄ‚îÄ data.json
‚îî‚îÄ‚îÄ anatel/
    ‚îî‚îÄ‚îÄ year=2025/month=11/day=01/
        ‚îî‚îÄ‚îÄ data.json
```

**Caracter√≠sticas:**
- Formato: Parquet (eficiente, comprimido)
- Particionamento: year/month/day
- Schema: Evolutivo (adicionar colunas OK)
- Reten√ß√£o: 90 dias (after ‚Üí cold storage)
- Compress√£o: Snappy

**Silver Layer (Cleaned):**
```
databricks://nova_corrente.silver/
‚îú‚îÄ‚îÄ items (Delta Table)
‚îú‚îÄ‚îÄ towers (Delta Table)
‚îú‚îÄ‚îÄ forecasts (Delta Table)
‚îú‚îÄ‚îÄ inventory (Delta Table)
‚îî‚îÄ‚îÄ weather_enriched (Delta Table)
```

**Caracter√≠sticas:**
- Formato: Delta Lake (ACID transactions)
- Schema: Aplicado e validado
- Qualidade: Great Expectations validado
- Reten√ß√£o: 2 anos
- Vacuum: 7 dias

**Gold Layer (Curated):**
```
databricks://nova_corrente.gold/
‚îú‚îÄ‚îÄ marts/
‚îÇ   ‚îú‚îÄ‚îÄ dim_items (Delta Table, partitioned)
‚îÇ   ‚îú‚îÄ‚îÄ dim_towers (Delta Table)
‚îÇ   ‚îú‚îÄ‚îÄ dim_time (Delta Table)
‚îÇ   ‚îú‚îÄ‚îÄ fact_forecasts (Delta Table, partitioned by date)
‚îÇ   ‚îî‚îÄ‚îÄ fact_inventory (Delta Table, partitioned by date)
‚îî‚îÄ‚îÄ metrics/
    ‚îî‚îÄ‚îÄ forecast_metrics (pre-aggregated)
```

**Caracter√≠sticas:**
- Formato: Delta Lake
- Schema: Star schema otimizado
- Performance: Clustered e particionado
- Reten√ß√£o: 3 anos
- Vacuum: 30 dias

---

### 2. Delta Lake Configuration

**Benef√≠cios:**
- ACID transactions
- Time travel (audit)
- Schema evolution
- Upserts/deletes incrementais
- Performance otimizado

**Configura√ß√£o:**
```python
# Delta Lake settings
spark.conf.set("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
spark.conf.set("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")

# Optimize Delta tables
spark.sql("OPTIMIZE delta.`/path/to/table`")

# Z-order clustering
spark.sql("OPTIMIZE delta.`/path/to/table` ZORDER BY (item_id, date)")

# Vacuum old files
spark.sql("VACUUM delta.`/path/to/table` RETAIN 7 DAYS")
```

---

## üîÑ CAMADA DE PROCESSAMENTO

### 1. dbt Transformations

**Pipeline:**
```mermaid
graph LR
    A[Bronze] --> B[Staging Models]
    B --> C[Intermediate Models]
    C --> D[Mart Models]
    D --> E[Gold Layer]
    E --> F[BI Tools]
```

**Performance:**
- Incremental models: Apenas novos dados
- Materializations: view/table/incremental
- Partitions: Por data automaticamente
- Clustering: Por chaves de neg√≥cio

**Configura√ß√£o:**
```yaml
# dbt_project.yml
models:
  nova_corrente_dbt:
    staging:
      +materialized: view
      +schema: staging
    marts:
      +materialized: incremental
      +unique_key: ['forecast_id']
      +on_schema_change: append_new_columns
      +partition_by: {'field': 'date', 'data_type': 'date'}
      +cluster_by: ['item_id']
```

---

### 2. Spark Processing

**Use Cases:**
- Transforma√ß√µes complexas (UDFs)
- Agrega√ß√µes grandes (billions de rows)
- Joins complexos
- Feature engineering pesado

**Exemplo:**
```python
# Spark job for complex aggregation
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

spark = SparkSession.builder \
    .appName("NovaCorrenteAggregation") \
    .getOrCreate()

df = spark.read.format("delta").load("s3://silver/items")

result = df \
    .groupBy("category", "month") \
    .agg(
        sum("quantity").alias("total_quantity"),
        avg("cost").alias("avg_cost"),
        countDistinct("item_id").alias("unique_items")
    ) \
    .write \
    .format("delta") \
    .mode("overwrite") \
    .save("s3://gold/aggregations")
```

---

## üéõÔ∏è CAMADA DE ORQUESTRA√á√ÉO

### 1. Airflow DAGs

**Estrutura:**
```
dags/
‚îú‚îÄ‚îÄ nova_corrente/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ extract_dag.py
‚îÇ   ‚îú‚îÄ‚îÄ bronze_to_silver_dag.py
‚îÇ   ‚îú‚îÄ‚îÄ dbt_dag.py
‚îÇ   ‚îú‚îÄ‚îÄ ml_training_dag.py
‚îÇ   ‚îî‚îÄ‚îÄ gold_refresh_dag.py
‚îî‚îÄ‚îÄ utils/
    ‚îî‚îÄ‚îÄ operators/
        ‚îú‚îÄ‚îÄ databricks_operator.py
        ‚îî‚îÄ‚îÄ dbt_operator.py
```

**DAG Completo:**
```python
from airflow import DAG
from airflow.providers.databricks.operators.databricks import DatabricksRunNowOperator
from airflow.operators.bash import BashOperator
from airflow.sensors.filesystem import FileSensor
from datetime import datetime, timedelta

default_args = {
    'owner': 'nova-corrente',
    'depends_on_past': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'nova_corrente_daily_pipeline',
    default_args=default_args,
    description='Complete daily pipeline',
    schedule_interval='@daily',
    start_date=datetime(2025, 11, 1),
    catchup=False,
    max_active_runs=1,
)

# Extract from sources
extract_erp = BashOperator(
    task_id='extract_erp',
    bash_command='python scripts/extract_erp.py',
    dag=dag,
)

extract_weather = BashOperator(
    task_id='extract_weather',
    bash_command='python scripts/extract_weather.py',
    dag=dag,
)

# Load to Bronze
load_bronze = DatabricksRunNowOperator(
    task_id='load_to_bronze',
    job_id=12345,
    dag=dag,
)

# Transform: Bronze ‚Üí Silver
silver_transformation = DatabricksRunNowOperator(
    task_id='bronze_to_silver',
    job_id=12346,
    dag=dag,
)

# Data Quality Check
quality_check = BashOperator(
    task_id='data_quality_check',
    bash_command='python scripts/great_expectations_check.py',
    dag=dag,
)

# dbt Transformations: Silver ‚Üí Gold
run_dbt = BashOperator(
    task_id='run_dbt',
    bash_command='cd dbt && dbt run --profiles-dir .',
    dag=dag,
)

test_dbt = BashOperator(
    task_id='test_dbt',
    bash_command='cd dbt && dbt test --profiles-dir .',
    dag=dag,
)

# ML Training (weekly)
ml_training = DatabricksRunNowOperator(
    task_id='ml_training',
    job_id=12347,
    dag=dag,
)

# Dependencies
[extract_erp, extract_weather] >> load_bronze
load_bronze >> silver_transformation
silver_transformation >> quality_check
quality_check >> run_dbt
run_dbt >> test_dbt
test_dbt >> ml_training
```

---

## ü§ñ CAMADA DE ML

### 1. MLflow Integration

**Estrutura:**
```
mlflow/
‚îú‚îÄ‚îÄ experiments/
‚îÇ   ‚îú‚îÄ‚îÄ forecast_prophet/
‚îÇ   ‚îú‚îÄ‚îÄ forecast_arima/
‚îÇ   ‚îî‚îÄ‚îÄ forecast_lstm/
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ nova_corrente_forecast/
‚îÇ       ‚îú‚îÄ‚îÄ v1 (Production)
‚îÇ       ‚îú‚îÄ‚îÄ v2 (Staging)
‚îÇ       ‚îî‚îÄ‚îÄ v3 (Development)
‚îî‚îÄ‚îÄ tracking_uri: databricks
```

**Exemplo:**
```python
import mlflow
import mlflow.prophet

# Set tracking URI
mlflow.set_tracking_uri("databricks")

with mlflow.start_run(experiment_id=123):
    # Log parameters
    mlflow.log_param("model_type", "prophet")
    mlflow.log_param("forecast_steps", 30)
    
    # Train model
    model = train_prophet_model(data)
    
    # Evaluate
    mape = evaluate_model(model, test_data)
    mlflow.log_metric("MAPE", mape)
    
    # Log model
    mlflow.prophet.log_model(model, "model")
    
    # Register
    mlflow.register_model(
        "runs:/{run_id}/model",
        "NovaCorrenteForecast"
    )
```

---

## üìä CAMADA DE APRESENTA√á√ÉO

### 1. Metabase Configuration

**Databases:**
- Databricks (primary)
- PostgreSQL (Metabase metadata)

**Dashboards:**
- Forecast Performance
- Inventory Management
- Business KPIs
- Data Quality

**Alerts:**
- MAPE > 15%
- Stock <= Reorder Point
- Data quality failures

---

### 2. dbt Semantic Layer API

**Query Example:**
```python
from dbt_semantic_interfaces import SemanticManifest

manifest = SemanticManifest.load("metrics.yml")

# Query metric
metric = manifest.get_metric("forecast_accuracy_mape")
result = metric.query(
    time_grain="month",
    dimensions=["category"],
    filters={"model_type": "prophet"}
)

print(result)
```

---

## üîí SEGURAN√áA E GOVERNAN√áA

### 1. Access Control

**Databricks:**
- Unity Catalog (table-level permissions)
- Row-level security (RLS)
- Column-level security

**Metabase:**
- Groups e permissions
- Row-level restrictions
- Data sandboxing

**dbt:**
- Environment-based profiles
- Role-based access

---

### 2. Data Lineage

**Tools:**
- dbt lineage (auto)
- DataHub (manual annotations)
- Airflow lineage (task dependencies)

**Visualization:**
```mermaid
graph TB
    A[Bronze: raw_items] --> B[Silver: stg_items]
    B --> C[Gold: dim_items]
    C --> D[Gold: fact_forecasts]
    D --> E[Metabase Dashboard]
```

---

## ‚ö° PERFORMANCE OPTIMIZATION

### 1. Query Optimization

**Indexes:**
- Delta Lake: Z-order clustering
- Automatic indexing on join keys

**Caching:**
- Metabase: Query caching (1 hour)
- dbt: Model caching (incremental)

**Partitioning:**
- Por data (automatic)
- Por categoria (opcional)

---

### 2. Cost Optimization

**Strategies:**
- Incremental models (s√≥ novos dados)
- Vacuum Delta tables regularly
- Archive old data to cold storage
- Right-size Databricks clusters

---

**Documento criado:** Novembro 2025  
**Vers√£o:** 1.0  
**Status:** ‚úÖ Arquitetura T√©cnica Completa

**CENTRALIZED REPORTS & CHANGELOG SYSTEM COMPLETE!**

