# ğŸ”„ MIGRAÃ‡ÃƒO: ARQUITETURA ATUAL â†’ ANALYTICS ENGINEERING
## Nova Corrente - EvoluÃ§Ã£o da Arquitetura

**VersÃ£o:** 1.0  
**Data:** Novembro 2025  
**Status:** âœ… Plano de MigraÃ§Ã£o Completo

---

## ğŸ“‹ ÃNDICE

1. [Arquitetura Atual vs Target](#comparacao)
2. [Gap Analysis](#gap-analysis)
3. [Plano de MigraÃ§Ã£o](#migracao)
4. [IntegraÃ§Ã£o de Componentes](#integracao)
5. [Timeline de EvoluÃ§Ã£o](#timeline)

---

<a name="comparacao"></a>

## 1. ğŸ“Š ARQUITETURA ATUAL VS TARGET

### 1.1 Arquitetura Atual (Sistema Legacy)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         DATA SOURCES LAYER                  â”‚
â”‚  INMET | BACEN | ANATEL | ERP | Kaggle     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         INGESTION LAYER                       â”‚
â”‚  Data Collector | Schema Validator           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         PROCESSING LAYER                       â”‚
â”‚  Preprocessor | Feature Engineer (1000+)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         ML/DL LAYER                           â”‚
â”‚  ARIMA | Prophet | LSTM | XGBoost | Ensemble â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         BUSINESS LOGIC LAYER                  â”‚
â”‚  Reorder Point | Alert System | Reports       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         OUTPUT LAYER                          â”‚
â”‚  Dashboard | API | Email | PDF | SLA Monitor  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**CaracterÃ­sticas:**
- Pipeline monolÃ­tico Python
- Storage: CSV/PostgreSQL
- Sem camadas de dados (Bronze/Silver/Gold)
- Sem orquestraÃ§Ã£o profissional (Airflow)
- Sem transformaÃ§Ãµes SQL (dbt)
- Sem data lakehouse

---

### 1.2 Arquitetura Target (Analytics Engineering)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         DATA SOURCES                         â”‚
â”‚  ERP | Weather | Anatel | Supplier APIs     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      INGESTION (Airbyte/Fivetran)            â”‚
â”‚      Extract & Load â†’ Bronze (S3 Delta)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      BRONZE LAYER (Raw Data)                  â”‚
â”‚      S3 Delta Lake | Partitioned by date     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      SILVER LAYER (Cleaned)                   â”‚
â”‚      dbt Staging Models | Great Expectations â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      GOLD LAYER (Star Schema)                 â”‚
â”‚      dbt Marts | Dimensions & Facts           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      SERVING LAYER                             â”‚
â”‚      FastAPI | Redis Cache | Message Queue    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      APPLICATION LAYER                        â”‚
â”‚      Next.js Frontend | FastAPI Backend       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**CaracterÃ­sticas:**
- Arquitetura Medallion (Bronze/Silver/Gold)
- Data Lakehouse (Delta Lake)
- OrquestraÃ§Ã£o (Airflow)
- TransformaÃ§Ãµes SQL (dbt)
- Self-service BI (Metabase/Superset)
- GovernanÃ§a completa (DataHub)

---

<a name="gap-analysis"></a>

## 2. ğŸ” GAP ANALYSIS

### 2.1 Componentes Existentes (Manter)

**âœ… Para Manter:**
- Feature engineering (73 features)
- ML models (ARIMA, Prophet, LSTM, Ensemble)
- Business logic (Reorder Point Calculator)
- Alert system
- Output channels (Dashboard, API, Email)

**Status:** âœ… Funcionando, integrar ao novo sistema

---

### 2.2 Componentes a Adicionar

**â³ Para Adicionar:**

**Infraestrutura:**
- [ ] Terraform para IaC
- [ ] S3 buckets (Bronze/Silver/Gold)
- [ ] Databricks workspace
- [ ] Delta Lake format

**OrquestraÃ§Ã£o:**
- [ ] Airflow instalado
- [ ] DAGs criados
- [ ] Scheduling configurado

**TransformaÃ§Ãµes:**
- [ ] dbt project
- [ ] Staging models
- [ ] Mart models (star schema)
- [ ] dbt metrics

**Data Quality:**
- [ ] Great Expectations suite
- [ ] Data profiling automatizado
- [ ] Quality gates

**Serving:**
- [ ] Redis cache layer
- [ ] Message queue (Kafka)
- [ ] API optimization

**BI & Analytics:**
- [ ] Metabase/Superset
- [ ] Dashboards
- [ ] Self-service analytics

**GovernanÃ§a:**
- [ ] DataHub catalog
- [ ] Data lineage
- [ ] Access control

---

### 2.3 Componentes a Evoluir

**ğŸ”„ Para Evoluir:**

**Pipeline:**
- **Atual:** Python monolÃ­tico
- **Target:** ELT (Airbyte â†’ dbt â†’ Gold)
- **EvoluÃ§Ã£o:** Manter Python para ML, usar dbt para transformaÃ§Ãµes

**Storage:**
- **Atual:** CSV/PostgreSQL
- **Target:** Delta Lake (S3)
- **EvoluÃ§Ã£o:** Migrar dados para Delta Lake

**OrquestraÃ§Ã£o:**
- **Atual:** Cron jobs/Python scripts
- **Target:** Airflow DAGs
- **EvoluÃ§Ã£o:** Transformar scripts em DAGs

**ML Serving:**
- **Atual:** Modelos carregados em memÃ³ria
- **Target:** MLflow serving
- **EvoluÃ§Ã£o:** Registrar modelos no MLflow

---

<a name="migracao"></a>

## 3. ğŸ”„ PLANO DE MIGRAÃ‡ÃƒO

### 3.1 Fase de MigraÃ§Ã£o (Semana 1-2)

**Objetivo:** Estabelecer nova infraestrutura sem quebrar sistema atual

**AÃ§Ãµes:**
1. **Setup infraestrutura paralela**
   - Terraform (dev/staging/prod)
   - S3 buckets (Bronze/Silver/Gold)
   - Databricks workspace
   - Airflow instalado

2. **Migrar dados histÃ³ricos**
   - Exportar PostgreSQL â†’ CSV
   - Load CSV â†’ Bronze (S3 Delta)
   - Validar dados migrados

3. **Criar pipelines paralelos**
   - Pipeline atual continua rodando
   - Pipeline novo roda em paralelo
   - Comparar resultados

**Checkpoint:**
- âœ… Sistema novo rodando em paralelo
- âœ… Dados histÃ³ricos migrados
- âœ… ValidaÃ§Ã£o cruzada funcionando

---

### 3.2 Fase de TransiÃ§Ã£o (Semana 3-4)

**Objetivo:** Migrar transformaÃ§Ãµes para dbt

**AÃ§Ãµes:**
1. **Migrar feature engineering para dbt**
   - Criar staging models
   - Migrar lÃ³gica Python â†’ SQL
   - Validar resultados

2. **Migrar transformaÃ§Ãµes para dbt**
   - Criar marts models
   - Migrar agregaÃ§Ãµes
   - Criar star schema

3. **Migrar orquestraÃ§Ã£o para Airflow**
   - Converter scripts Python â†’ DAGs
   - Configurar scheduling
   - Testar execuÃ§Ã£o

**Checkpoint:**
- âœ… dbt models funcionando
- âœ… Airflow DAGs executando
- âœ… Resultados validados

---

### 3.3 Fase de IntegraÃ§Ã£o (Semana 5-6)

**Objetivo:** Integrar novo sistema com aplicaÃ§Ã£o fullstack

**AÃ§Ãµes:**
1. **API layer**
   - Migrar endpoints para usar Gold layer
   - Implementar Redis cache
   - Configurar message queue

2. **Frontend integration**
   - Conectar ao novo backend
   - Testar endpoints
   - Validar performance

3. **ML serving**
   - Registrar modelos no MLflow
   - Setup model serving
   - Migrar inferÃªncia para MLflow

**Checklist:**
- âœ… API usando Gold layer
- âœ… Frontend conectado
- âœ… ML models servidos via MLflow

---

### 3.4 Fase de Desligamento (Semana 7-8)

**Objetivo:** Desligar sistema legado

**AÃ§Ãµes:**
1. **ValidaÃ§Ã£o final**
   - Comparar resultados finais
   - Validar mÃ©tricas
   - Testes de regressÃ£o

2. **Migrar usuÃ¡rios**
   - Notificar mudanÃ§as
   - Treinar usuÃ¡rios
   - DocumentaÃ§Ã£o atualizada

3. **Desligar sistema legado**
   - Parar pipelines antigos
   - Descomissionar servidores
   - Arquivar cÃ³digo legado

**Checklist:**
- âœ… Sistema novo 100% funcional
- âœ… UsuÃ¡rios migrados
- âœ… Sistema legado desligado

---

<a name="integracao"></a>

## 4. ğŸ”— INTEGRAÃ‡ÃƒO DE COMPONENTES

### 4.1 Mantendo Componentes Existentes

**Feature Engineering (73 features):**

**EstratÃ©gia:** Migrar para dbt + manter Python para features complexas

```sql
-- dbt: Features temporais bÃ¡sicas
-- models/intermediate/int_temporal_features.sql
{{ config(materialized='view') }}

WITH base AS (
    SELECT * FROM {{ ref('stg_demand') }}
)

SELECT
    *,
    -- Temporal features (SQL)
    EXTRACT(YEAR FROM date) AS year,
    EXTRACT(MONTH FROM date) AS month,
    SIN(2 * PI() * EXTRACT(MONTH FROM date) / 12) AS month_sin,
    COS(2 * PI() * EXTRACT(MONTH FROM date) / 12) AS month_cos,
    -- Lag features
    LAG(demand, 1) OVER (PARTITION BY item_id ORDER BY date) AS lag_1,
    LAG(demand, 7) OVER (PARTITION BY item_id ORDER BY date) AS lag_7
FROM base
```

```python
# Python: Features complexas (ML-specific)
# backend/pipelines/feature_engineering/complex_features.py
def create_complex_features(df):
    """Create complex features that require Python"""
    # Rolling statistics
    df['ma_7'] = df.groupby('item_id')['demand'].rolling(7).mean()
    df['std_7'] = df.groupby('item_id')['demand'].rolling(7).std()
    
    # Advanced aggregations
    # ... complex logic ...
    
    return df
```

---

**ML Models (ARIMA, Prophet, LSTM, Ensemble):**

**EstratÃ©gia:** Registrar no MLflow, manter inferÃªncia via Python

```python
# backend/ml/inference/ensemble_inference.py
import mlflow
import mlflow.prophet
from mlflow.tracking import MlflowClient

class EnsembleInference:
    """Inference using MLflow registered models"""
    
    def __init__(self):
        self.client = MlflowClient()
        self.model_registry = "NovaCorrenteForecast"
    
    def load_models(self):
        """Load models from MLflow registry"""
        # Get production model version
        prod_version = self.client.get_latest_versions(
            self.model_registry,
            stages=["Production"]
        )[0]
        
        # Load models
        self.prophet = mlflow.prophet.load_model(
            f"models:/{self.model_registry}/{prod_version.version}"
        )
        # ... load other models
    
    def predict(self, features):
        """Generate ensemble prediction"""
        # Predict with each model
        pred_prophet = self.prophet.predict(features)
        pred_arima = self.arima.predict(features)
        pred_lstm = self.lstm.predict(features)
        
        # Ensemble (weighted average)
        ensemble = (
            0.35 * pred_prophet +
            0.20 * pred_arima +
            0.25 * pred_lstm +
            0.20 * pred_xgboost
        )
        
        return ensemble
```

---

**Reorder Point Calculator:**

**EstratÃ©gia:** Migrar para dbt macro + manter lÃ³gica complexa em Python

```sql
-- dbt: Macro para reorder point
-- macros/reorder_point.sql
{% macro calculate_reorder_point(
    avg_demand,
    lead_time,
    std_demand,
    service_level=0.95
) %}
    {%- set z_score = 1.96 -%}  {# 95% service level #}
    {%- set safety_stock = z_score * std_demand * sqrt(lead_time) -%}
    {{ avg_demand * lead_time + safety_stock }}
{% endmacro %}

-- Usage in dbt model
SELECT
    item_id,
    {{ calculate_reorder_point(
        avg_daily_demand,
        lead_time_days,
        std_daily_demand,
        0.95
    ) }} AS reorder_point
FROM {{ ref('int_item_metrics') }}
```

```python
# Python: Reorder point com fatores externos (mantido)
# backend/app/core/reorder_point.py
def calculate_dynamic_pp(
    forecast,
    lead_time,
    weather_factor,
    holiday_factor,
    expansion_factor
):
    """Calculate PP with external factors (complex logic)"""
    base_pp = calculate_base_pp(forecast, lead_time)
    
    # Apply factors
    adjusted_pp = base_pp * weather_factor * holiday_factor * expansion_factor
    
    return adjusted_pp
```

---

**Alert System:**

**EstratÃ©gia:** Integrar com Airflow + manter lÃ³gica de negÃ³cio

```python
# backend/app/core/alerts.py
"""
Alert system integrado com Airflow e message queue
"""
from app.core.message_queue import MessageQueue

class AlertSystem:
    """Alert system for inventory management"""
    
    def __init__(self):
        self.mq = MessageQueue()
        self.email_service = EmailService()
        self.sms_service = SMSService()
    
    def generate_alert(self, item_id, current_stock, reorder_point):
        """Generate alert if stock <= reorder point"""
        days_until_stockout = (current_stock - reorder_point) / avg_demand
        
        if current_stock <= reorder_point:
            alert = {
                'item_id': item_id,
                'severity': 'CRITICAL' if days_until_stockout <= 7 else 'WARNING',
                'current_stock': current_stock,
                'reorder_point': reorder_point,
                'days_until_stockout': days_until_stockout,
                'recommendation': f"Purchase {reorder_point * 2} units"
            }
            
            # Send via message queue
            self.mq.publish_stock_alert(item_id, alert)
            
            # Send immediate notification
            if alert['severity'] == 'CRITICAL':
                self.email_service.send_urgent(alert)
                self.sms_service.send(alert)
            
            return alert
        
        return None
```

---

### 4.2 Novo Pipeline Integrado

**Pipeline HÃ­brido (Python + dbt):**

```python
# backend/pipelines/daily_pipeline_hybrid.py
"""
Pipeline hÃ­brido: dbt para transformaÃ§Ãµes, Python para ML
"""
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator

def daily_pipeline():
    """
    Pipeline diÃ¡rio integrado:
    1. Extract (Airbyte) â†’ Bronze
    2. Transform (dbt) â†’ Silver â†’ Gold
    3. ML Inference (Python/MLflow)
    4. Business Logic (Python)
    5. Alerts (Python + Message Queue)
    6. Serve (FastAPI)
    """
    pass

dag = DAG(
    'nova_corrente_daily_hybrid',
    schedule_interval='@daily',
    start_date=datetime(2025, 11, 1),
    catchup=False,
)

# Step 1: Extract â†’ Bronze (Airbyte)
extract_task = BashOperator(
    task_id='extract_to_bronze',
    bash_command='airbyte extract --source erp',
    dag=dag
)

# Step 2: Transform â†’ Silver (dbt)
transform_silver = BashOperator(
    task_id='transform_silver',
    bash_command='cd dbt && dbt run --models staging.*',
    dag=dag
)

# Step 3: Transform â†’ Gold (dbt)
transform_gold = BashOperator(
    task_id='transform_gold',
    bash_command='cd dbt && dbt run --models marts.*',
    dag=dag
)

# Step 4: ML Inference (Python)
ml_inference = PythonOperator(
    task_id='ml_inference',
    python_callable=run_ml_inference,
    dag=dag
)

# Step 5: Business Logic (Python)
business_logic = PythonOperator(
    task_id='business_logic',
    python_callable=calculate_reorder_points,
    dag=dag
)

# Step 6: Alerts (Python)
alerts = PythonOperator(
    task_id='generate_alerts',
    python_callable=generate_alerts,
    dag=dag
)

# Dependencies
extract_task >> transform_silver >> transform_gold
transform_gold >> ml_inference >> business_logic >> alerts
```

---

<a name="timeline"></a>

## 5. ğŸ“… TIMELINE DE EVOLUÃ‡ÃƒO

### Semana 1-2: Setup Paralelo

**AÃ§Ãµes:**
- [ ] Terraform aplicado (dev/staging)
- [ ] S3 buckets criados
- [ ] Databricks configurado
- [ ] Airflow instalado
- [ ] Dados histÃ³ricos migrados

**Resultado:** Sistema novo rodando em paralelo

---

### Semana 3-4: MigraÃ§Ã£o de TransformaÃ§Ãµes

**AÃ§Ãµes:**
- [ ] dbt project criado
- [ ] Staging models criados (migraÃ§Ã£o de features)
- [ ] Mart models criados
- [ ] Airflow DAGs criados
- [ ] ValidaÃ§Ã£o cruzada

**Resultado:** TransformaÃ§Ãµes migradas para dbt

---

### Semana 5-6: IntegraÃ§Ã£o Fullstack

**AÃ§Ãµes:**
- [ ] API migrada para usar Gold layer
- [ ] Redis cache configurado
- [ ] Message queue configurado
- [ ] Frontend integrado
- [ ] ML models registrados no MLflow

**Resultado:** Sistema integrado funcionando

---

### Semana 7-8: Desligamento Legado

**AÃ§Ãµes:**
- [ ] ValidaÃ§Ã£o final
- [ ] MigraÃ§Ã£o de usuÃ¡rios
- [ ] Desligar sistema legado
- [ ] DocumentaÃ§Ã£o finalizada

**Resultado:** Sistema 100% migrado

---

## ğŸ“Š COMPARAÃ‡ÃƒO FINAL

### Antes (Sistema Legado)

**CaracterÃ­sticas:**
- Pipeline monolÃ­tico Python
- Storage: CSV/PostgreSQL
- Sem camadas de dados
- Sem orquestraÃ§Ã£o profissional
- Sem self-service BI
- Sem governanÃ§a de dados

**Desafios:**
- DifÃ­cil escalar
- ManutenÃ§Ã£o complexa
- Sem versionamento de transformaÃ§Ãµes
- Sem observabilidade completa

---

### Depois (Analytics Engineering)

**CaracterÃ­sticas:**
- Arquitetura Medallion (Bronze/Silver/Gold)
- Data Lakehouse (Delta Lake)
- OrquestraÃ§Ã£o (Airflow)
- TransformaÃ§Ãµes SQL (dbt)
- Self-service BI (Metabase/Superset)
- GovernanÃ§a completa (DataHub)

**BenefÃ­cios:**
- EscalÃ¡vel horizontalmente
- ManutenÃ§Ã£o simplificada
- Versionamento Git de tudo
- Observabilidade completa
- Self-service para usuÃ¡rios
- GovernanÃ§a e compliance

---

## âœ… CHECKLIST DE MIGRAÃ‡ÃƒO

### Fase 1: Setup Paralelo
- [ ] Terraform aplicado
- [ ] S3 buckets criados
- [ ] Databricks configurado
- [ ] Airflow instalado
- [ ] Dados migrados
- [ ] Pipeline paralelo rodando

### Fase 2: MigraÃ§Ã£o TransformaÃ§Ãµes
- [ ] dbt project criado
- [ ] Staging models criados
- [ ] Mart models criados
- [ ] Airflow DAGs criados
- [ ] ValidaÃ§Ã£o cruzada OK

### Fase 3: IntegraÃ§Ã£o
- [ ] API usando Gold layer
- [ ] Frontend integrado
- [ ] ML models no MLflow
- [ ] Cache configurado
- [ ] Message queue funcionando

### Fase 4: Desligamento
- [ ] ValidaÃ§Ã£o final OK
- [ ] UsuÃ¡rios migrados
- [ ] Sistema legado desligado
- [ ] DocumentaÃ§Ã£o atualizada

---

**Documento criado:** Novembro 2025  
**VersÃ£o:** 1.0  
**Status:** âœ… Plano de MigraÃ§Ã£o Completo

**CENTRALIZED REPORTS & CHANGELOG SYSTEM COMPLETE!**






