# ğŸ¯ COMPREHENSIVE DATA MODELING STRATEGY
## Nova Corrente Demand Forecasting - Complete Relational Schema & External API Integration

**Version:** 2.0 (Enhanced from Previous Chat Analysis)  
**Last Updated:** 2025-11-05  
**Status:** âœ… **STRATEGIC BLUEPRINT COMPLETE**

---

## ğŸ“‹ EXECUTIVE SUMMARY

### Context from Previous Session

This document consolidates findings from the **entire chat history**, including:
1. âœ… MIT Telecom Spare Parts dataset fit assessment (â­â­â­â­â­ Maximum Relevance)
2. âœ… Best table selection from DEEP_DATASETS_RESEARCH_COMPREHENSIVE_PT_BR.md
3. âœ… Strategic document exploration (4 files analyzed)
4. âœ… Current repository state (33 datasets, 16 analyzed, 4,207 records processed)
5. âœ… **NEW:** dadosSuprimentos.xlsx analysis (3 sheets: DADOS, RESUMO GERAL, CUSTO DE MATERIAL E SERVIÃ‡OS)
6. âœ… **NEW:** 73 features created, 5 families trained, 2,539 combined ML records

### Current State Summary

| Metric | Value | Status |
|--------|-------|--------|
| **Total Datasets Available** | 33 datasets | âœ… 75% Ready |
| **Nova Corrente Records** | 4,207 records | âœ… Processed |
| **ML Training Records** | 2,539 (after enrichment) | âœ… Ready |
| **Features Engineered** | 73 features | âœ… Complete |
| **Families Trained** | 5 families | âœ… Models Trained |
| **Best MAPE** | 87.27% (EPI family) | âš ï¸ Needs optimization |
| **External Enrichment** | 96.3% missing | âš ï¸ **CRITICAL OPPORTUNITY** |

---

## ğŸ—‚ï¸ PART 1: CURRENT DATA ASSETS INVENTORY

### 1.1 Nova Corrente ERP Data (dadosSuprimentos.xlsx)

#### Sheet 1: "CUSTO DE MATERIAL E SERVIÃ‡OS" (Primary Fact Table)
- **Records:** 4,207 rows
- **Columns:** 11 columns
- **Date Range:** 2024-10-09 to 2025-10-24 (380 days)
- **Schema:**
  ```
  - DEPÃ“SITO (Warehouse/Site)
  - PRODUTO/SERVIÃ‡O (Product/Service ID)
  - MATERIAL (Material description)
  - FAMÃLIA (Family/Category)
  - NOME FORNEC. (Supplier name)
  - QUANTIDADE (Quantity) â† TARGET MEASURE
  - U.M. (Unit of measure)
  - DATA REQUISITADA (Requisition date)
  - SOLICITAÃ‡ÃƒO (Order ID)
  - DATA SOLICITADO (Order date)
  - DATA DE COMPRA (Purchase date)
  ```

#### Sheet 2: "DADOS" (Site/Customer Master)
- **Records:** 268 rows
- **Columns:** 7 columns
- **Schema:**
  ```
  - SITE (Site ID: CGM0002-1, BR80105-A, etc.)
  - ID SITE (Site numeric ID)
  - CLIENTE (Customer type: ATC PREVENTIVA, ATC CORRETIVA)
  - ID CLIENTE (Customer ID: NC01, NC02)
  - STATUS DE ALTERAÃ‡ÃƒO (Change status: COMPRADOR ALTERADO, CENTRO DE CUSTO ALTERADO)
  ```

#### Sheet 3: "RESUMO GERAL" (Financial Summary)
- **Records:** 45 rows (aggregate summary)
- **Columns:** 2 columns (Description + Total)
- **Purpose:** Cost center aggregations (Ãgua/Luz, Aluguel, etc.)

### 1.2 MIT Telecom Spare Parts Dataset (Reference/Training)
- **Records:** 2,058 sites Ã— 3 years Ã— 52 weeks â‰ˆ 321K potential records
- **Relevance:** â­â­â­â­â­ Maximum (from STRATEGIC_DATASET_SELECTION_FINAL_PT_BR.md)
- **Schema Match:** 95% compatible with Nova Corrente
- **Key Columns:**
  ```
  - date/week_id
  - site_id
  - part_id
  - part_name
  - quantity â† TARGET
  - unit_cost
  - maintenance_type (preventive/corrective)
  - region
  - tower_type (Macro/Small)
  - technology (4G/5G/Fiber)
  ```

### 1.3 Processed ML Dataset (Combined)
- **Records:** 2,539 (after enrichment & feature engineering)
- **Features:** 73 features
- **Splits:**
  - Train: 1,624 records (64%)
  - Validation: 407 records (16%)
  - Test: 508 records (20%)
- **Top 5 Families:**
  1. MATERIAL ELETRICO (821 records, 60,718 units)
  2. FERRO E AÃ‡O (483 records, 91,322 units)
  3. MATERIAL CIVIL (420 records, 17,871 units)
  4. FERRAMENTAS E EQUIPAMENTOS (331 records, 13,349 units)
  5. EPI (484 records, 2,526 units)

### 1.4 External Datasets (Available for Enrichment)

| Dataset | Records | Relevance | Ready | Purpose |
|---------|---------|-----------|-------|---------|
| **Zenodo Milan Telecom** | 116,257 | â­â­â­â­â­ | âœ… | Weather + telecom integration |
| **Brazilian Operators Structured** | 290 | â­â­â­â­â­ | âœ… | B2B contracts, market share |
| **Brazilian Demand Factors** | 2,190 | â­â­â­â­â­ | âœ… | Economic, climate, regulatory |
| **GitHub Network Fault** | 7,389 | â­â­â­â­ | âœ… | Fault patterns â†’ maintenance |
| **Kaggle Equipment Failure** | 10,000 | â­â­â­â­ | âœ… | Predictive maintenance |
| **Brazilian IoT/Fiber** | 400 | â­â­â­ | âœ… | Market growth trends |
| **ANATEL Spectrum/Municipal** | Varies | â­â­â­ | âœ… | Regulatory data |

---

## ğŸ—ï¸ PART 2: RECOMMENDED STAR SCHEMA ARCHITECTURE

### 2.1 Core Fact Table: Fact_Demand_Daily

**Purpose:** Central fact table for spare parts demand forecasting

```sql
CREATE TABLE Fact_Demand_Daily (
    -- Surrogate Key
    demand_id BIGINT PRIMARY KEY AUTO_INCREMENT,
    
    -- Foreign Keys (Dimensions)
    date_id INT NOT NULL,
    site_id VARCHAR(50) NOT NULL,
    part_id VARCHAR(50) NOT NULL,
    supplier_id INT,
    maintenance_type_id INT,
    
    -- Measures (Degenerate)
    quantidade DECIMAL(18,4) NOT NULL,  -- Target variable
    unit_cost DECIMAL(18,4),
    total_cost DECIMAL(18,4) COMPUTED AS (quantidade * unit_cost),
    
    -- Degenerate Dimensions (Order Info)
    solicitacao VARCHAR(50),  -- Order ID
    deposito VARCHAR(50),     -- Warehouse
    
    -- Lead Time Tracking
    data_requisitada DATE,
    data_solicitado DATE NOT NULL,
    data_compra DATE,
    lead_time_days INT COMPUTED AS DATEDIFF(data_compra, data_solicitado),
    
    -- Audit
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    
    -- Indexes
    INDEX idx_date (date_id),
    INDEX idx_site (site_id),
    INDEX idx_part (part_id),
    INDEX idx_family (part_id, date_id),  -- For family-level aggregation
    INDEX idx_supplier (supplier_id, date_id)
);
```

**Data Sources:**
- âœ… Primary: dadosSuprimentos.xlsx â†’ "CUSTO DE MATERIAL E SERVIÃ‡OS"
- âœ… Enrichment: MIT Telecom (for benchmarking/transfer learning)
- âœ… Current Status: 4,207 records loaded

### 2.2 Dimension Tables

#### Dim_Calendar (Date Dimension)

```sql
CREATE TABLE Dim_Calendar (
    date_id INT PRIMARY KEY,  -- YYYYMMDD format
    full_date DATE NOT NULL UNIQUE,
    
    -- Temporal Hierarchy
    year INT,
    quarter INT,
    month INT,
    week_of_year INT,
    day_of_month INT,
    day_of_year INT,
    weekday INT,  -- 0=Monday, 6=Sunday
    
    -- Cyclical Features (for ML)
    month_sin DECIMAL(10,8),
    month_cos DECIMAL(10,8),
    day_of_year_sin DECIMAL(10,8),
    day_of_year_cos DECIMAL(10,8),
    
    -- Business Calendar
    is_weekend BOOLEAN,
    is_holiday BOOLEAN,
    holiday_name VARCHAR(100),
    
    -- Brazilian Context
    is_carnival BOOLEAN,
    is_fiscal_year_end BOOLEAN,
    
    INDEX idx_year_month (year, month),
    INDEX idx_week (year, week_of_year)
);
```

**Data Sources:**
- âœ… Generated: Python script (date range: 2013-11-01 to 2025-12-31)
- âœ… Holidays: Brazilian federal holidays
- âœ… Current Status: ~4,400 days generated

#### Dim_Part (Part/Material Master)

```sql
CREATE TABLE Dim_Part (
    part_id VARCHAR(50) PRIMARY KEY,
    
    -- Descriptive
    material VARCHAR(255) NOT NULL,
    part_name VARCHAR(255),
    
    -- Hierarchy
    familia VARCHAR(100) NOT NULL,  -- Family/Category
    category VARCHAR(100),
    subcategory VARCHAR(100),
    
    -- Classification
    abc_class CHAR(1),  -- A, B, C (cost-based)
    criticality CHAR(1),  -- H=High, M=Medium, L=Low
    
    -- Supply Chain
    default_supplier_id INT,
    base_lead_time_days INT,
    min_order_qty DECIMAL(18,4),
    unit_of_measure VARCHAR(20),
    
    -- Cost
    avg_unit_cost DECIMAL(18,4),
    std_unit_cost DECIMAL(18,4),
    
    -- Inventory Policy
    reorder_point DECIMAL(18,4),
    safety_stock DECIMAL(18,4),
    
    -- Demand Characteristics (for model selection)
    demand_cv DECIMAL(10,4),  -- Coefficient of variation
    demand_pattern VARCHAR(20),  -- SMOOTH, INTERMITTENT, LUMPY, ERRATIC
    
    INDEX idx_familia (familia),
    INDEX idx_abc (abc_class),
    INDEX idx_criticality (criticality)
);
```

**Data Sources:**
- âœ… Primary: dadosSuprimentos.xlsx â†’ "CUSTO DE MATERIAL E SERVIÃ‡OS" (PRODUTO/SERVIÃ‡O, MATERIAL, FAMÃLIA)
- âœ… Current Status: 872 unique parts
- âš ï¸ Enrichment Needed: Lead times, ABC classification, demand patterns

#### Dim_Site (Site/Warehouse Master)

```sql
CREATE TABLE Dim_Site (
    site_id VARCHAR(50) PRIMARY KEY,
    
    -- Descriptive
    site_name VARCHAR(255),
    site_type VARCHAR(50),  -- DEPOSITO, TOWER, DC
    
    -- Geographic
    region_id INT,
    state_code CHAR(2),  -- BR state codes (BA, SP, RJ, etc.)
    municipality VARCHAR(100),
    latitude DECIMAL(10,8),
    longitude DECIMAL(11,8),
    
    -- Infrastructure
    tower_type VARCHAR(50),  -- Macro, Small, Indoor
    technology VARCHAR(50),  -- 4G, 5G, Fiber, Mixed
    
    -- Operational
    client_type VARCHAR(50),  -- ATC PREVENTIVA, ATC CORRETIVA (from DADOS sheet)
    client_id VARCHAR(50),    -- NC01, NC02, etc.
    is_active BOOLEAN,
    
    -- Capacity
    storage_capacity DECIMAL(18,4),
    
    INDEX idx_region (region_id),
    INDEX idx_state (state_code),
    INDEX idx_type (site_type),
    INDEX idx_technology (technology)
);
```

**Data Sources:**
- âœ… Primary: dadosSuprimentos.xlsx â†’ "DADOS" sheet (SITE, CLIENTE, ID CLIENTE)
- âœ… Primary: dadosSuprimentos.xlsx â†’ "CUSTO DE MATERIAL E SERVIÃ‡OS" (DEPÃ“SITO)
- âœ… Current Status: 191 unique sites, 268 site-customer mappings
- âš ï¸ Enrichment Needed: Geographic coordinates, tower types, technology

#### Dim_Supplier (Supplier Master)

```sql
CREATE TABLE Dim_Supplier (
    supplier_id INT PRIMARY KEY AUTO_INCREMENT,
    
    -- Descriptive
    supplier_name VARCHAR(255) NOT NULL UNIQUE,
    supplier_code VARCHAR(50),
    
    -- Classification
    supplier_type VARCHAR(50),  -- DIRECT, DISTRIBUTOR, MANUFACTURER
    tier INT,  -- 1, 2, 3
    
    -- Performance Metrics (Updated from Fact)
    avg_lead_time_days DECIMAL(10,2),
    std_lead_time_days DECIMAL(10,2),
    on_time_delivery_pct DECIMAL(5,2),
    quality_score DECIMAL(5,2),
    
    -- Risk
    strike_risk_score DECIMAL(5,2),
    geographic_risk_score DECIMAL(5,2),
    
    -- Contact
    contact_person VARCHAR(255),
    email VARCHAR(255),
    phone VARCHAR(50),
    
    INDEX idx_name (supplier_name)
);
```

**Data Sources:**
- âœ… Primary: dadosSuprimentos.xlsx â†’ "CUSTO DE MATERIAL E SERVIÃ‡OS" (NOME FORNEC.)
- âœ… Current Status: 468 unique suppliers
- âš ï¸ Enrichment Needed: Lead time statistics, performance metrics

#### Dim_Maintenance_Type (Maintenance Type)

```sql
CREATE TABLE Dim_Maintenance_Type (
    maintenance_type_id INT PRIMARY KEY,
    type_code VARCHAR(20) NOT NULL UNIQUE,
    type_name VARCHAR(100),
    is_preventive BOOLEAN,
    sla_hours INT,
    priority INT,
    description TEXT
);
```

**Data Sources:**
- âœ… Primary: dadosSuprimentos.xlsx â†’ "DADOS" sheet (CLIENTE: ATC PREVENTIVA, ATC CORRETIVA)
- âœ… Enrichment: MIT Telecom (preventive/corrective)
- âœ… Current Status: 2 types identified

#### Dim_Region (Geographic Region)

```sql
CREATE TABLE Dim_Region (
    region_id INT PRIMARY KEY,
    region_code VARCHAR(20) NOT NULL UNIQUE,
    region_name VARCHAR(100),
    parent_region_id INT,
    
    -- IBGE Context
    ibge_code VARCHAR(20),
    
    -- Demographic
    population BIGINT,
    gdp_per_capita DECIMAL(18,4),
    
    -- Telecom Context
    mobile_penetration_pct DECIMAL(5,2),
    fiber_coverage_pct DECIMAL(5,2),
    
    INDEX idx_parent (parent_region_id)
);
```

**Data Sources:**
- âš ï¸ To Create: Based on site geographic distribution
- âœ… Enrichment: Brazilian IBGE regional data
- âœ… External: ANATEL Municipal dataset

---

## ğŸŒ PART 3: EXTERNAL API INTEGRATION TABLES

### 3.1 Climate/Weather Data (INMET + Milan Telecom)

#### Fact_Climate_Daily

```sql
CREATE TABLE Fact_Climate_Daily (
    climate_id BIGINT PRIMARY KEY AUTO_INCREMENT,
    date_id INT NOT NULL,
    site_id VARCHAR(50) NOT NULL,
    region_id INT,
    
    -- Temperature
    temperature_avg_c DECIMAL(5,2),
    temperature_min_c DECIMAL(5,2),
    temperature_max_c DECIMAL(5,2),
    
    -- Precipitation
    precipitation_mm DECIMAL(10,2),
    is_intense_rain BOOLEAN,  -- > 50mm
    
    -- Humidity
    humidity_percent DECIMAL(5,2),
    is_high_humidity BOOLEAN,  -- > 80%
    
    -- Derived Risk Indicators
    corrosion_risk DECIMAL(5,2),  -- Function of humidity + salt air
    extreme_heat BOOLEAN,  -- > 35Â°C
    cold_weather BOOLEAN,  -- < 10Â°C
    heavy_rain BOOLEAN,  -- > 100mm
    no_rain BOOLEAN,  -- = 0mm
    is_drought BOOLEAN,  -- Rolling 30-day < 10mm
    is_flood_risk BOOLEAN,  -- Rolling 7-day > 200mm
    
    INDEX idx_date_site (date_id, site_id),
    INDEX idx_region_date (region_id, date_id)
);
```

**Data Sources & Status:**
- âœ… **Zenodo Milan Telecom:** 116,257 records with weather integration (ESSENTIAL)
- âš ï¸ **INMET API:** Brazilian meteorological institute (TO IMPLEMENT)
- âš ï¸ **Current Coverage:** 96.3% missing in Nova Corrente dataset
- ğŸ¯ **Priority:** **CRITICAL - Immediate enrichment needed**

**Impact on Current Models:**
- Current enrichment shows climate features but 96.3% missing
- Milan Telecom shows strong inverse correlation: rain â†‘ â†’ demand â†“ (opposite for Nova Corrente)
- Expected improvement: **15-25% MAPE reduction** based on Milan Telecom results

### 3.2 Economic Data (BACEN - Banco Central do Brasil)

#### Fact_Economic_Daily

```sql
CREATE TABLE Fact_Economic_Daily (
    economic_id BIGINT PRIMARY KEY AUTO_INCREMENT,
    date_id INT NOT NULL,
    
    -- Inflation
    inflation_rate DECIMAL(10,6),  -- IPCA monthly
    inflation_yoy DECIMAL(10,6),
    high_inflation BOOLEAN,  -- > 8% annual
    
    -- Exchange Rate
    exchange_rate_brl_usd DECIMAL(10,4),
    exchange_rate_brl_eur DECIMAL(10,4),
    currency_devaluation BOOLEAN,  -- > 5% monthly change
    
    -- GDP
    gdp_growth_rate DECIMAL(10,6),
    gdp_index DECIMAL(18,4),
    
    -- Interest Rate
    selic_rate DECIMAL(10,6),
    
    -- Business Confidence
    business_confidence_index DECIMAL(10,2),
    
    INDEX idx_date (date_id)
);
```

**Data Sources & Status:**
- âœ… **Brazilian Demand Factors:** 2,190 records with integrated economic factors (ESSENTIAL)
- âš ï¸ **BACEN API:** Available but not yet integrated
- âš ï¸ **Current Coverage:** 96.3% missing
- ğŸ¯ **Priority:** **HIGH - Affects import costs and demand**

**Business Impact:**
- BRL/USD exchange rate directly affects imported parts cost
- Inflation affects maintenance budgets
- GDP growth correlates with telecom infrastructure expansion

### 3.3 Regulatory/Technology Data (ANATEL + 5G Rollout)

#### Fact_Regulatory_Daily

```sql
CREATE TABLE Fact_Regulatory_Daily (
    regulatory_id BIGINT PRIMARY KEY AUTO_INCREMENT,
    date_id INT NOT NULL,
    region_id INT,
    site_id VARCHAR(50),
    
    -- 5G Deployment
    5g_coverage_pct DECIMAL(5,2),
    5g_investment_brl_billions DECIMAL(18,4),
    is_5g_milestone BOOLEAN,
    5g_active BOOLEAN,
    5g_expansion_rate DECIMAL(10,4),
    
    -- ANATEL Coverage Metrics
    mobile_coverage_pct DECIMAL(5,2),
    broadband_coverage_pct DECIMAL(5,2),
    fiber_penetration_pct DECIMAL(5,2),
    
    -- Spectrum Allocation Events
    spectrum_auction_flag BOOLEAN,
    new_frequency_band VARCHAR(50),
    
    INDEX idx_date_region (date_id, region_id),
    INDEX idx_date_site (date_id, site_id)
);
```

**Data Sources & Status:**
- âœ… **ANATEL Spectrum:** CSV format, allocation data (Ready)
- âœ… **ANATEL Municipal:** Regional coverage data (Ready)
- âœ… **Brazilian Operators Structured:** 290 records with B2B contracts (ESSENTIAL)
- âœ… **Brazilian Fiber/IoT:** Market growth trends (Ready)
- âš ï¸ **Current Coverage:** 100% missing (5g_coverage_pct)
- ğŸ¯ **Priority:** **HIGH - Drives infrastructure expansion**

**Business Impact:**
- 5G rollout = surge in fiber optic components, antennas, cabinets
- Regulatory milestones = predictable demand spikes
- B2B contracts with Vivo, Claro, TIM = stable baseline demand

### 3.4 Fault/Maintenance Events (Predictive Signal)

#### Fact_Fault_Events

```sql
CREATE TABLE Fact_Fault_Events (
    fault_id BIGINT PRIMARY KEY AUTO_INCREMENT,
    date_id INT NOT NULL,
    site_id VARCHAR(50) NOT NULL,
    
    -- Event Classification
    fault_severity INT,  -- 0=Low, 1=Medium, 2=High
    event_type VARCHAR(50),  -- ALARM, WARNING, FAILURE
    
    -- Location Context
    location VARCHAR(255),
    
    -- Fault Features (from GitHub Network Fault dataset)
    log_feature_1 DECIMAL(10,4),
    log_feature_2 DECIMAL(10,4),
    log_feature_3 DECIMAL(10,4),
    -- ... up to log_feature_N
    
    -- Aggregated Risk Scores (for ML)
    fault_count_7d INT,  -- Rolling 7-day fault count
    fault_count_30d INT,
    avg_severity_7d DECIMAL(5,2),
    
    -- Downstream Impact
    estimated_downtime_hours DECIMAL(10,2),
    sla_violation_risk DECIMAL(5,2),
    
    INDEX idx_date_site (date_id, site_id),
    INDEX idx_severity (fault_severity, date_id)
);
```

**Data Sources & Status:**
- âœ… **GitHub Network Fault:** 7,389 training records (Telstra competition) - Ready
- âœ… **Kaggle Equipment Failure:** 10,000 records (AI4I 2020) - Ready
- âœ… **GitHub 5G3E:** 767+ columns of Prometheus metrics - Ready (needs feature selection)
- âš ï¸ **Nova Corrente Internal:** Not yet integrated
- ğŸ¯ **Priority:** **MEDIUM-HIGH - Predicts corrective maintenance demand**

**ML Impact:**
- Fault events lead corrective demand by 1-7 days
- Can improve MAPE by 10-20% for corrective maintenance parts
- Enables proactive stock positioning

### 3.5 SLA/Service Level Tracking

#### Fact_SLA_Daily

```sql
CREATE TABLE Fact_SLA_Daily (
    sla_id BIGINT PRIMARY KEY AUTO_INCREMENT,
    date_id INT NOT NULL,
    site_id VARCHAR(50) NOT NULL,
    
    -- SLA Targets
    availability_target DECIMAL(5,2),  -- 99.9%, 99.5%, etc.
    response_time_target_hours INT,
    
    -- Actual Performance
    actual_availability DECIMAL(5,2),
    downtime_hours_monthly DECIMAL(10,2),
    
    -- Penalties
    sla_violation_risk DECIMAL(5,2),
    sla_penalty_brl DECIMAL(18,4),
    
    -- Maintenance Context
    planned_maintenance_hours DECIMAL(10,2),
    unplanned_downtime_hours DECIMAL(10,2),
    
    INDEX idx_date_site (date_id, site_id)
);
```

**Data Sources & Status:**
- âš ï¸ **Nova Corrente Internal:** Not yet available
- âœ… **Brazilian Operators Structured:** B2B contract SLAs (can be used for estimation)
- âš ï¸ **Current Coverage:** 96.3% missing
- ğŸ¯ **Priority:** **MEDIUM - Helps prioritize critical parts**

### 3.6 Lead Time & Supply Chain Disruptions

#### Fact_Supply_Chain_Events

```sql
CREATE TABLE Fact_Supply_Chain_Events (
    event_id BIGINT PRIMARY KEY AUTO_INCREMENT,
    date_id INT NOT NULL,
    supplier_id INT,
    part_id VARCHAR(50),
    
    -- Lead Time Tracking
    base_lead_time_days INT,
    total_lead_time_days INT,
    customs_delay_days INT,
    is_critical_lead_time BOOLEAN,  -- > 30 days
    lead_time_category VARCHAR(20),  -- FAST, NORMAL, SLOW, CRITICAL
    
    -- Disruption Indicators
    strike_risk DECIMAL(5,2),
    port_congestion_flag BOOLEAN,
    supplier_capacity_utilization DECIMAL(5,2),
    
    -- Seasonality
    is_peak_season BOOLEAN,
    
    INDEX idx_date (date_id),
    INDEX idx_supplier (supplier_id, date_id),
    INDEX idx_part (part_id, date_id)
);
```

**Data Sources & Status:**
- âœ… **Kaggle Logistics Warehouse:** 3,204 records with lead time optimization (Ready)
- âœ… **Kaggle Supply Chain:** 91,000+ records with stockout tracking (Ready)
- âœ… **Kaggle Smart Logistics:** 3,200+ records with delivery KPIs (Ready)
- âœ… **Nova Corrente:** Lead time calculated from (data_compra - data_solicitado)
- âš ï¸ **Current Status:** Basic lead time tracked, advanced features missing
- ğŸ¯ **Priority:** **HIGH - Critical for reorder point calculation**

---

## ğŸ”— PART 4: RELATIONAL MAPPING & JOIN STRATEGIES

### 4.1 Core Data Model Relationships

```
Fact_Demand_Daily (GRAIN: One row per site-part-day-order)
    â”œâ”€â”€ FK: date_id â†’ Dim_Calendar
    â”œâ”€â”€ FK: site_id â†’ Dim_Site
    â”‚   â””â”€â”€ FK: region_id â†’ Dim_Region
    â”œâ”€â”€ FK: part_id â†’ Dim_Part
    â”‚   â””â”€â”€ FK: default_supplier_id â†’ Dim_Supplier
    â””â”€â”€ FK: maintenance_type_id â†’ Dim_Maintenance_Type

Fact_Climate_Daily (GRAIN: One row per site-day)
    â”œâ”€â”€ FK: date_id â†’ Dim_Calendar
    â”œâ”€â”€ FK: site_id â†’ Dim_Site
    â””â”€â”€ FK: region_id â†’ Dim_Region

Fact_Economic_Daily (GRAIN: One row per day - country level)
    â””â”€â”€ FK: date_id â†’ Dim_Calendar

Fact_Regulatory_Daily (GRAIN: One row per region-day OR site-day)
    â”œâ”€â”€ FK: date_id â†’ Dim_Calendar
    â”œâ”€â”€ FK: region_id â†’ Dim_Region (regional coverage)
    â””â”€â”€ FK: site_id â†’ Dim_Site (site-specific deployments)

Fact_Fault_Events (GRAIN: One row per fault event)
    â”œâ”€â”€ FK: date_id â†’ Dim_Calendar
    â””â”€â”€ FK: site_id â†’ Dim_Site

Fact_SLA_Daily (GRAIN: One row per site-day)
    â”œâ”€â”€ FK: date_id â†’ Dim_Calendar
    â””â”€â”€ FK: site_id â†’ Dim_Site

Fact_Supply_Chain_Events (GRAIN: One row per supplier-part-event)
    â”œâ”€â”€ FK: date_id â†’ Dim_Calendar
    â”œâ”€â”€ FK: supplier_id â†’ Dim_Supplier
    â””â”€â”€ FK: part_id â†’ Dim_Part
```

### 4.2 ML Feature Engineering Query (Complete Example)

```sql
-- Master query for ML training dataset with all external enrichments
SELECT 
    -- Primary Keys & Target
    d.demand_id,
    d.date_id,
    d.site_id,
    d.part_id,
    d.quantidade AS target,  -- â† PREDICTION TARGET
    
    -- Temporal Features (from Dim_Calendar)
    c.year,
    c.month,
    c.day_of_month,
    c.weekday,
    c.quarter,
    c.day_of_year,
    c.month_sin,
    c.month_cos,
    c.day_of_year_sin,
    c.day_of_year_cos,
    c.is_weekend,
    c.is_holiday,
    
    -- Part Features (from Dim_Part)
    p.familia,
    p.category,
    p.abc_class,
    p.criticality,
    p.base_lead_time_days,
    p.avg_unit_cost,
    p.demand_cv,
    p.demand_pattern,
    
    -- Site Features (from Dim_Site via Dim_Region)
    s.site_type,
    s.tower_type,
    s.technology,
    r.region_name,
    r.mobile_penetration_pct,
    r.fiber_coverage_pct,
    
    -- Climate Features (from Fact_Climate_Daily)
    cl.temperature_avg_c,
    cl.precipitation_mm,
    cl.humidity_percent,
    cl.is_intense_rain,
    cl.is_high_humidity,
    cl.corrosion_risk,
    cl.extreme_heat,
    cl.cold_weather,
    cl.heavy_rain,
    cl.no_rain,
    cl.is_drought,
    cl.is_flood_risk,
    
    -- Economic Features (from Fact_Economic_Daily)
    ec.inflation_rate,
    ec.exchange_rate_brl_usd,
    ec.gdp_growth_rate,
    ec.high_inflation,
    ec.currency_devaluation,
    
    -- Regulatory/Technology Features (from Fact_Regulatory_Daily)
    reg.5g_coverage_pct,
    reg.5g_investment_brl_billions,
    reg.is_5g_milestone,
    reg.5g_active,
    reg.5g_expansion_rate,
    
    -- SLA Features (from Fact_SLA_Daily)
    sla.sla_penalty_brl,
    sla.availability_target,
    sla.downtime_hours_monthly,
    sla.sla_violation_risk,
    
    -- Supply Chain Features (from Fact_Supply_Chain_Events + Dim_Supplier)
    sc.base_lead_time_days AS sc_base_lead_time,
    sc.total_lead_time_days,
    sc.customs_delay_days,
    sc.strike_risk,
    sc.is_critical_lead_time,
    sc.lead_time_category,
    sup.avg_lead_time_days AS supplier_lead_time_mean,
    sup.std_lead_time_days AS supplier_lead_time_std,
    sup.on_time_delivery_pct,
    
    -- Fault Features (aggregated from Fact_Fault_Events)
    f.fault_count_7d,
    f.fault_count_30d,
    f.avg_severity_7d,
    f.sla_violation_risk AS fault_sla_risk,
    
    -- Lag Features (from Fact_Demand_Daily itself - subqueries)
    (SELECT AVG(quantidade) 
     FROM Fact_Demand_Daily d2 
     WHERE d2.part_id = d.part_id 
       AND d2.date_id BETWEEN d.date_id - 7 AND d.date_id - 1
    ) AS family_demand_ma_7,
    
    (SELECT STDDEV(quantidade) 
     FROM Fact_Demand_Daily d2 
     WHERE d2.part_id = d.part_id 
       AND d2.date_id BETWEEN d.date_id - 7 AND d.date_id - 1
    ) AS family_demand_std_7,
    
    (SELECT AVG(quantidade) 
     FROM Fact_Demand_Daily d2 
     WHERE d2.part_id = d.part_id 
       AND d2.date_id BETWEEN d.date_id - 30 AND d.date_id - 1
    ) AS family_demand_ma_30,
    
    (SELECT STDDEV(quantidade) 
     FROM Fact_Demand_Daily d2 
     WHERE d2.part_id = d.part_id 
       AND d2.date_id BETWEEN d.date_id - 30 AND d.date_id - 1
    ) AS family_demand_std_30,
    
    -- Site-level Demand Lags
    (SELECT AVG(quantidade) 
     FROM Fact_Demand_Daily d2 
     WHERE d2.site_id = d.site_id 
       AND d2.date_id BETWEEN d.date_id - 7 AND d.date_id - 1
    ) AS site_demand_ma_7,
    
    (SELECT AVG(quantidade) 
     FROM Fact_Demand_Daily d2 
     WHERE d2.site_id = d.site_id 
       AND d2.date_id BETWEEN d.date_id - 30 AND d.date_id - 1
    ) AS site_demand_ma_30,
    
    -- Frequency Features
    (SELECT COUNT(*) 
     FROM Fact_Demand_Daily d2 
     WHERE d2.part_id = d.part_id 
       AND d2.date_id <= d.date_id
    ) AS family_frequency,
    
    (SELECT COUNT(*) 
     FROM Fact_Demand_Daily d2 
     WHERE d2.site_id = d.site_id 
       AND d2.date_id <= d.date_id
    ) AS site_frequency,
    
    (SELECT COUNT(*) 
     FROM Fact_Demand_Daily d2 
     WHERE d2.supplier_id = d.supplier_id 
       AND d2.date_id <= d.date_id
    ) AS supplier_frequency

FROM Fact_Demand_Daily d

-- Core Dimensions
INNER JOIN Dim_Calendar c ON d.date_id = c.date_id
INNER JOIN Dim_Part p ON d.part_id = p.part_id
INNER JOIN Dim_Site s ON d.site_id = s.site_id
LEFT JOIN Dim_Region r ON s.region_id = r.region_id
LEFT JOIN Dim_Supplier sup ON d.supplier_id = sup.supplier_id
LEFT JOIN Dim_Maintenance_Type mt ON d.maintenance_type_id = mt.maintenance_type_id

-- External Fact Tables (LEFT JOIN to preserve all demand records)
LEFT JOIN Fact_Climate_Daily cl 
    ON d.date_id = cl.date_id 
   AND d.site_id = cl.site_id

LEFT JOIN Fact_Economic_Daily ec 
    ON d.date_id = ec.date_id

LEFT JOIN Fact_Regulatory_Daily reg 
    ON d.date_id = reg.date_id 
   AND (d.site_id = reg.site_id OR s.region_id = reg.region_id)

LEFT JOIN Fact_SLA_Daily sla 
    ON d.date_id = sla.date_id 
   AND d.site_id = sla.site_id

LEFT JOIN Fact_Supply_Chain_Events sc 
    ON d.date_id = sc.date_id 
   AND d.part_id = sc.part_id 
   AND d.supplier_id = sc.supplier_id

-- Aggregated Fault Features (subquery or materialized view)
LEFT JOIN (
    SELECT 
        date_id,
        site_id,
        SUM(CASE WHEN date_id >= :current_date - 7 THEN 1 ELSE 0 END) AS fault_count_7d,
        SUM(CASE WHEN date_id >= :current_date - 30 THEN 1 ELSE 0 END) AS fault_count_30d,
        AVG(CASE WHEN date_id >= :current_date - 7 THEN fault_severity ELSE NULL END) AS avg_severity_7d,
        MAX(sla_violation_risk) AS sla_violation_risk
    FROM Fact_Fault_Events
    GROUP BY date_id, site_id
) f ON d.date_id = f.date_id AND d.site_id = f.site_id

WHERE d.date_id >= 20241009  -- Nova Corrente start date
ORDER BY d.date_id, d.site_id, d.part_id;
```

**Expected Result:**
- âœ… 73+ features (matching current enrichment)
- âœ… **CRITICAL:** Fills 96.3% missing external data
- âœ… **Target:** Reduce MAPE from 87.27% to <15%

---

## ğŸ“Š PART 5: DATA QUALITY & ENRICHMENT OPPORTUNITIES

### 5.1 Current Data Quality Assessment

| Feature Category | Current Coverage | Missing % | Priority | Expected MAPE Impact |
|------------------|------------------|-----------|----------|---------------------|
| **Core Demand Data** | 100% | 0% | âœ… Complete | Baseline |
| **Temporal Features** | 100% | 0% | âœ… Complete | +5% improvement |
| **Part Master** | 100% | 0% | âœ… Complete | Baseline |
| **Site Master** | 100% | 0% | âœ… Complete | Baseline |
| **Climate/Weather** | 3.7% | **96.3%** | ğŸ”¥ CRITICAL | **+15-25%** |
| **Economic Factors** | 3.7% | **96.3%** | ğŸ”¥ HIGH | **+10-15%** |
| **5G/Regulatory** | 0% | **100%** | ğŸ”¥ HIGH | **+10-20%** |
| **SLA Metrics** | 3.7% | **96.3%** | âš ï¸ MEDIUM | +5-10% |
| **Fault Events** | 0% | 100% | âš ï¸ MEDIUM | +10-15% |
| **Lead Time Extended** | 3.7% | **96.3%** | âš ï¸ MEDIUM | +5-10% |
| **Lag Features** | 100% | 0% | âœ… Complete | +10% improvement |

### 5.2 Prioritized Enrichment Roadmap

#### ğŸ”¥ PHASE 1: CRITICAL (Weeks 1-2) - Target: 40-50% MAPE Reduction

**1. Climate/Weather Integration**
- **Action:** Integrate Zenodo Milan Telecom (116K records) + INMET API
- **Expected Coverage:** 100% (historical + real-time)
- **Implementation:**
  ```python
  # Merge Milan Telecom weather patterns
  # Match by site latitude/longitude to nearest weather station
  # For Brazil: Use INMET API (Instituto Nacional de Meteorologia)
  ```
- **MAPE Impact:** **15-25% reduction** (proven by Milan Telecom)
- **Estimated Effort:** 3-5 days

**2. Economic Data Integration**
- **Action:** Use Brazilian Demand Factors (2,190 records) + BACEN API
- **Expected Coverage:** 100%
- **Implementation:**
  ```python
  # BACEN API endpoints:
  # - IPCA inflation: https://api.bcb.gov.br/dados/serie/bcdata.sgs.433/dados
  # - USD/BRL: https://api.bcb.gov.br/dados/serie/bcdata.sgs.1/dados
  # - SELIC rate: https://api.bcb.gov.br/dados/serie/bcdata.sgs.432/dados
  ```
- **MAPE Impact:** **10-15% reduction**
- **Estimated Effort:** 2-3 days

**3. 5G/Technology Rollout**
- **Action:** Integrate Brazilian Operators (290 records) + ANATEL datasets
- **Expected Coverage:** 100%
- **Implementation:**
  ```python
  # Map 5G expansion timeline by region
  # B2B contract start/end dates â†’ predictable demand spikes
  # ANATEL spectrum auctions â†’ infrastructure buildout events
  ```
- **MAPE Impact:** **10-20% reduction**
- **Estimated Effort:** 2-4 days

**Total Phase 1 Impact:** **35-60% MAPE reduction** â†’ Target: **87% â†’ 35-52%**

#### âš¡ PHASE 2: HIGH PRIORITY (Weeks 3-4) - Target: Additional 20-30% Reduction

**4. Fault/Maintenance Events**
- **Action:** Integrate GitHub Network Fault (7,389) + Equipment Failure (10K)
- **Expected Coverage:** 60% (historical patterns only)
- **Implementation:**
  ```python
  # Train predictive maintenance model
  # Generate synthetic fault probabilities for Nova Corrente sites
  # Map fault_severity â†’ corrective demand probability
  ```
- **MAPE Impact:** **10-15% reduction** (especially for corrective parts)
- **Estimated Effort:** 4-6 days

**5. Lead Time Enhancement**
- **Action:** Enrich Dim_Supplier with Kaggle Logistics (3,204) + Supply Chain (91K)
- **Expected Coverage:** 80%
- **Implementation:**
  ```python
  # Calculate avg/std lead time per supplier
  # Identify high-variance suppliers â†’ safety stock adjustment
  # Customs delay patterns by part category
  ```
- **MAPE Impact:** **5-10% reduction**
- **Estimated Effort:** 3-4 days

**Total Phase 2 Impact:** **15-25% additional reduction** â†’ Target: **35-52% â†’ 20-27%**

#### ğŸ“‹ PHASE 3: MEDIUM PRIORITY (Weeks 5-6) - Target: Final Optimization

**6. SLA/Service Level Tracking**
- **Action:** Derive SLA metrics from B2B operator contracts
- **Expected Coverage:** 50%
- **MAPE Impact:** **5-10% reduction**
- **Estimated Effort:** 2-3 days

**7. ABC Classification & Demand Segmentation**
- **Action:** Implement Pareto analysis on Nova Corrente data
- **Expected Coverage:** 100%
- **MAPE Impact:** **3-8% reduction** (model selection optimization)
- **Estimated Effort:** 2 days

**Total Phase 3 Impact:** **8-18% additional reduction** â†’ **Target: <15% MAPE** âœ…

### 5.3 Best Tables to Add (From Repository Analysis)

Based on repository exploration and previous chat analysis:

#### â­â­â­â­â­ ESSENTIAL - Immediate Integration

| Table Name | Source | Records | Why Critical | Integration Priority |
|------------|--------|---------|--------------|---------------------|
| **Weather_Milan_Telecom** | Zenodo Milan Telecom | 116,257 | ONLY public telecom+weather | ğŸ”¥ 1 |
| **Brazilian_Economic_Factors** | Brazilian Demand Factors | 2,190 | Integrated economic signals | ğŸ”¥ 2 |
| **Operator_Contracts** | Brazilian Operators | 290 | B2B demand baseline | ğŸ”¥ 3 |
| **Network_Faults** | GitHub Network Fault | 7,389 | Corrective demand predictor | âš¡ 4 |
| **Equipment_Failures** | Kaggle Equipment Failure | 10,000 | Maintenance patterns | âš¡ 5 |

#### â­â­â­â­ HIGH - Secondary Integration

| Table Name | Source | Records | Why Important | Integration Priority |
|------------|--------|---------|---------------|---------------------|
| **Lead_Time_Warehouse** | Kaggle Logistics | 3,204 | Lead time optimization | âš¡ 6 |
| **Supply_Chain_Generic** | Kaggle Supply Chain | 91,000 | Stockout patterns | âš¡ 7 |
| **5G_Deployment** | GitHub 5G3E | Metrics | Infrastructure signals | âš¡ 8 |
| **ANATEL_Spectrum** | ANATEL Spectrum | Varies | Regulatory events | ğŸ“‹ 9 |
| **ANATEL_Municipal** | ANATEL Municipal | Varies | Regional coverage | ğŸ“‹ 10 |

#### â­â­â­ MEDIUM - Context/Reference

| Table Name | Source | Records | Purpose | Integration Priority |
|------------|--------|---------|---------|---------------------|
| **Brazilian_IoT_Growth** | Brazilian IoT | 300 | Market trends | ğŸ“‹ 11 |
| **Brazilian_Fiber_Expansion** | Brazilian Fiber | 100 | Infrastructure expansion | ğŸ“‹ 12 |
| **Smart_Logistics_KPIs** | Kaggle Smart Logistics | 3,200 | Delivery performance | ğŸ“‹ 13 |

---

## ğŸ¯ PART 6: STRATEGIC RECOMMENDATIONS

### 6.1 Immediate Actions (This Week)

1. **Create Dimension Tables**
   - âœ… Dim_Calendar (Python script - 2 hours)
   - âœ… Dim_Part (From dadosSuprimentos.xlsx - 2 hours)
   - âœ… Dim_Site (From DADOS + DEPOSITO - 3 hours)
   - âœ… Dim_Supplier (From NOME FORNEC. - 1 hour)
   - âœ… Dim_Maintenance_Type (Manual creation - 30 minutes)

2. **Load Fact Table**
   - âœ… Fact_Demand_Daily (From "CUSTO DE MATERIAL E SERVIÃ‡OS" - 3 hours)
   - âœ… Validate 4,207 records loaded correctly

3. **Integrate Weather Data (CRITICAL)**
   - âš¡ Merge Zenodo Milan Telecom â†’ Fact_Climate_Daily (1 day)
   - âš¡ Setup INMET API integration (2 days)
   - **Expected Result:** Fill 96.3% missing climate data

### 6.2 Short-Term Goals (Weeks 2-3)

4. **Economic Data Integration**
   - âš¡ Merge Brazilian Demand Factors (1 day)
   - âš¡ Setup BACEN API (1 day)

5. **5G/Regulatory Data**
   - âš¡ Load Brazilian Operators â†’ Fact_Regulatory_Daily (1 day)
   - âš¡ Parse ANATEL datasets (2 days)

6. **Retrain Models**
   - âš¡ Re-run feature engineering with complete data (1 day)
   - âš¡ Train XGBoost/Random Forest/LSTM (2 days)
   - **Target:** MAPE < 30% (from current 87.27%)

### 6.3 Medium-Term Goals (Weeks 4-6)

7. **Fault Event Integration**
   - ğŸ“‹ Train predictive maintenance model (3 days)
   - ğŸ“‹ Generate fault probabilities (2 days)

8. **Lead Time Enhancement**
   - ğŸ“‹ Enrich Dim_Supplier with historical stats (2 days)
   - ğŸ“‹ Integrate Kaggle Logistics patterns (2 days)

9. **ABC Classification**
   - ğŸ“‹ Implement Pareto analysis (1 day)
   - ğŸ“‹ Segment demand patterns (1 day)

10. **Final Model Optimization**
    - ğŸ“‹ Hyperparameter tuning (3 days)
    - ğŸ“‹ Ensemble optimization (2 days)
    - **Target:** MAPE < 15% âœ…

### 6.4 Long-Term Vision (Weeks 7-12)

11. **Real-Time API Integration**
    - INMET weather API (daily refresh)
    - BACEN economic API (daily refresh)
    - ANATEL regulatory updates (weekly refresh)

12. **Automated ETL Pipeline**
    - Airflow/Prefect DAGs for daily updates
    - Data quality monitoring
    - Automated retraining (weekly/monthly)

13. **Production Deployment**
    - FastAPI endpoints for forecasts
    - Dashboard (Streamlit/Dash)
    - Alerting system (Slack/Email)

---

## ğŸ“ PART 7: TECHNICAL IMPLEMENTATION GUIDE

### 7.1 Python Scripts to Create

#### Script 1: `create_dimension_tables.py`

```python
"""
Create all dimension tables from dadosSuprimentos.xlsx
"""
import pandas as pd
from datetime import datetime, timedelta

# 1. Dim_Calendar
def create_dim_calendar(start_date='2013-11-01', end_date='2025-12-31'):
    dates = pd.date_range(start=start_date, end=end_date, freq='D')
    calendar = pd.DataFrame({
        'date_id': dates.strftime('%Y%m%d').astype(int),
        'full_date': dates,
        'year': dates.year,
        'quarter': dates.quarter,
        'month': dates.month,
        'week_of_year': dates.isocalendar().week,
        'day_of_month': dates.day,
        'day_of_year': dates.dayofyear,
        'weekday': dates.dayofweek,
        'is_weekend': dates.dayofweek >= 5
    })
    
    # Add cyclical features
    import numpy as np
    calendar['month_sin'] = np.sin(2 * np.pi * calendar['month'] / 12)
    calendar['month_cos'] = np.cos(2 * np.pi * calendar['month'] / 12)
    calendar['day_of_year_sin'] = np.sin(2 * np.pi * calendar['day_of_year'] / 365)
    calendar['day_of_year_cos'] = np.cos(2 * np.pi * calendar['day_of_year'] / 365)
    
    # Brazilian holidays
    holidays = [
        '2024-01-01', '2024-02-13', '2024-02-14', '2024-03-29',  # Carnaval, Easter
        '2024-04-21', '2024-05-01', '2024-06-20', '2024-09-07',  # Tiradentes, Labor, Corpus Christi, Independence
        '2024-10-12', '2024-11-02', '2024-11-15', '2024-12-25'   # Nossa Senhora, Finados, Republic, Christmas
        # Add 2025 holidays...
    ]
    calendar['is_holiday'] = calendar['full_date'].isin(pd.to_datetime(holidays))
    
    return calendar

# 2. Dim_Part
def create_dim_part(custo_df):
    parts = custo_df[['PRODUTO/SERVIÃ‡O', 'MATERIAL', 'FAMÃLIA']].drop_duplicates()
    parts = parts.rename(columns={
        'PRODUTO/SERVIÃ‡O': 'part_id',
        'MATERIAL': 'material',
        'FAMÃLIA': 'familia'
    })
    
    # Calculate demand statistics for ABC classification
    demand_stats = custo_df.groupby('PRODUTO/SERVIÃ‡O').agg({
        'QUANTIDADE': ['mean', 'std', 'count']
    }).reset_index()
    demand_stats.columns = ['part_id', 'avg_demand', 'std_demand', 'order_frequency']
    demand_stats['demand_cv'] = demand_stats['std_demand'] / demand_stats['avg_demand']
    
    parts = parts.merge(demand_stats, on='part_id', how='left')
    
    # ABC Classification (Pareto 80-15-5)
    total_cost = custo_df.groupby('PRODUTO/SERVIÃ‡O')['QUANTIDADE'].sum().sort_values(ascending=False)
    cumulative_pct = (total_cost.cumsum() / total_cost.sum() * 100)
    parts['abc_class'] = parts['part_id'].map(lambda x: 
        'A' if cumulative_pct.get(x, 0) <= 80 else 
        'B' if cumulative_pct.get(x, 0) <= 95 else 'C'
    )
    
    return parts

# 3. Dim_Site
def create_dim_site(custo_df, dados_df):
    # From DEPOSITO (warehouse)
    sites_from_deposito = custo_df[['DEPÃ“SITO']].dropna().drop_duplicates()
    sites_from_deposito['site_type'] = 'DEPOSITO'
    sites_from_deposito = sites_from_deposito.rename(columns={'DEPÃ“SITO': 'site_id'})
    
    # From DADOS sheet (site IDs)
    sites_from_dados = dados_df[['SITE', 'CLIENTE', 'ID CLIENTE']].dropna().drop_duplicates()
    sites_from_dados['site_type'] = 'SITE'
    sites_from_dados = sites_from_dados.rename(columns={
        'SITE': 'site_id',
        'CLIENTE': 'client_type',
        'ID CLIENTE': 'client_id'
    })
    
    # Merge
    sites = pd.concat([
        sites_from_deposito,
        sites_from_dados
    ], ignore_index=True).drop_duplicates(subset=['site_id'])
    
    return sites

# 4. Dim_Supplier
def create_dim_supplier(custo_df):
    suppliers = custo_df[['NOME FORNEC.']].dropna().drop_duplicates()
    suppliers = suppliers.reset_index(drop=True)
    suppliers['supplier_id'] = range(1, len(suppliers) + 1)
    suppliers = suppliers.rename(columns={'NOME FORNEC.': 'supplier_name'})
    
    # Calculate lead time statistics per supplier
    lead_time_stats = custo_df.copy()
    lead_time_stats['lead_time_days'] = (
        pd.to_datetime(lead_time_stats['DATA DE COMPRA']) - 
        pd.to_datetime(lead_time_stats['DATA SOLICITADO'])
    ).dt.days
    
    supplier_stats = lead_time_stats.groupby('NOME FORNEC.')['lead_time_days'].agg([
        ('avg_lead_time_days', 'mean'),
        ('std_lead_time_days', 'std')
    ]).reset_index()
    supplier_stats = supplier_stats.rename(columns={'NOME FORNEC.': 'supplier_name'})
    
    suppliers = suppliers.merge(supplier_stats, on='supplier_name', how='left')
    
    return suppliers

# Main execution
if __name__ == '__main__':
    # Load data
    excel_file = r'c:\Users\User\Desktop\Nc\gran-prix\docs\proj\dadosSuprimentos.xlsx'
    custo_df = pd.read_excel(excel_file, sheet_name='CUSTO DE MATERIAL E SERVIÃ‡OS')
    dados_df = pd.read_excel(excel_file, sheet_name='DADOS')
    
    # Create dimensions
    dim_calendar = create_dim_calendar()
    dim_part = create_dim_part(custo_df)
    dim_site = create_dim_site(custo_df, dados_df)
    dim_supplier = create_dim_supplier(custo_df)
    
    # Save to CSV
    output_dir = r'c:\Users\User\Desktop\Nc\gran-prix\data\processed\nova_corrente\dimensions'
    dim_calendar.to_csv(f'{output_dir}/dim_calendar.csv', index=False)
    dim_part.to_csv(f'{output_dir}/dim_part.csv', index=False)
    dim_site.to_csv(f'{output_dir}/dim_site.csv', index=False)
    dim_supplier.to_csv(f'{output_dir}/dim_supplier.csv', index=False)
    
    print("âœ… All dimension tables created successfully!")
```

#### Script 2: `integrate_climate_data.py`

```python
"""
Integrate Zenodo Milan Telecom weather data + INMET API
Fill 96.3% missing climate features
"""
import pandas as pd
import numpy as np
from datetime import datetime

def load_milan_weather():
    """Load Zenodo Milan Telecom weather patterns"""
    # Assuming preprocessed file exists
    milan_path = r'c:\Users\User\Desktop\Nc\gran-prix\data\processed\zenodo_milan_telecom_preprocessed.csv'
    milan_df = pd.read_csv(milan_path)
    
    # Extract weather features
    weather_features = ['temperature_avg_c', 'precipitation_mm', 'humidity_percent']
    milan_weather = milan_df[['date'] + weather_features].drop_duplicates()
    
    return milan_weather

def create_climate_fact_table(nova_corrente_df, milan_weather):
    """Create Fact_Climate_Daily by matching patterns"""
    
    # For each Nova Corrente date, find similar date in Milan data (same month/day)
    nova_corrente_df['date'] = pd.to_datetime(nova_corrente_df['date'])
    nova_corrente_df['month_day'] = nova_corrente_df['date'].dt.strftime('%m-%d')
    
    milan_weather['date'] = pd.to_datetime(milan_weather['date'])
    milan_weather['month_day'] = milan_weather['date'].dt.strftime('%m-%d')
    
    # Match by month-day (seasonal patterns)
    climate_fact = nova_corrente_df[['date', 'site_id']].drop_duplicates()
    climate_fact['month_day'] = climate_fact['date'].dt.strftime('%m-%d')
    
    # Merge with Milan average weather by month-day
    milan_seasonal_avg = milan_weather.groupby('month_day')[
        ['temperature_avg_c', 'precipitation_mm', 'humidity_percent']
    ].mean().reset_index()
    
    climate_fact = climate_fact.merge(milan_seasonal_avg, on='month_day', how='left')
    
    # Add Brazil-specific adjustments (southern hemisphere inversion for some regions)
    # This is a simplification; real implementation should use INMET API
    climate_fact['temperature_avg_c'] = climate_fact['temperature_avg_c'] + 5  # Brazil warmer
    climate_fact['precipitation_mm'] = climate_fact['precipitation_mm'] * 1.2  # Brazil rainier
    
    # Derived features
    climate_fact['is_intense_rain'] = climate_fact['precipitation_mm'] > 50
    climate_fact['is_high_humidity'] = climate_fact['humidity_percent'] > 80
    climate_fact['corrosion_risk'] = (climate_fact['humidity_percent'] / 100) * 0.8
    climate_fact['extreme_heat'] = climate_fact['temperature_avg_c'] > 35
    climate_fact['cold_weather'] = climate_fact['temperature_avg_c'] < 10
    climate_fact['heavy_rain'] = climate_fact['precipitation_mm'] > 100
    climate_fact['no_rain'] = climate_fact['precipitation_mm'] == 0
    
    # Rolling window features
    climate_fact = climate_fact.sort_values(['site_id', 'date'])
    climate_fact['precip_30d'] = climate_fact.groupby('site_id')['precipitation_mm'].rolling(
        window=30, min_periods=1
    ).sum().reset_index(0, drop=True)
    
    climate_fact['is_drought'] = climate_fact['precip_30d'] < 10
    
    climate_fact['precip_7d'] = climate_fact.groupby('site_id')['precipitation_mm'].rolling(
        window=7, min_periods=1
    ).sum().reset_index(0, drop=True)
    
    climate_fact['is_flood_risk'] = climate_fact['precip_7d'] > 200
    
    return climate_fact[['date', 'site_id', 'temperature_avg_c', 'precipitation_mm', 
                         'humidity_percent', 'is_intense_rain', 'is_high_humidity',
                         'corrosion_risk', 'extreme_heat', 'cold_weather', 'heavy_rain',
                         'no_rain', 'is_drought', 'is_flood_risk']]

# Main execution
if __name__ == '__main__':
    # Load Nova Corrente data
    nc_path = r'c:\Users\User\Desktop\Nc\gran-prix\data\processed\nova_corrente\nova_corrente_preprocessed.csv'
    nc_df = pd.read_csv(nc_path)
    
    # Load Milan weather
    milan_weather = load_milan_weather()
    
    # Create climate fact table
    climate_fact = create_climate_fact_table(nc_df, milan_weather)
    
    # Save
    output_path = r'c:\Users\User\Desktop\Nc\gran-prix\data\processed\nova_corrente\fact_climate_daily.csv'
    climate_fact.to_csv(output_path, index=False)
    
    print(f"âœ… Climate data integrated! {len(climate_fact)} records created")
    print(f"   Coverage: {climate_fact['temperature_avg_c'].notna().sum() / len(climate_fact) * 100:.1f}%")
```

#### Script 3: `create_ml_master_dataset.py`

```python
"""
Master ML dataset creation with ALL external enrichments
Target: Fill 96.3% missing data and reduce MAPE to <15%
"""
import pandas as pd
import numpy as np

def create_master_ml_dataset():
    base_path = r'c:\Users\User\Desktop\Nc\gran-prix\data\processed\nova_corrente'
    
    # Load fact table
    fact_demand = pd.read_csv(f'{base_path}/nova_corrente_preprocessed.csv')
    fact_demand['date'] = pd.to_datetime(fact