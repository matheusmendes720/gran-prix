#!/usr/bin/env python3
"""
üîß NOVA CORRENTE - PIPELINE HARDENING & DEPENDENCY FIXES
Complete production-ready pipeline with optional dependency handling
"""

import pandas as pd
import numpy as np
import json
import logging
from pathlib import Path
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# ML imports
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import joblib

# Optional imports with guards
try:
    import prophet
    PROPHET_AVAILABLE = True
    logger = logging.getLogger(__name__)
    logger.info("‚úÖ Prophet available")
except ImportError:
    PROPHET_AVAILABLE = False
    prophet = None
    logger = logging.getLogger(__name__)
    logger.warning("‚ö†Ô∏è Prophet not available - using fallback models")

try:
    import cmdstanpy
    CMDSTAN_AVAILABLE = True
    logger.info("‚úÖ CmdStan available")
except ImportError:
    CMDSTAN_AVAILABLE = False
    logger.warning("‚ö†Ô∏è CmdStan not available - Prophet will be disabled")

try:
    import pmdarima
    ARIMA_AVAILABLE = True
    logger.info("‚úÖ ARIMA available")
except ImportError:
    ARIMA_AVAILABLE = False
    logger.warning("‚ö†Ô∏è ARIMA not available - using fallback models")

try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
    logger.info("‚úÖ XGBoost available")
except ImportError:
    XGBOOST_AVAILABLE = False
    xgb = None
    logger.warning("‚ö†Ô∏è XGBoost not available - using fallback models")

try:
    import tensorflow as tf
    TENSORFLOW_AVAILABLE = True
    logger.info("‚úÖ TensorFlow available")
except ImportError:
    TENSORFLOW_AVAILABLE = False
    tf = None
    logger.warning("‚ö†Ô∏è TensorFlow not available - LSTM models disabled")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class NovaCorrenteHardenedPipeline:
    """
    Production-ready ML Pipeline with optional dependency management
    """
    
    def __init__(self, enable_prophet=False):
        self.enable_prophet = enable_prophet and PROPHET_AVAILABLE and CMDSTAN_AVAILABLE
        self.output_dir = Path('data/outputs/nova_corrente')
        self.models_dir = Path('models/nova_corrente')
        self.reports_dir = Path('docs/reports')
        
        # Ensure directories exist
        for dir_path in [self.output_dir, self.models_dir, self.reports_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        # Log availability status
        logger.info(f"üîß Pipeline Configuration:")
        logger.info(f"  Prophet Available: {PROPHET_AVAILABLE}")
        logger.info(f"  CmdStan Available: {CMDSTAN_AVAILABLE}")
        logger.info(f"  Prophet Enabled: {self.enable_prophet}")
        logger.info(f"  ARIMA Available: {ARIMA_AVAILABLE}")
        logger.info(f"  XGBoost Available: {XGBOOST_AVAILABLE}")
        logger.info(f"  TensorFlow Available: {TENSORFLOW_AVAILABLE}")
    
    def load_production_data(self):
        """Load production-ready data"""
        logger.info("üìä Loading Production Data...")
        
        # Try to load comprehensive data
        try:
            enriched_path = self.output_dir / 'nova_corrente_enriched.csv'
            if enriched_path.exists():
                df = pd.read_csv(enriched_path)
                logger.info(f"‚úÖ Loaded enriched data: {len(df)} records, {len(df.columns)} columns")
            else:
                logger.warning("Enriched data not found, creating synthetic production data")
                df = self.create_production_synthetic_data()
        except Exception as e:
            logger.error(f"Error loading data: {e}")
            df = self.create_production_synthetic_data()
        
        return df
    
    def create_production_synthetic_data(self):
        """Create high-quality synthetic production data"""
        logger.info("üé≤ Creating Production Synthetic Data...")
        
        dates = pd.date_range('2023-01-01', '2024-12-31', freq='D')
        families = ['EPI', 'FERRAMENTAS_E_EQUIPAMENTOS', 'FERRO_E_A√áO', 'MATERIAL_CIVIL', 'MATERIAL_ELETRICO']
        suppliers = ['Supplier_A', 'Supplier_B', 'Supplier_C', 'Supplier_D', 'Supplier_E']
        sites = ['Site_001', 'Site_002', 'Site_003', 'Site_004', 'Site_005']
        
        data = []
        
        for date in dates:
            for family in families:
                # Family-specific base demand patterns
                if family == 'EPI':
                    base_demand = 80 + 30 * np.sin(2 * np.pi * date.dayofyear / 365)
                    volatility = 0.3
                elif family == 'FERRAMENTAS_E_EQUIPAMENTOS':
                    base_demand = 120 + 40 * np.cos(2 * np.pi * date.dayofyear / 365)
                    volatility = 0.25
                elif family == 'FERRO_E_A√áO':
                    base_demand = 150 + 60 * np.sin(2 * np.pi * (date.dayofyear - 90) / 365)
                    volatility = 0.35
                elif family == 'MATERIAL_CIVIL':
                    base_demand = 60 + 20 * np.cos(2 * np.pi * date.dayofyear / 180)
                    volatility = 0.2
                else:  # MATERIAL_ELETRICO
                    base_demand = 90 + 25 * np.sin(2 * np.pi * (date.dayofyear + 45) / 365)
                    volatility = 0.28
                
                # Add external factors impact
                weather_factor = 1 + 0.1 * np.sin(2 * np.pi * date.dayofyear / 365) + np.random.normal(0, 0.05)
                economic_factor = 1 + 0.05 * np.sin(2 * np.pi * date.month / 12) + np.random.normal(0, 0.03)
                seasonal_factor = 1 + 0.2 * np.sin(2 * np.pi * date.dayofyear / 365.25)
                
                # Calculate final demand
                daily_demand = base_demand * weather_factor * economic_factor * seasonal_factor
                daily_demand = max(10, daily_demand + np.random.normal(0, base_demand * volatility))
                
                # Lead time based on family and external conditions
                base_lead_time = {'EPI': 7, 'FERRAMENTAS_E_EQUIPAMENTOS': 10, 'FERRO_E_A√áO': 15, 'MATERIAL_CIVIL': 12, 'MATERIAL_ELETRICO': 8}[family]
                lead_time = max(1, base_lead_time + np.random.exponential(3) + np.random.normal(0, 2))
                
                # Create record
                data.append({
                    'date': date,
                    'familia': family,
                    'material': f'Material_{family}_{date.strftime("%Y%m%d")}',
                    'quantidade': daily_demand,
                    'lead_time_days': lead_time,
                    'fornecedor': np.random.choice(suppliers),
                    'site_id': np.random.choice(sites),
                    'deposito': 'Main',
                    'year': date.year,
                    'month': date.month,
                    'day': date.day,
                    'dayofweek': date.dayofweek,
                    'quarter': date.quarter,
                    'is_weekend': date.dayofweek >= 5,
                    
                    # External factors
                    'temperature_avg_c': 20 + 15 * np.sin(2 * np.pi * date.dayofyear / 365) + np.random.normal(0, 5),
                    'precipitation_mm': max(0, np.random.exponential(8) * weather_factor),
                    'humidity_percent': 60 + 25 * np.random.random(),
                    'is_intense_rain': np.random.choice([True, False], p=[0.08, 0.92]),
                    'inflation_rate': 4 + 2 * np.sin(2 * np.pi * date.month / 12) + np.random.normal(0, 0.5),
                    'exchange_rate_brl_usd': 4.8 + 0.7 * np.sin(2 * np.pi * date.dayofyear / 365) + np.random.normal(0, 0.1),
                    'gdp_growth_rate': 2.5 + 1.5 * np.sin(2 * np.pi * date.dayofyear / 365) + np.random.normal(0, 0.3),
                    '5g_coverage_pct': 30 + 40 * (date - pd.Timestamp('2023-01-01')).days / 730 + np.random.normal(0, 5),
                    'sla_penalty_brl': np.random.uniform(0, 500000),
                    'availability_target': 0.99 + np.random.uniform(-0.01, 0),
                    'downtime_hours_monthly': max(0, np.random.exponential(5)),
                    'sla_violation_risk': np.random.choice(['high', 'medium', 'low'], p=[0.15, 0.35, 0.5]),
                    'corrosion_risk': np.random.choice(['high', 'medium', 'low'], p=[0.2, 0.3, 0.5])
                })
        
        return pd.DataFrame(data)
    
    def advanced_feature_engineering(self, df):
        """Production-ready feature engineering"""
        logger.info("‚öôÔ∏è Advanced Feature Engineering...")
        
        # Ensure date is datetime
        df['date'] = pd.to_datetime(df['date'])
        df = df.sort_values('date').reset_index(drop=True)
        
        # Temporal features
        df['year'] = df['date'].dt.year
        df['month'] = df['date'].dt.month
        df['day'] = df['date'].dt.day
        df['quarter'] = df['date'].dt.quarter
        df['dayofweek'] = df['date'].dt.dayofweek
        df['day_of_year'] = df['date'].dt.dayofyear
        df['is_weekend'] = df['dayofweek'] >= 5
        df['is_month_start'] = df['day'] <= 5
        df['is_month_end'] = df['day'] >= 25
        df['is_quarter_start'] = df['month'].isin([1, 4, 7, 10]) & (df['day'] <= 7)
        df['is_quarter_end'] = df['month'].isin([3, 6, 9, 12]) & (df['day'] >= 25)
        
        # Seasonal features
        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
        df['day_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365.25)
        df['day_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365.25)
        df['quarter_sin'] = np.sin(2 * np.pi * df['quarter'] / 4)
        df['quarter_cos'] = np.cos(2 * np.pi * df['quarter'] / 4)
        
        # Lag features by family
        for lag in [1, 3, 7, 14, 30]:
            df[f'demand_lag_{lag}'] = df.groupby('familia')['quantidade'].shift(lag)
        
        # Rolling window features
        for window in [3, 7, 14, 30, 60]:
            df[f'demand_mean_{window}'] = df.groupby('familia')['quantidade'].transform(
                lambda x: x.rolling(window, min_periods=1).mean()
            )
            df[f'demand_std_{window}'] = df.groupby('familia')['quantidade'].transform(
                lambda x: x.rolling(window, min_periods=1).std().fillna(0)
            )
            df[f'demand_median_{window}'] = df.groupby('familia')['quantidade'].transform(
                lambda x: x.rolling(window, min_periods=1).median()
            )
            df[f'demand_min_{window}'] = df.groupby('familia')['quantidade'].transform(
                lambda x: x.rolling(window, min_periods=1).min()
            )
            df[f'demand_max_{window}'] = df.groupby('familia')['quantidade'].transform(
                lambda x: x.rolling(window, min_periods=1).max()
            )
        
        # Lead time features
        for window in [7, 30]:
            df[f'lead_time_mean_{window}'] = df.groupby('familia')['lead_time_days'].transform(
                lambda x: x.rolling(window, min_periods=1).mean()
            )
            df[f'lead_time_std_{window}'] = df.groupby('familia')['lead_time_days'].transform(
                lambda x: x.rolling(window, min_periods=1).std().fillna(0)
            )
        
        # External interaction features
        df['weather_demand_interaction'] = df['temperature_avg_c'] * df['demand_mean_7']
        df['economic_pressure_index'] = df['inflation_rate'] * df['exchange_rate_brl_usd']
        df['5g_demand_correlation'] = df['5g_coverage_pct'] * df['demand_mean_30']
        df['sla_risk_multiplier'] = df['sla_penalty_brl'] * (df['sla_violation_risk'] == 'high').astype(int)
        
        # Risk features
        df['supply_risk_index'] = (
            (df['lead_time_days'] > 15).astype(int) +
            (df['sla_violation_risk'] == 'high').astype(int) +
            (df['corrosion_risk'] == 'high').astype(int) +
            (df['is_intense_rain']).astype(int)
        )
        
        # Seasonal risk features
        df['seasonal_risk_high'] = df['month'].isin([12, 1, 2]).astype(int)  # Summer in Brazil
        df['seasonal_risk_medium'] = df['month'].isin([6, 7, 8]).astype(int)  # Winter
        
        # One-hot encoding
        df = pd.get_dummies(df, columns=['familia'], prefix='family')
        df = pd.get_dummies(df, columns=['sla_violation_risk'], prefix='sla_risk')
        df = pd.get_dummies(df, columns=['corrosion_risk'], prefix='corrosion')
        
        logger.info(f"‚úÖ Feature Engineering Complete: {df.shape}")
        return df
    
    def train_robust_models(self, df):
        """Train robust models with optional dependency handling"""
        logger.info("ü§ñ Training Robust Models...")
        
        # Prepare features and target
        target_col = 'quantidade'
        
        # Remove rows with missing target
        df_clean = df.dropna(subset=[target_col])
        df_clean = df_clean[(df_clean[target_col] > 0) & (df_clean[target_col] < 10000)]
        
        # Select numeric features
        exclude_cols = ['date', 'material', 'fornecedor', 'site_id', 'deposito']
        numeric_cols = df_clean.select_dtypes(include=[np.number]).columns
        feature_cols = [col for col in numeric_cols if col != target_col and col not in exclude_cols]
        
        X = df_clean[feature_cols].fillna(0)
        y = df_clean[target_col]
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=None
        )
        
        # Scale features
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        models = {}
        
        # Always include RandomForest
        models['RandomForest'] = RandomForestRegressor(
            n_estimators=200, max_depth=15, random_state=42, n_jobs=-1
        )
        
        # Always include GradientBoosting
        models['GradientBoosting'] = GradientBoostingRegressor(
            n_estimators=200, learning_rate=0.05, max_depth=8, random_state=42
        )
        
        # Include XGBoost if available
        if XGBOOST_AVAILABLE:
            models['XGBoost'] = xgb.XGBRegressor(
                n_estimators=200, learning_rate=0.05, max_depth=8, random_state=42, n_jobs=-1
            )
        
        # Include Prophet if enabled and available
        if self.enable_prophet and PROPHET_AVAILABLE:
            logger.info("üîÆ Adding Prophet model...")
            models['Prophet'] = self._train_prophet_model(df_clean)
        
        # Train and evaluate models
        results = {}
        
        for model_name, model in models.items():
            logger.info(f"Training {model_name}...")
            
            if model_name == 'Prophet':
                # Prophet is handled separately
                results[model_name] = model
                continue
            
            # Train model
            model.fit(X_train_scaled, y_train)
            
            # Predictions
            y_pred = model.predict(X_test_scaled)
            
            # Metrics
            mae = mean_absolute_error(y_test, y_pred)
            rmse = np.sqrt(mean_squared_error(y_test, y_pred))
            r2 = r2_score(y_test, y_pred)
            mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
            
            # Feature importance
            if hasattr(model, 'feature_importances_'):
                feature_importance = dict(zip(feature_cols, model.feature_importances_))
                top_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:15]
            else:
                top_features = []
            
            results[model_name] = {
                'model': model,
                'scaler': scaler,
                'features': feature_cols,
                'metrics': {
                    'mae': mae,
                    'rmse': rmse,
                    'r2': r2,
                    'mape': mape
                },
                'top_features': top_features,
                'test_predictions': y_pred,
                'test_actual': y_test
            }
            
            logger.info(f"  {model_name}: MAE={mae:.2f}, RMSE={rmse:.2f}, R¬≤={r2:.3f}, MAPE={mape:.1f}%")
        
        return results
    
    def _train_prophet_model(self, df):
        """Train Prophet model if available"""
        try:
            # Prepare data for Prophet
            prophet_df = df[['date', 'quantidade']].copy()
            prophet_df.columns = ['ds', 'y']
            
            # Add regressors
            regressors = ['temperature_avg_c', 'inflation_rate', '5g_coverage_pct', 'lead_time_days']
            for regressor in regressors:
                if regressor in df.columns:
                    prophet_df[regressor] = df[regressor]
            
            # Train Prophet
            prophet_model = prophet.Prophet(
                yearly_seasonality=True,
                weekly_seasonality=True,
                daily_seasonality=False,
                changepoint_prior_scale=0.05,
                seasonality_prior_scale=10.0,
                holidays_prior_scale=10.0,
                mcmc_samples=0,
                interval_width=0.8,
                uncertainty_samples=1000
            )
            
            # Add regressors
            for regressor in regressors:
                if regressor in prophet_df.columns:
                    prophet_model.add_regressor(regressor)
            
            # Fit model
            prophet_model.fit(prophet_df)
            
            logger.info("‚úÖ Prophet model trained successfully")
            return {
                'model': prophet_model,
                'type': 'Prophet',
                'regressors': regressors,
                'trained': True
            }
            
        except Exception as e:
            logger.error(f"‚ùå Prophet training failed: {e}")
            return {
                'model': None,
                'type': 'Prophet',
                'trained': False,
                'error': str(e)
            }
    
    def generate_production_forecasts(self, models, df):
        """Generate production-ready forecasts"""
        logger.info("üìà Generating Production Forecasts...")
        
        forecasts = {}
        
        # Get best non-Prophet model
        best_model_name = None
        best_mae = float('inf')
        
        for model_name, model_data in models.items():
            if model_name != 'Prophet' and 'metrics' in model_data:
                if model_data['metrics']['mae'] < best_mae:
                    best_mae = model_data['metrics']['mae']
                    best_model_name = model_name
        
        if best_model_name:
            best_model = models[best_model_name]['model']
            scaler = models[best_model_name]['scaler']
            features = models[best_model_name]['features']
            
            # Generate future dates (30-day forecast)
            last_date = pd.to_datetime(df['date']).max()
            future_dates = pd.date_range(
                start=last_date + timedelta(days=1),
                periods=30,
                freq='D'
            )
            
            # Create future dataframe
            future_df = pd.DataFrame({'date': future_dates})
            
            # Add temporal features
            future_df['year'] = future_df['date'].dt.year
            future_df['month'] = future_df['date'].dt.month
            future_df['day'] = future_df['date'].dt.day
            future_df['quarter'] = future_df['date'].dt.quarter
            future_df['dayofweek'] = future_df['date'].dt.dayofweek
            future_df['day_of_year'] = future_df['date'].dt.dayofyear
            future_df['is_weekend'] = future_df['dayofweek'] >= 5
            
            # Add seasonal features
            future_df['month_sin'] = np.sin(2 * np.pi * future_df['month'] / 12)
            future_df['month_cos'] = np.cos(2 * np.pi * future_df['month'] / 12)
            future_df['day_sin'] = np.sin(2 * np.pi * future_df['day_of_year'] / 365.25)
            future_df['day_cos'] = np.cos(2 * np.pi * future_df['day_of_year'] / 365.25)
            
            # Add external factors (synthetic future values)
            future_df['temperature_avg_c'] = 20 + 15 * np.sin(2 * np.pi * future_df['day_of_year'] / 365)
            future_df['inflation_rate'] = 4 + 2 * np.sin(2 * np.pi * future_df['month'] / 12)
            future_df['5g_coverage_pct'] = 70 + 10 * np.sin(2 * np.pi * future_df['day_of_year'] / 365)
            future_df['lead_time_days'] = 10 + np.random.exponential(3, len(future_df))
            
            # One-hot encode families (use average patterns)
            families = ['EPI', 'FERRAMENTAS_E_EQUIPAMENTOS', 'FERRO_E_A√áO', 'MATERIAL_CIVIL', 'MATERIAL_ELETRICO']
            for family in families:
                future_df[f'family_{family}'] = 0  # Will be set per family
            
            # Generate forecasts per family
            for family in families:
                family_future_df = future_df.copy()
                family_future_df[f'family_{family}'] = 1
                
                # Add lag features (use historical means)
                family_history = df[df['familia'] == family]
                if len(family_history) > 0:
                    for lag in [1, 3, 7, 14, 30]:
                        family_future_df[f'demand_lag_{lag}'] = family_history['quantidade'].tail(lag).mean()
                    
                    for window in [3, 7, 14, 30]:
                        family_future_df[f'demand_mean_{window}'] = family_history['quantidade'].tail(window).mean()
                        family_future_df[f'demand_std_{window}'] = family_history['quantidade'].tail(window).std()
                
                # Select available features
                available_features = [col for col in features if col in family_future_df.columns]
                X_future = family_future_df[available_features].fillna(0)
                
                # Scale and predict
                X_future_scaled = scaler.transform(X_future)
                family_forecasts = best_model.predict(X_future_scaled)
                
                # Ensure positive forecasts
                family_forecasts = np.maximum(family_forecasts, 1)
                
                forecasts[family] = {
                    'dates': future_dates.strftime('%Y-%m-%d').tolist(),
                    'forecast': family_forecasts.tolist(),
                    'confidence_interval': self._calculate_confidence_interval(family_forecasts),
                    'model_used': best_model_name
                }
        
        return forecasts
    
    def _calculate_confidence_interval(self, forecasts, confidence=0.95):
        """Calculate confidence intervals for forecasts"""
        z_score = 1.96 if confidence == 0.95 else 1.645
        std_dev = np.std(forecasts)
        mean_forecast = np.mean(forecasts)
        
        lower_bound = mean_forecast - z_score * std_dev
        upper_bound = mean_forecast + z_score * std_dev
        
        return {
            'lower': lower_bound,
            'upper': upper_bound,
            'confidence_level': confidence
        }
    
    def save_production_artifacts(self, models, forecasts):
        """Save all production artifacts"""
        logger.info("üíæ Saving Production Artifacts...")
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # Save models
        for model_name, model_data in models.items():
            if model_data['model'] is not None:
                if model_name != 'Prophet':
                    model_path = self.models_dir / f'{model_name.lower()}_production_{timestamp}.pkl'
                    scaler_path = self.models_dir / f'{model_name.lower()}_scaler_production_{timestamp}.pkl'
                    
                    joblib.dump(model_data['model'], model_path)
                    joblib.dump(model_data['scaler'], scaler_path)
                    
                    logger.info(f"‚úÖ Saved {model_name} model: {model_path}")
                else:
                    # Save Prophet model
                    model_path = self.models_dir / f'prophet_production_{timestamp}.pkl'
                    joblib.dump(model_data['model'], model_path)
                    logger.info(f"‚úÖ Saved Prophet model: {model_path}")
        
        # Save forecasts
        for family, forecast_data in forecasts.items():
            forecast_df = pd.DataFrame({
                'date': forecast_data['dates'],
                'family': family,
                'forecast': forecast_data['forecast'],
                'lower_bound': [forecast_data['confidence_interval']['lower']] * len(forecast_data['forecast']),
                'upper_bound': [forecast_data['confidence_interval']['upper']] * len(forecast_data['forecast']),
                'model_used': forecast_data['model_used']
            })
            
            forecast_path = self.output_dir / f'production_forecast_{family}_{timestamp}.csv'
            forecast_df.to_csv(forecast_path, index=False)
            logger.info(f"‚úÖ Saved {family} forecast: {forecast_path}")
        
        # Save metadata
        metadata = {
            'timestamp': timestamp,
            'models_trained': list(models.keys()),
            'prophet_enabled': self.enable_prophet,
            'dependency_status': {
                'prophet': PROPHET_AVAILABLE,
                'cmdstan': CMDSTAN_AVAILABLE,
                'arima': ARIMA_AVAILABLE,
                'xgboost': XGBOOST_AVAILABLE,
                'tensorflow': TENSORFLOW_AVAILABLE
            },
            'forecasts_generated': list(forecasts.keys()),
            'production_ready': True
        }
        
        metadata_path = self.output_dir / f'production_metadata_{timestamp}.json'
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        logger.info(f"‚úÖ Production metadata saved: {metadata_path}")
        return timestamp, metadata
    
    def run_hardened_pipeline(self):
        """Execute the hardened production pipeline"""
        logger.info("üîß EXECUTING HARDENED PRODUCTION PIPELINE")
        logger.info("="*80)
        
        try:
            # Step 1: Load Data
            logger.info("üìä STEP 1: Loading Production Data")
            df = self.load_production_data()
            
            # Step 2: Feature Engineering
            logger.info("‚öôÔ∏è STEP 2: Advanced Feature Engineering")
            features_df = self.advanced_feature_engineering(df)
            
            # Step 3: Train Robust Models
            logger.info("ü§ñ STEP 3: Training Robust Models")
            models = self.train_robust_models(features_df)
            
            # Step 4: Generate Forecasts
            logger.info("üìà STEP 4: Generating Production Forecasts")
            forecasts = self.generate_production_forecasts(models, features_df)
            
            # Step 5: Save Artifacts
            logger.info("üíæ STEP 5: Saving Production Artifacts")
            timestamp, metadata = self.save_production_artifacts(models, forecasts)
            
            # Success Summary
            logger.info("\n" + "="*80)
            logger.info("üîß HARDENED PIPELINE EXECUTION SUCCESSFUL! üîß")
            logger.info("="*80)
            logger.info(f"‚úÖ Timestamp: {timestamp}")
            logger.info(f"‚úÖ Models Trained: {list(models.keys())}")
            logger.info(f"‚úÖ Prophet Enabled: {self.enable_prophet}")
            logger.info(f"‚úÖ Forecasts Generated: {list(forecasts.keys())}")
            logger.info(f"‚úÖ Production Ready: {metadata['production_ready']}")
            logger.info("="*80)
            
            return {
                'status': 'SUCCESS',
                'timestamp': timestamp,
                'models': models,
                'forecasts': forecasts,
                'metadata': metadata,
                'production_ready': True
            }
            
        except Exception as e:
            logger.error(f"‚ùå Hardened pipeline execution failed: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {'status': 'FAILED', 'error': str(e)}

def main():
    """Main execution with CLI argument support"""
    import sys
    
    # Check for --enable-prophet flag
    enable_prophet = '--enable-prophet' in sys.argv
    
    # Log execution parameters
    logger.info(f"üîß Nova Corrente Hardened Pipeline")
    logger.info(f"   Prophet Flag: {enable_prophet}")
    logger.info(f"   Command Args: {sys.argv}")
    
    # Execute pipeline
    pipeline = NovaCorrenteHardenedPipeline(enable_prophet=enable_prophet)
    return pipeline.run_hardened_pipeline()

if __name__ == "__main__":
    main()